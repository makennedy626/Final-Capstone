{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TODO: Read 6.1 Time Series and 6.4 Advanced NLP\n",
    "\n",
    "# Abstract\n",
    "Abstract will be written after all analyses are complete.\n",
    "For a copy of the proposal, visit \n",
    "\n",
    "# Table of Contents\n",
    "## I. Objective\n",
    "## II. Data Collection\n",
    "## III. Scraping\n",
    "## IV. Unsupervised Neural Networks on Scraped Data to Generate Sentiment Analysis\n",
    "## V. Supervised Neural Networks to Predict Rises in Popularity\n",
    "## VI. Results\n",
    "## VII. Conclusion\n",
    "## VIII. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Objective\n",
    "\n",
    "What is the Problem You Are Attempting to Solve?\n",
    "\tWith the internet becoming increasingly involved in the average person‚Äôs daily life, many companies have taken to ecommerce, which is the electronic sale of goods through the internet. For an entrepreneur to break into the ecommerce market or for a large company to add their sources of revenue, a niche market must be found.\n",
    "\tFor the sake of clarity in this project, a niche market will be defined using this Wikipedia description:\n",
    "A niche market is the subset of the market on which a specific product is focused. The market niche defines as the product features aimed at satisfying specific market needs, as well as the price range, production quality, and the demographics that is intended to impact.\n",
    "The niche market is highly specialized, and aiming to survive among the competition from numerous super companies. Even established companies create products for different niches, for example, Hewlett-Packard has all-in-one machines for printing, scanning, and faxing targeted for the home office niche while at the same time having separate machines with one of these functions for big businesses.\n",
    "With this definition in mind, the problem will be defined as, ‚ÄúWith competition saturating the ecommerce market, what is the best niche market for a business or entrepreneur to expand into?‚Äù\n",
    "\tTo narrow the scope of this project, the focus will be on electronics appliances. \n",
    "How is Your Solution Valuable?\n",
    "The result of this project will be a solution that exceeds the performance of other niche market research tools in terms of finding the ideal niche market in a given sector. The ideal niche market is one that has some or all of the following criteria:\n",
    "‚Ä¢\thigh search volume\n",
    "‚Ä¢\tproducts that are difficult for consumers to find locally\n",
    "‚Ä¢\tsolves a common problem\n",
    "‚Ä¢\thas a customer base that is passionate\n",
    "‚Ä¢\tis not overrun with websites that have a Google PageRank of five or higher\n",
    "‚Ä¢\thas accessible suppliers.\n",
    "o\tGoogle PageRank is explained in the Appendix\n",
    "The solution will drastically reduce the number of hours and amount of wages spent for market research and will be a valuable tool for anyone in ecommerce, from large companies to individual entrepreneurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data Collection\n",
    "\n",
    "What is Your Data Source and How Will You Access It?\n",
    "Links to sources in Appendix, Access explained in Techniques Section Below\n",
    "Data for Machine Learning Models - Facebook and Twitter APIs\n",
    "Competition Research - PageRank Status for Chrome\n",
    "Validation - Google Trends\n",
    "\n",
    "\n",
    "### What Techniques From the Course Do You Anticipate Using?\n",
    "\n",
    "\n",
    "The project will begin with a broad search of keywords related to technology appliances on Keyword Planner (see appendix for description). For example, a keyword search was conducted for \"Consumer Electronics Appliances\" that returned seven hundred keywords with information on their average monthly searches and their level of competition (Low, Medium, or High). This information will be downloaded and filtered to generate a list of keywords that have a high number of average monthly searches and a low rank of competition. Keyword Planner measures competition by evaluating the number of advertisers that showed on each keyword relative to all keywords across Google. \n",
    "\n",
    "\n",
    "Twitter and Facebook will then be scraped to gather posts that reference the products in the keyword list. With this data, an Unsupervised Neural Network will perform Natural Language Processing to generate sentiment analysis of the products.\n",
    "\n",
    "\n",
    "The combined data from all techniques will be used as the features for a \n",
    "Supervised Neural Network that will predict the rise in popularity of a product over a six month period.\n",
    "The predictions will be evaluated against the rise in popularity of the product(s) via Google Trends results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Planning Results\n",
    "\n",
    "The Keyword Planner returned over seven hundred keywords under \"Consumer Electronics.\" The keywords were filtered to a list that had Average Monthly Searches between ten thousand and one hundred thousand and a Competition value under .70. The resulting list contained forty-eight keywords. After ruling out any brand names and similar keywords, this project will proceed with three of the items: Walkie Talkies, Radar Detectors, and Car Audio systems. An AliExpress search was conducted to verify the existence of manufacturers that were in a tolerable price range and able to ship one-piece orders. The tolerable price range was as follows: Walkie Talkies < \\$25, Radar Detectors < \\$15, and Car Audio < \\$45.\n",
    "\n",
    "Links to these items are in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Scraping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do we need to hit Amazon and Ebay APIs to compare prices vs our manufacturer?\n",
    "\n",
    "############### Yup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape Twitter's API, I made a new twitter account and registered an application under it to be given the necessary keys and access tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "##### Mining Twitter API #############\n",
    "######################################\n",
    "\n",
    "# https://developer.twitter.com/en/docs/tweets/filter-realtime/api-reference/post-statuses-filter.html\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = '8AZsgftPqH9dbyLfc5IFWo39v'\n",
    "consumer_secret = 'MuMAVS7rIEYa0dsyauTOeJMkjRgmU5NEvG2uzyOtWItJLli48u'\n",
    "access_token = '963889537842851841-FGZVIMTpgCS760gVa3xNu9okSg8hB0j'\n",
    "access_secret = 'obZiBaCujDaDzgT50s3q5fuKUTkvQuouQ57wUfl1YH2us'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "car_audio_json = [status for status in api.search(q='Car Audio', lan='en')] \n",
    "# Optional paramters: [, locale][, rpp][, page][, since_id][, geocode][, show_user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRATCH BEAT\n",
      "https://t.co/W0lqocTB5u\n",
      "Medium up-tempo exciting disco / funk scratch and vocal effects over end sectio https://t.co/cMuJnjHDot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Car Audio'\n",
    "\n",
    "car_audio_tweets = [status for status in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(100)]\n",
    "print(car_audio_tweets[0].full_text)\n",
    "len(car_audio_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SCRATCH BEAT\\nhttps://t.co/W0lqocTB5u\\nMedium up-tempo exciting disco / funk scratch and vocal effects over end sectio https://t.co/cMuJnjHDot'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_text = []\n",
    "for text in car_audio_tweets:\n",
    "    ca_text.append(text.full_text)\n",
    "ca_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @SwitchoFlippo: Guide to Walkie Talkies - Ebuyer Blog https://t.co/FAjuOeRKUn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Walkie Talkies'\n",
    "\n",
    "walkie_talkie_tweets = [status for status in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(100)]\n",
    "print(walkie_talkie_tweets[0].full_text)\n",
    "len(walkie_talkie_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @SwitchoFlippo: Guide to Walkie Talkies - Ebuyer Blog https://t.co/FAjuOeRKUn'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_text = []\n",
    "for text in walkie_talkie_tweets:\n",
    "    wt_text.append(text.full_text)\n",
    "wt_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I liked a @YouTube video https://t.co/Dn21VWy27D Living With a Radar Detector- Worth the Buy?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'radar detector'\n",
    "\n",
    "radar_detector_tweets = [status for status in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(100)]\n",
    "print(radar_detector_tweets[0].full_text)\n",
    "len(radar_detector_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I liked a @YouTube video https://t.co/Dn21VWy27D Living With a Radar Detector- Worth the Buy?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_text = []\n",
    "for text in radar_detector_tweets:\n",
    "    rd_text.append(text.full_text)\n",
    "rd_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Category, Text]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the lists of texts into a DataFrame\n",
    "import pandas as pd\n",
    "texts = {'Category':[], 'Text':[]}\n",
    "columns = texts.keys()\n",
    "all_texts = pd.DataFrame(data=texts, columns=columns)\n",
    "all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {'Category':'rd_text', 'Text':[ca_text, wt_text, rd_text]}\n",
    "all_texts = all_texts.append(pd.DataFrame(data=texts, columns=texts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/Dn21VWy2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>360 Degree Radar Detector https://t.co/ODC0t1A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@PureBread_ Gonna buy a radar detector and do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@shannon_last Yet you can't have a radar detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Uniden R3 Extreme Long Range Radar Laser Detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @AnniemuMary: Yeah? Well my boyfriend had a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Uniden Radar Detector Prices About to Increase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>#Hgdo_–≤–∏–¥–µ–æ—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ç–æ—Ä –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–π 2 –≤ 1 —Ä—É—Å...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @xmicky: Disassembling a coyote S radar det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>360 Degree Laser/Radar Detector With Voice Ale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@gxrcvi Accidentally doing 70 in a 45?? How th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>2016 @Porsche GT3 RS now available for purchas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@ABC I'm sure no politicians get any tickets a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Escort Passport S75 Radar Detector w/ BSM Filt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>The one thing that lies to me the most is my r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Just $20.12,  YKT - FD013 V7 High-performance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Only $20.12,  YKT - FD013 V7 High-performance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Just $20.12,  YKT - FD013 V7 High-performance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Just $20.12,  YKT - FD013 V7 High-performance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Escort Passport S75 Radar Detector for $179.99...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@wirecutter hey! Got me a New toy‚Ä¶do you have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Disassembling a coyote S radar detector! \\ndis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Introducing Radar Zero, paperback sized, simpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>üöóüëÆThe penalty for radar detector use in üá®üá≠#Swi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>üöóüëÆThe penalty for radar detector use in üá®üá≠#Swi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Escort Passport S55 High Performance Pro Radar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @keanuareed: *Being ‚Äúpulled over‚Äù*\\nCop: Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>My radar detector is really my best friend dawg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/r4juPBnC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>China offers: https://t.co/HX3icQL7Co -KDH CN-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @DixonPolice: That moment a cop pulls someo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Ghost Detector &amp;amp; Radar Tracker App Apk Rev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Beltronics RX65 Red Professional Series Radar/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>360 Degree Radar Detector https://t.co/ClR9xjY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>üì£üëÆüì£Ban for radar detector use in üáÆüáπ#Italy.\\nPe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>UniqueX Fashion Sticky Pad Dash Mat Cell Phone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>well CP‚Äôs car has gotten a lot smoother of a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>My mom is making me buy a radar detector cause...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/lRjIbOcf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/SZ3Qy5vt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @DixonPolice: That moment a cop pulls someo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @DixonPolice: That moment a cop pulls someo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>If you're looking for a road #travel #gadget t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @DixonPolice: That moment a cop pulls someo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @DixonPolice: That moment a cop pulls someo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Bateu uma nostalgia agora do ver√£o de 2016 num...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>\"You know why I pulled you over?\"\\nBecause my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @DixonPolice: That moment a cop pulls someo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[SCRATCH BEAT\\nhttps://t.co/W0lqocTB5u\\nMedium...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[RT @SwitchoFlippo: Guide to Walkie Talkies - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[I liked a @YouTube video https://t.co/Dn21VWy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[SCRATCH BEAT\\nhttps://t.co/W0lqocTB5u\\nMedium...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[RT @SwitchoFlippo: Guide to Walkie Talkies - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[I liked a @YouTube video https://t.co/Dn21VWy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[SCRATCH BEAT\\nhttps://t.co/W0lqocTB5u\\nMedium...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[RT @SwitchoFlippo: Guide to Walkie Talkies - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[I liked a @YouTube video https://t.co/Dn21VWy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[SCRATCH BEAT\\nhttps://t.co/W0lqocTB5u\\nMedium...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[RT @SwitchoFlippo: Guide to Walkie Talkies - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[I liked a @YouTube video https://t.co/Dn21VWy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                               Text\n",
       "0   rd_text  I liked a @YouTube video https://t.co/Dn21VWy2...\n",
       "1   rd_text  360 Degree Radar Detector https://t.co/ODC0t1A...\n",
       "2   rd_text  @PureBread_ Gonna buy a radar detector and do ...\n",
       "3   rd_text  @shannon_last Yet you can't have a radar detec...\n",
       "4   rd_text  Uniden R3 Extreme Long Range Radar Laser Detec...\n",
       "5   rd_text  RT @AnniemuMary: Yeah? Well my boyfriend had a...\n",
       "6   rd_text  Uniden Radar Detector Prices About to Increase...\n",
       "7   rd_text  #Hgdo_–≤–∏–¥–µ–æ—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ç–æ—Ä –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–π 2 –≤ 1 —Ä—É—Å...\n",
       "8   rd_text  RT @xmicky: Disassembling a coyote S radar det...\n",
       "9   rd_text  360 Degree Laser/Radar Detector With Voice Ale...\n",
       "10  rd_text  @gxrcvi Accidentally doing 70 in a 45?? How th...\n",
       "11  rd_text  2016 @Porsche GT3 RS now available for purchas...\n",
       "12  rd_text  @ABC I'm sure no politicians get any tickets a...\n",
       "13  rd_text  Escort Passport S75 Radar Detector w/ BSM Filt...\n",
       "14  rd_text  The one thing that lies to me the most is my r...\n",
       "15  rd_text  Just $20.12,  YKT - FD013 V7 High-performance ...\n",
       "16  rd_text  Only $20.12,  YKT - FD013 V7 High-performance ...\n",
       "17  rd_text  Just $20.12,  YKT - FD013 V7 High-performance ...\n",
       "18  rd_text  Just $20.12,  YKT - FD013 V7 High-performance ...\n",
       "19  rd_text  Escort Passport S75 Radar Detector for $179.99...\n",
       "20  rd_text  @wirecutter hey! Got me a New toy‚Ä¶do you have ...\n",
       "21  rd_text  Disassembling a coyote S radar detector! \\ndis...\n",
       "22  rd_text  Introducing Radar Zero, paperback sized, simpl...\n",
       "23  rd_text  üöóüëÆThe penalty for radar detector use in üá®üá≠#Swi...\n",
       "24  rd_text  üöóüëÆThe penalty for radar detector use in üá®üá≠#Swi...\n",
       "25  rd_text  Escort Passport S55 High Performance Pro Radar...\n",
       "26  rd_text  RT @keanuareed: *Being ‚Äúpulled over‚Äù*\\nCop: Mo...\n",
       "27  rd_text    My radar detector is really my best friend dawg\n",
       "28  rd_text  I liked a @YouTube video https://t.co/r4juPBnC...\n",
       "29  rd_text  China offers: https://t.co/HX3icQL7Co -KDH CN-...\n",
       "..      ...                                                ...\n",
       "82  rd_text  RT @DixonPolice: That moment a cop pulls someo...\n",
       "83  rd_text  Ghost Detector &amp; Radar Tracker App Apk Rev...\n",
       "84  rd_text  Beltronics RX65 Red Professional Series Radar/...\n",
       "85  rd_text  360 Degree Radar Detector https://t.co/ClR9xjY...\n",
       "86  rd_text  üì£üëÆüì£Ban for radar detector use in üáÆüáπ#Italy.\\nPe...\n",
       "87  rd_text  UniqueX Fashion Sticky Pad Dash Mat Cell Phone...\n",
       "88  rd_text  well CP‚Äôs car has gotten a lot smoother of a r...\n",
       "89  rd_text  My mom is making me buy a radar detector cause...\n",
       "90  rd_text  I liked a @YouTube video https://t.co/lRjIbOcf...\n",
       "91  rd_text  I liked a @YouTube video https://t.co/SZ3Qy5vt...\n",
       "92  rd_text  RT @DixonPolice: That moment a cop pulls someo...\n",
       "93  rd_text  RT @DixonPolice: That moment a cop pulls someo...\n",
       "94  rd_text  If you're looking for a road #travel #gadget t...\n",
       "95  rd_text  RT @DixonPolice: That moment a cop pulls someo...\n",
       "96  rd_text  RT @DixonPolice: That moment a cop pulls someo...\n",
       "97  rd_text  Bateu uma nostalgia agora do ver√£o de 2016 num...\n",
       "98  rd_text  \"You know why I pulled you over?\"\\nBecause my ...\n",
       "99  rd_text  RT @DixonPolice: That moment a cop pulls someo...\n",
       "0   rd_text  [SCRATCH BEAT\\nhttps://t.co/W0lqocTB5u\\nMedium...\n",
       "1   rd_text  [RT @SwitchoFlippo: Guide to Walkie Talkies - ...\n",
       "2   rd_text  [I liked a @YouTube video https://t.co/Dn21VWy...\n",
       "0   rd_text  [SCRATCH BEAT\\nhttps://t.co/W0lqocTB5u\\nMedium...\n",
       "1   rd_text  [RT @SwitchoFlippo: Guide to Walkie Talkies - ...\n",
       "2   rd_text  [I liked a @YouTube video https://t.co/Dn21VWy...\n",
       "0   rd_text  [SCRATCH BEAT\\nhttps://t.co/W0lqocTB5u\\nMedium...\n",
       "1   rd_text  [RT @SwitchoFlippo: Guide to Walkie Talkies - ...\n",
       "2   rd_text  [I liked a @YouTube video https://t.co/Dn21VWy...\n",
       "0   rd_text  [SCRATCH BEAT\\nhttps://t.co/W0lqocTB5u\\nMedium...\n",
       "1   rd_text  [RT @SwitchoFlippo: Guide to Walkie Talkies - ...\n",
       "2   rd_text  [I liked a @YouTube video https://t.co/Dn21VWy...\n",
       "\n",
       "[212 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Unsupervised Neural Network for Sentiment Classification of Twitter Data\n",
    "\n",
    "The model is initially trained on IMDB sentiment analysis and can be found here: https://keras.io/datasets/\n",
    "\n",
    "The twitter data is vectorized using text_to_word_sequence and tokenizer from the keras preprocessing library: https://keras.io/preprocessing/text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 173s - loss: 0.4221 - acc: 0.8023 - val_loss: 0.3464 - val_acc: 0.8512\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 168s - loss: 0.2263 - acc: 0.9117 - val_loss: 0.3525 - val_acc: 0.8462\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 161s - loss: 0.1303 - acc: 0.9527 - val_loss: 0.4317 - val_acc: 0.8416\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 161s - loss: 0.0727 - acc: 0.9741 - val_loss: 0.6637 - val_acc: 0.8326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2dc83c87cc0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example from https://github.com/keras-team/keras/blob/master/examples/imdb_bidirectional_lstm.py\n",
    "\n",
    "'''Trains a Bidirectional LSTM on the IMDB sentiment classification task.\n",
    "Output after 4 epochs on CPU: ~0.8146\n",
    "Time per epoch on CPU (Core i7): ~150s.\n",
    "'''\n",
    "\n",
    "########################################################################\n",
    "########### LOOK INTO USING GPU TO SAVE ON CPU LOAD ####################\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text']\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing import text\n",
    "\n",
    "results = []\n",
    "\n",
    "for text in all_texts:\n",
    "    results = keras.preprocessing.text.text_to_word_sequence(text,\n",
    "                                               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                               lower=True,\n",
    "                                               split=\" \")\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'num_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-57dcd61d71d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m                                    \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                                    \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                                    char_level=False)\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got multiple values for argument 'num_words'"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for text in all_texts:\n",
    "\n",
    "    results = keras.preprocessing.text.Tokenizer(ca_text,\n",
    "                                   num_words=None,\n",
    "                                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,\n",
    "                                   split=\" \",\n",
    "                                   char_level=False,\n",
    "                                   oov_token=None)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Supervised Neural Network for Predicting Rises in Popularity\n",
    "\n",
    "The combined data from all techniques will be used as the features for a Supervised Neural Network that will predict the rise in popularity of a product over a six month period. The predictions will be evaluated against the rise in popularity of the product(s) via Google Trends results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to download Google Trends Data\n",
    "\n",
    "# x is the result of the sentiment analyses\n",
    "\n",
    "# y is whether the product rose in popularity in the last six months on Google Trends Data\n",
    "\n",
    "# Modify paramaters to fit text classification with two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, activation='relu', input_shape=(3072,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# Output shape should be equal to the number of classes (10)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = 64, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, verbose=False)\n",
    "print('Test Loss: ', score[0])\n",
    "print('Test Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII. Appendix\n",
    "\n",
    "Google PageRank Description (from Google Technology page):\n",
    "PageRank Explained\n",
    "PageRank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page‚Äôs value. In essence, Google interprets a link from page A to page B as a vote, by page A, for page B. But, Google looks at considerably more than the sheer volume of votes, or links a page receives; for example, it also analyzes the page that casts the vote. Votes cast by pages that are themselves ‚Äúimportant‚Äù weigh more heavily and help to make other pages ‚Äúimportant.‚Äù Using these and other factors, Google provides its views on pages‚Äô relative importance.\n",
    "Of course, important pages mean nothing to you if they don‚Äôt match your query. So, Google combines PageRank with sophisticated text-matching techniques to find pages that are both important and relevant to your search. Google goes far beyond the number of times a term appears on a page and examines all dozens of aspects of the page‚Äôs content (and the content of the pages linking to it) to determine if it‚Äôs a good match for your query.\n",
    "\n",
    "Keyword Planner Description (from Google Adwords page):\n",
    "Keyword Planner is a free AdWords tool for new or experienced advertisers that‚Äôs like a workshop for building new Search Network campaigns or expanding existing ones. You can search for keyword and ad group ideas, see how a list of keywords might perform, and even create a new keyword list by multiplying several lists of keywords together. Keyword Planner can also help you choose competitive bids and budgets to use with your campaigns.\n",
    "Sources:\n",
    "Facebook: http://graph.facebook.com\n",
    "Google Trends: https://developers.google.com/gdata/docs/directory\n",
    "Google News: https://newsapi.org/s/google-news-api\n",
    "Keyword Planner: https://adwords.google.com/ko/KeywordPlanner/Home?sourceid=awo&__u=3050284228&__c=9347440984&authuser=0#search\n",
    "Twitter: https://developer.twitter.com/en/docs/basics/getting-started\n",
    "\n",
    "Links to AliExpress items: \n",
    "\n",
    "https://www.aliexpress.com/item/2pcs-New-Black-Retevis-RT628-Portable-radio-Walkie-Talkie-sets-0-5W-8CH-UHF-Europe-Frequency/32407070024.html?spm=2114.search0104.3.27.383641a8WAVsOz&ws_ab_test=searchweb0_0,searchweb201602_3_10152_10151_10065_10344_10130_10068_10324_10342_10547_10325_10343_10546_10340_10548_10341_10545_10084_10083_10618_10307_10313_10059_10534_100031_10629_10103_10626_10625_10624_10623_10622_10621_10620_10142,searchweb201603_25,ppcSwitch_5&algo_expid=c512be04-3611-4608-90ee-6e55891cae81-6&algo_pvid=c512be04-3611-4608-90ee-6e55891cae81&priceBeautifyAB=0\n",
    "\n",
    "https://www.aliexpress.com/item/4012B-4-1-inch-1-Din-Car-Radio-Auto-Audio-Stereo-FM-Bluetooth-2-0-Support/32838433123.html?spm=2114.search0104.3.25.6529687a4mQ9YS&ws_ab_test=searchweb0_0,searchweb201602_3_10152_10151_10065_10344_10130_10068_10324_10342_10547_10325_10343_10546_10340_10548_10341_10545_10084_10083_10618_10307_10313_10059_10534_100031_10629_10103_10626_10625_10624_10623_10622_10621_10620_10142,searchweb201603_25,ppcSwitch_5&algo_expid=78926112-aa75-459d-888f-2b887bd13743-3&algo_pvid=78926112-aa75-459d-888f-2b887bd13743&priceBeautifyAB=0\n",
    "\n",
    "https://www.aliexpress.com/item/Clear-Stock-Excelvan-E8-Car-Radar-Detector-360-Degree-Speed-Safety-Anti-Police-Scanning-Advanced-Voice/32839841753.html?spm=2114.search0104.3.1.1bc42b2a9yCeVF&ws_ab_test=searchweb0_0,searchweb201602_3_10152_10151_10065_10344_10130_10068_10324_10342_10547_10325_10343_10546_10340_10548_10341_10545_10084_10083_10618_10307_10313_10059_10534_100031_10629_10103_10626_10625_10624_10623_10622_10621_10620_10142,searchweb201603_25,ppcSwitch_5&algo_expid=9a63780f-134d-4690-9830-ebf6c5fff3aa-0&algo_pvid=9a63780f-134d-4690-9830-ebf6c5fff3aa&priceBeautifyAB=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra code that is not yet implemented:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#Import the necessary modules\n",
    "import datetime\n",
    "from newsapi import NewsApiClient\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Matthew Kennedy's API Key, business will need to register for their own api and place it below (registration is free)\n",
    "newsapi = NewsApiClient(api_key='2613ce5e838a464b814b7d5b4c2e6bf8')\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date = now.day\n",
    "\n",
    "keyword_list = []\n",
    "\n",
    "####################################################################\n",
    "########## Add keywords from Keyword Planner to keyword_list########\n",
    "####################################################################\n",
    "\n",
    "all_articles = newsapi.get_everything(q=[keyword_list]\n",
    "                                     #, from_paramater = '2018-01-08' #this can be changed to any date\n",
    "                                     , to = date\n",
    "                                     , language = 'en'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "data = all_articles['articles']\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# The Json is a list of dictionary values. This code will store it to a DataFrame.\n",
    "results = {'Description': [], 'Publish Date': [], 'URL': []}\n",
    "columns = results.keys()\n",
    "df_data = pd.DataFrame(data=results, columns=columns)\n",
    "\n",
    "for entry in data:\n",
    "    #print(data[entry]['description'])\n",
    "    description = entry['description']\n",
    "    date = entry['publishedAt']\n",
    "    url = entry['url']\n",
    "    results = {'Description':[description], 'Publish Date':[date], 'URL': [url]}\n",
    "    df_data = df_data.append(pd.DataFrame(data=results, columns=results.keys()), ignore_index = False)\n",
    "    \n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "############################################################################\n",
    "###### Now, create a crawler to crawl the information of each url ##########\n",
    "############################################################################\n",
    "\n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# Make a list for the urls to be stored into from the dataframe\n",
    "url_list = []\n",
    "for entry in df_data['URL']:\n",
    "    url_list.append(entry)\n",
    "    \n",
    "#print(url_list)\n",
    "\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = url_list\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        # for article in response.xpath('//article'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                'everything': response.xpath('//text()').extract()\n",
    "                #'all': response.xpath('/descendant-or-self::node()').extract()\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                #'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                #'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                #'text': article.xpath('//*[@id=\"post-669657\"]/div[3]').extract()\n",
    "                #,'body': article.xpath('//*[@id=\"single-post\"]').extract()\n",
    "                #'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'firstpage4.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "######## This doesn't stop running because it is a stream and it stays open. \n",
    "######## Cell after this one downloads the stream, but it is run through cmd line\n",
    "\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    " \n",
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('python.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    " \n",
    "\n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=['Car Audio', 'Walkie Talkies', 'Radar Detector'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# To run this code, first edit config.py with your configuration, then:\n",
    "#\n",
    "# mkdir data\n",
    "# python twitter_stream_download.py -q apple -d data\n",
    "# \n",
    "# It will produce the list of tweets for the query \"apple\" \n",
    "# in the file data/stream_apple.json\n",
    "\n",
    "import tweepy\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import time\n",
    "import argparse\n",
    "import string\n",
    "import config\n",
    "import json\n",
    "\n",
    "def get_parser():\n",
    "    \"\"\"Get parser for command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Twitter Downloader\")\n",
    "    parser.add_argument(\"-q\",\n",
    "                        \"--query\",\n",
    "                        dest=\"query\",\n",
    "                        help=\"Query/Filter\",\n",
    "                        default='-')\n",
    "    parser.add_argument(\"-d\",\n",
    "                        \"--data-dir\",\n",
    "                        dest=\"data_dir\",\n",
    "                        help=\"Output/Data Directory\")\n",
    "    return parser\n",
    "\n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    \"\"\"Custom StreamListener for streaming data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, query):\n",
    "        query_fname = format_filename(query)\n",
    "        self.outfile = \"%s/stream_%s.json\" % (data_dir, query_fname)\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open(self.outfile, 'a') as f:\n",
    "                f.write(data)\n",
    "                print(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "            time.sleep(5)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "\n",
    "\n",
    "def format_filename(fname):\n",
    "    \"\"\"Convert file name into a safe string.\n",
    "    Arguments:\n",
    "        fname -- the file name to convert\n",
    "    Return:\n",
    "        String -- converted file name\n",
    "    \"\"\"\n",
    "    return ''.join(convert_valid(one_char) for one_char in fname)\n",
    "\n",
    "\n",
    "def convert_valid(one_char):\n",
    "    \"\"\"Convert a character into '_' if invalid.\n",
    "    Arguments:\n",
    "        one_char -- the char to convert\n",
    "    Return:\n",
    "        Character -- converted char\n",
    "    \"\"\"\n",
    "    valid_chars = \"-_.%s%s\" % (string.ascii_letters, string.digits)\n",
    "    if one_char in valid_chars:\n",
    "        return one_char\n",
    "    else:\n",
    "        return '_'\n",
    "\n",
    "@classmethod\n",
    "def parse(cls, api, raw):\n",
    "    status = cls.first_parse(api, raw)\n",
    "    setattr(status, 'json', json.dumps(raw))\n",
    "    return status\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = get_parser()\n",
    "    args = parser.parse_args()\n",
    "    auth = OAuthHandler(config.consumer_key, config.consumer_secret)\n",
    "    auth.set_access_token(config.access_token, config.access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    twitter_stream = Stream(auth, MyListener(args.data_dir, args.query))\n",
    "    twitter_stream.filter(track=[args.query])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will send a request for \n",
    "\n",
    "curl -X POST \"https://api.twitter.com/1.1/tweets/search/:product/:label.json\" -d '{\"query\":\"Car Audio, Walkie Talkies, Radar Detector\",\"maxResults\":\"500\"}' -H \"Authorization: 963889537842851841-FGZVIMTpgCS760gVa3xNu9okSg8hB0j, obZiBaCujDaDzgT50s3q5fuKUTkvQuouQ57wUfl1YH2us\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying something else\n",
    "class MyModelParser(tweepy.parsers.ModelParser):\n",
    "    def parse(self, method, payload:\n",
    "              result = super(MyModelparser, self).parse(method, payload)\n",
    "              result._payload = json.loads(payload)\n",
    "api = tweepy.API(auth, parser=MyModelParser())\n",
    "results = api.search(q='Car Audio', lan='en')\n",
    "              \n",
    "for s in results._payload:\n",
    "              print(json.dumps(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "walkie_talkies_json = api.search(q='Walkie Talkies', lan='en') \n",
    "# Optional paramters: [, locale][, rpp][, page][, since_id][, geocode][, show_user])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
