{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TODO: Read 6.1 Time Series and 6.4 Advanced NLP\n",
    "\n",
    "# Abstract\n",
    "Abstract will be written after all analyses are complete.\n",
    "For a copy of the proposal, visit \n",
    "\n",
    "# Table of Contents\n",
    "## I. Objective\n",
    "## II. Data Collection\n",
    "## III. Scraping\n",
    "## IV. Unsupervised Neural Networks on Scraped Data to Generate Sentiment Analysis\n",
    "## V. Supervised Neural Networks to Predict Rises in Popularity\n",
    "## VI. Results\n",
    "## VII. Conclusion\n",
    "## VIII. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Objective\n",
    "\n",
    "What is the Problem You Are Attempting to Solve?\n",
    "\tWith the internet becoming increasingly involved in the average person’s daily life, many companies have taken to ecommerce, which is the electronic sale of goods through the internet. For an entrepreneur to break into the ecommerce market or for a large company to add their sources of revenue, a niche market must be found.\n",
    "\tFor the sake of clarity in this project, a niche market will be defined using this Wikipedia description:\n",
    "A niche market is the subset of the market on which a specific product is focused. The market niche defines as the product features aimed at satisfying specific market needs, as well as the price range, production quality, and the demographics that is intended to impact.\n",
    "The niche market is highly specialized, and aiming to survive among the competition from numerous super companies. Even established companies create products for different niches, for example, Hewlett-Packard has all-in-one machines for printing, scanning, and faxing targeted for the home office niche while at the same time having separate machines with one of these functions for big businesses.\n",
    "With this definition in mind, the problem will be defined as, “With competition saturating the ecommerce market, what is the best niche market for a business or entrepreneur to expand into?”\n",
    "\tTo narrow the scope of this project, the focus will be on electronics appliances. \n",
    "How is Your Solution Valuable?\n",
    "The result of this project will be a solution that exceeds the performance of other niche market research tools in terms of finding the ideal niche market in a given sector. The ideal niche market is one that has some or all of the following criteria:\n",
    "•\thigh search volume\n",
    "•\tproducts that are difficult for consumers to find locally\n",
    "•\tsolves a common problem\n",
    "•\thas a customer base that is passionate\n",
    "•\tis not overrun with websites that have a Google PageRank of five or higher\n",
    "•\thas accessible suppliers.\n",
    "o\tGoogle PageRank is explained in the Appendix\n",
    "The solution will drastically reduce the number of hours and amount of wages spent for market research and will be a valuable tool for anyone in ecommerce, from large companies to individual entrepreneurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data Collection\n",
    "\n",
    "What is Your Data Source and How Will You Access It?\n",
    "Links to sources in Appendix, Access explained in Techniques Section Below\n",
    "Data for Machine Learning Models - Facebook and Twitter APIs\n",
    "Competition Research - PageRank Status for Chrome\n",
    "Validation - Google Trends\n",
    "\n",
    "\n",
    "### What Techniques From the Course Do You Anticipate Using?\n",
    "\n",
    "\n",
    "The project will begin with a broad search of keywords related to technology appliances on Keyword Planner (see appendix for description). For example, a keyword search was conducted for \"Consumer Electronics Appliances\" that returned seven hundred keywords with information on their average monthly searches and their level of competition (Low, Medium, or High). This information will be downloaded and filtered to generate a list of keywords that have a high number of average monthly searches and a low rank of competition. Keyword Planner measures competition by evaluating the number of advertisers that showed on each keyword relative to all keywords across Google. \n",
    "\n",
    "\n",
    "Twitter and Facebook will then be scraped to gather posts that reference the products in the keyword list. With this data, an Unsupervised Neural Network will perform Natural Language Processing to generate sentiment analysis of the products.\n",
    "\n",
    "\n",
    "The combined data from all techniques will be used as the features for a \n",
    "Supervised Neural Network that will predict the rise in popularity of a product over a six month period.\n",
    "The predictions will be evaluated against the rise in popularity of the product(s) via Google Trends results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Planning Results\n",
    "\n",
    "The Keyword Planner returned over seven hundred keywords under \"Consumer Electronics.\" The keywords were filtered to a list that had Average Monthly Searches between ten thousand and one hundred thousand and a Competition value under .70. The resulting list contained forty-eight keywords. After ruling out any brand names and similar keywords, this project will proceed with three of the items: Walkie Talkies, Radar Detectors, and Car Audio systems. An AliExpress search was conducted to verify the existence of manufacturers that were in a tolerable price range and able to ship one-piece orders. The tolerable price range was as follows: Walkie Talkies < \\$25, Radar Detectors < \\$15, and Car Audio < \\$45.\n",
    "\n",
    "Links to these items are in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Scraping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape Twitter's API, I made a new twitter account and registered an application under it to be given the necessary keys and access tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "##### Mining Twitter API #############\n",
    "######################################\n",
    "\n",
    "# https://developer.twitter.com/en/docs/tweets/filter-realtime/api-reference/post-statuses-filter.html\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = '8AZsgftPqH9dbyLfc5IFWo39v'\n",
    "consumer_secret = 'MuMAVS7rIEYa0dsyauTOeJMkjRgmU5NEvG2uzyOtWItJLli48u'\n",
    "access_token = '963889537842851841-FGZVIMTpgCS760gVa3xNu9okSg8hB0j'\n",
    "access_secret = 'obZiBaCujDaDzgT50s3q5fuKUTkvQuouQ57wUfl1YH2us'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "car_audio_json = [status for status in api.search(q='Car Audio', lan='en')] \n",
    "# Optional paramters: [, locale][, rpp][, page][, since_id][, geocode][, show_user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I liked a @YouTube video https://t.co/ZAZIvs2YF6 Extreme Car Audio FAIL &amp; Fix - \"Bucket o' BASS\" Chevy Silverado - Intro, Walk Around\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Car Audio'\n",
    "\n",
    "car_audio_tweets = [status for status in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(100)]\n",
    "print(car_audio_tweets[0].full_text)\n",
    "len(car_audio_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I liked a @YouTube video https://t.co/ZAZIvs2YF6 Extreme Car Audio FAIL &amp; Fix - \"Bucket o\\' BASS\" Chevy Silverado - Intro, Walk Around'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_text = []\n",
    "for text in car_audio_tweets:\n",
    "    ca_text.append(text.full_text)\n",
    "ca_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Category, Text]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the lists of texts into a DataFrame\n",
    "import pandas as pd\n",
    "texts = {'Category':[], 'Text':[]}\n",
    "columns = texts.keys()\n",
    "all_texts = pd.DataFrame(data=texts, columns=columns)\n",
    "all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {'Category':'ca_text', 'Text':ca_text}\n",
    "all_texts = all_texts.append(pd.DataFrame(data=texts, columns=texts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are Walkie Talkies? (with pictures) https://t.co/mquPOW5YpL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Walkie Talkies'\n",
    "\n",
    "walkie_talkie_tweets = [status for status in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(100)]\n",
    "print(walkie_talkie_tweets[0].full_text)\n",
    "len(walkie_talkie_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are Walkie Talkies? (with pictures) https://t.co/mquPOW5YpL'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_text = []\n",
    "for text in walkie_talkie_tweets:\n",
    "    wt_text.append(text.full_text)\n",
    "wt_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = {'Category':'wt_text', 'Text':wt_text}\n",
    "all_texts = all_texts.append(pd.DataFrame(data=texts, columns=texts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal Sport Radar Gun Detector Speeds Baseball Tennis Hockey Soccer Lacrosse https://t.co/tIoI8MJL04 https://t.co/RgAxRVHwpv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'radar detector'\n",
    "\n",
    "radar_detector_tweets = [status for status in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(100)]\n",
    "print(radar_detector_tweets[0].full_text)\n",
    "len(radar_detector_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Personal Sport Radar Gun Detector Speeds Baseball Tennis Hockey Soccer Lacrosse https://t.co/tIoI8MJL04 https://t.co/RgAxRVHwpv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_text = []\n",
    "for text in radar_detector_tweets:\n",
    "    rd_text.append(text.full_text)\n",
    "rd_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = {'Category':'rd_text', 'Text':rd_text}\n",
    "all_texts = all_texts.append(pd.DataFrame(data=texts, columns=texts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/ZAZIvs2Y...</td>\n",
       "      <td>[[I, liked, a, @YouTube, video, https://t.co/Z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @talknaboutcars: LISTEN to our new TAC #107...</td>\n",
       "      <td>[[RT, @talknaboutcars, :, LISTEN, to, our, new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/1A6KOLIS...</td>\n",
       "      <td>[[I, liked, a, @YouTube, video, https://t.co/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>This is the type of @SpotsySheriff see please ...</td>\n",
       "      <td>[[This, is, the, type, of, @SpotsySheriff, see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>【car audio U-23】日産 セレナ by サウンドステーション AVカンサイ宝塚店...</td>\n",
       "      <td>[[【, car, audio, U, -, 23, 】, 日産, セレナ, by, サウン...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @talknaboutcars: LISTEN to our new TAC #107...</td>\n",
       "      <td>[[RT, @talknaboutcars, :, LISTEN, to, our, new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>【car audio U-23】日産 セレナ by サウンドステーション AVカンサイ宝塚店...</td>\n",
       "      <td>[[【, car, audio, U, -, 23, 】, 日産, セレナ, by, サウン...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>LISTEN to our new TAC #107 podcast with @CarKr...</td>\n",
       "      <td>[[LISTEN, to, our, new, TAC, #107, podcast, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>レスポンス 【car audio U-23】日産 セレナ by サウンドステーション AVカ...</td>\n",
       "      <td>[[レスポンス, 【, car, audio, U, -, 23, 】, 日産, セレナ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>@Pink_About_it Same idiocracy as plastering PI...</td>\n",
       "      <td>[[@Pink_About_it, Same, idiocracy, as, plaster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @responsejp: 【car audio U-23】日産 セレナ by サウンド...</td>\n",
       "      <td>[[RT, @responsejp, :, 【, car, audio, U, -, 23,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @MTXAudio: Choosing the right subwoofer for...</td>\n",
       "      <td>[[RT, @MTXAudio, :, Choosing, the, right, subw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>NEW KICKER 11HS8 8″ 150W Hideaway Car Audio Po...</td>\n",
       "      <td>[[NEW, KICKER, 11, HS8, 8, ″, 150, W, Hideaway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @responsejp: 【car audio U-23】日産 セレナ by サウンド...</td>\n",
       "      <td>[[RT, @responsejp, :, 【, car, audio, U, -, 23,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @responsejp: 【car audio U-23】日産 セレナ by サウンド...</td>\n",
       "      <td>[[RT, @responsejp, :, 【, car, audio, U, -, 23,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>【car audio U-23】日産 セレナ by サウンドステーション AVカンサイ宝塚店...</td>\n",
       "      <td>[[【, car, audio, U, -, 23, 】, 日産, セレナ, by, サウン...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>@Superlitio Con mi esposo y los hemos escuchad...</td>\n",
       "      <td>[[@Superlitio, Con, mi, esposo, y, los, hemos,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>Need a new stereo? Alram or accessories for yo...</td>\n",
       "      <td>[[Need, a, new, stereo, ?, Alram, or, accessor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>then sends me a 25 minute audio and calls me \"...</td>\n",
       "      <td>[[then, sends, me, a, 25, minute, audio, and, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/gfEtWyFH...</td>\n",
       "      <td>[[I, liked, a, @YouTube, video, https://t.co/g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/jN4GG5O3...</td>\n",
       "      <td>[[I, liked, a, @YouTube, video, https://t.co/j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>@holly_the_grey @MarciIen Chief Mark Saunders ...</td>\n",
       "      <td>[[@holly_the_grey, @MarciIen, Chief, Mark, Sau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>@mattgallowaycbc Chief Mark Saunders has even ...</td>\n",
       "      <td>[[@mattgallowaycbc, Chief, Mark, Saunders, has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/O84SNxmS...</td>\n",
       "      <td>[[I, liked, a, @YouTube, video, https://t.co/O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>@KarenJohnsonTV @johnmaher0 @MarciIen Chief Ma...</td>\n",
       "      <td>[[@KarenJohnsonTV, @johnmaher0, @MarciIen, Chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I added a video to a @YouTube playlist https:/...</td>\n",
       "      <td>[[I, added, a, video, to, a, @YouTube, playlis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>【car audio U-23】日産 セレナ by サウンドステーション AVカンサイ宝塚店...</td>\n",
       "      <td>[[【, car, audio, U, -, 23, 】, 日産, セレナ, by, サウン...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>【car audio U-23】日産 セレナ by サウンドステーション AVカンサイ宝塚店...</td>\n",
       "      <td>[[【, car, audio, U, -, 23, 】, 日産, セレナ, by, サウン...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>レスポンス 【car audio U-23】日産 セレナ by サウンドステーション AVカ...</td>\n",
       "      <td>[[レスポンス, 【, car, audio, U, -, 23, 】, 日産, セレナ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @VAULTFestival: CALLING ALL WRITERS! @fever...</td>\n",
       "      <td>[[RT, @VAULTFestival, :, CALLING, ALL, WRITERS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @TJBdeals: Escort Passport S75 Radar Detect...</td>\n",
       "      <td>[[RT, @TJBdeals, :, Escort, Passport, S75, Rad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>BlendMount Radar Detector Mount for Uniden,Sta...</td>\n",
       "      <td>[[BlendMount, Radar, Detector, Mount, for, Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@steveaustinBSR what gimmick radar detector do...</td>\n",
       "      <td>[[@steveaustinBSR, what, gimmick, radar, detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/Dn21VWy2...</td>\n",
       "      <td>[[I, liked, a, @YouTube, video, https://t.co/D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>360 Degree Radar Detector https://t.co/ODC0t1A...</td>\n",
       "      <td>[[360, Degree, Radar, Detector, https://t.co/O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@PureBread_ Gonna buy a radar detector and do ...</td>\n",
       "      <td>[[@PureBread_, Gonna, buy, a, radar, detector,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@shannon_last Yet you can't have a radar detec...</td>\n",
       "      <td>[[@shannon_last, Yet, you, can't, have, a, rad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Uniden R3 Extreme Long Range Radar Laser Detec...</td>\n",
       "      <td>[[Uniden, R3, Extreme, Long, Range, Radar, Las...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @AnniemuMary: Yeah? Well my boyfriend had a...</td>\n",
       "      <td>[[RT, @AnniemuMary, :, Yeah, ?, Well, my, boyf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Uniden Radar Detector Prices About to Increase...</td>\n",
       "      <td>[[Uniden, Radar, Detector, Prices, About, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>#Hgdo_видеорегистратор автомобильный 2 в 1 рус...</td>\n",
       "      <td>[[#Hgdo_видеорегистратор, автомобильный, 2, в,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @xmicky: Disassembling a coyote S radar det...</td>\n",
       "      <td>[[RT, @xmicky, :, Disassembling, a, coyote, S,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>360 Degree Laser/Radar Detector With Voice Ale...</td>\n",
       "      <td>[[360, Degree, Laser, /, Radar, Detector, With...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@gxrcvi Accidentally doing 70 in a 45?? How th...</td>\n",
       "      <td>[[@gxrcvi, Accidentally, doing, 70, in, a, 45,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>2016 @Porsche GT3 RS now available for purchas...</td>\n",
       "      <td>[[2016, @Porsche, GT3, RS, now, available, for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@ABC I'm sure no politicians get any tickets a...</td>\n",
       "      <td>[[@ABC, I'm, sure, no, politicians, get, any, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Escort Passport S75 Radar Detector w/ BSM Filt...</td>\n",
       "      <td>[[Escort, Passport, S75, Radar, Detector, w, /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>The one thing that lies to me the most is my r...</td>\n",
       "      <td>[[The, one, thing, that, lies, to, me, the, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Just $20.12,  YKT - FD013 V7 High-performance ...</td>\n",
       "      <td>[[Just, $, 20.12, ,, YKT, -, FD013, V7, High-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Special $20.12,  YKT - FD013 V7 High-performan...</td>\n",
       "      <td>[[Special, $, 20.12, ,, YKT, -, FD013, V7, Hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Special $20.12,  YKT - FD013 V7 High-performan...</td>\n",
       "      <td>[[Special, $, 20.12, ,, YKT, -, FD013, V7, Hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Special $20.12,  YKT - FD013 V7 High-performan...</td>\n",
       "      <td>[[Special, $, 20.12, ,, YKT, -, FD013, V7, Hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Only $20.12,  YKT - FD013 V7 High-performance ...</td>\n",
       "      <td>[[Only, $, 20.12, ,, YKT, -, FD013, V7, High-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Just $20.12,  YKT - FD013 V7 High-performance ...</td>\n",
       "      <td>[[Just, $, 20.12, ,, YKT, -, FD013, V7, High-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Only $20.12,  YKT - FD013 V7 High-performance ...</td>\n",
       "      <td>[[Only, $, 20.12, ,, YKT, -, FD013, V7, High-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Special $20.12,  YKT - FD013 V7 High-performan...</td>\n",
       "      <td>[[Special, $, 20.12, ,, YKT, -, FD013, V7, Hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Now $20.12,  YKT - FD013 V7 High-performance R...</td>\n",
       "      <td>[[Now, $, 20.12, ,, YKT, -, FD013, V7, High-pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Special $20.12,  YKT - FD013 V7 High-performan...</td>\n",
       "      <td>[[Special, $, 20.12, ,, YKT, -, FD013, V7, Hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Only $20.12,  YKT - FD013 V7 High-performance ...</td>\n",
       "      <td>[[Only, $, 20.12, ,, YKT, -, FD013, V7, High-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Just $20.12,  YKT - FD013 V7 High-performance ...</td>\n",
       "      <td>[[Just, $, 20.12, ,, YKT, -, FD013, V7, High-p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                               Text  \\\n",
       "0   ca_text  I liked a @YouTube video https://t.co/ZAZIvs2Y...   \n",
       "1   ca_text  RT @talknaboutcars: LISTEN to our new TAC #107...   \n",
       "2   ca_text  I liked a @YouTube video https://t.co/1A6KOLIS...   \n",
       "3   ca_text  This is the type of @SpotsySheriff see please ...   \n",
       "4   ca_text  【car audio U-23】日産 セレナ by サウンドステーション AVカンサイ宝塚店...   \n",
       "5   ca_text  RT @talknaboutcars: LISTEN to our new TAC #107...   \n",
       "6   ca_text  【car audio U-23】日産 セレナ by サウンドステーション AVカンサイ宝塚店...   \n",
       "7   ca_text  LISTEN to our new TAC #107 podcast with @CarKr...   \n",
       "8   ca_text  レスポンス 【car audio U-23】日産 セレナ by サウンドステーション AVカ...   \n",
       "9   ca_text  @Pink_About_it Same idiocracy as plastering PI...   \n",
       "10  ca_text  RT @responsejp: 【car audio U-23】日産 セレナ by サウンド...   \n",
       "11  ca_text  RT @MTXAudio: Choosing the right subwoofer for...   \n",
       "12  ca_text  NEW KICKER 11HS8 8″ 150W Hideaway Car Audio Po...   \n",
       "13  ca_text  RT @responsejp: 【car audio U-23】日産 セレナ by サウンド...   \n",
       "14  ca_text  RT @responsejp: 【car audio U-23】日産 セレナ by サウンド...   \n",
       "15  ca_text  【car audio U-23】日産 セレナ by サウンドステーション AVカンサイ宝塚店...   \n",
       "16  ca_text  @Superlitio Con mi esposo y los hemos escuchad...   \n",
       "17  ca_text  Need a new stereo? Alram or accessories for yo...   \n",
       "18  ca_text  then sends me a 25 minute audio and calls me \"...   \n",
       "19  ca_text  I liked a @YouTube video https://t.co/gfEtWyFH...   \n",
       "20  ca_text  I liked a @YouTube video https://t.co/jN4GG5O3...   \n",
       "21  ca_text  @holly_the_grey @MarciIen Chief Mark Saunders ...   \n",
       "22  ca_text  @mattgallowaycbc Chief Mark Saunders has even ...   \n",
       "23  ca_text  I liked a @YouTube video https://t.co/O84SNxmS...   \n",
       "24  ca_text  @KarenJohnsonTV @johnmaher0 @MarciIen Chief Ma...   \n",
       "25  ca_text  I added a video to a @YouTube playlist https:/...   \n",
       "26  ca_text  【car audio U-23】日産 セレナ by サウンドステーション AVカンサイ宝塚店...   \n",
       "27  ca_text  【car audio U-23】日産 セレナ by サウンドステーション AVカンサイ宝塚店...   \n",
       "28  ca_text  レスポンス 【car audio U-23】日産 セレナ by サウンドステーション AVカ...   \n",
       "29  ca_text  RT @VAULTFestival: CALLING ALL WRITERS! @fever...   \n",
       "..      ...                                                ...   \n",
       "70  rd_text  RT @TJBdeals: Escort Passport S75 Radar Detect...   \n",
       "71  rd_text  BlendMount Radar Detector Mount for Uniden,Sta...   \n",
       "72  rd_text  @steveaustinBSR what gimmick radar detector do...   \n",
       "73  rd_text  I liked a @YouTube video https://t.co/Dn21VWy2...   \n",
       "74  rd_text  360 Degree Radar Detector https://t.co/ODC0t1A...   \n",
       "75  rd_text  @PureBread_ Gonna buy a radar detector and do ...   \n",
       "76  rd_text  @shannon_last Yet you can't have a radar detec...   \n",
       "77  rd_text  Uniden R3 Extreme Long Range Radar Laser Detec...   \n",
       "78  rd_text  RT @AnniemuMary: Yeah? Well my boyfriend had a...   \n",
       "79  rd_text  Uniden Radar Detector Prices About to Increase...   \n",
       "80  rd_text  #Hgdo_видеорегистратор автомобильный 2 в 1 рус...   \n",
       "81  rd_text  RT @xmicky: Disassembling a coyote S radar det...   \n",
       "82  rd_text  360 Degree Laser/Radar Detector With Voice Ale...   \n",
       "83  rd_text  @gxrcvi Accidentally doing 70 in a 45?? How th...   \n",
       "84  rd_text  2016 @Porsche GT3 RS now available for purchas...   \n",
       "85  rd_text  @ABC I'm sure no politicians get any tickets a...   \n",
       "86  rd_text  Escort Passport S75 Radar Detector w/ BSM Filt...   \n",
       "87  rd_text  The one thing that lies to me the most is my r...   \n",
       "88  rd_text  Just $20.12,  YKT - FD013 V7 High-performance ...   \n",
       "89  rd_text  Special $20.12,  YKT - FD013 V7 High-performan...   \n",
       "90  rd_text  Special $20.12,  YKT - FD013 V7 High-performan...   \n",
       "91  rd_text  Special $20.12,  YKT - FD013 V7 High-performan...   \n",
       "92  rd_text  Only $20.12,  YKT - FD013 V7 High-performance ...   \n",
       "93  rd_text  Just $20.12,  YKT - FD013 V7 High-performance ...   \n",
       "94  rd_text  Only $20.12,  YKT - FD013 V7 High-performance ...   \n",
       "95  rd_text  Special $20.12,  YKT - FD013 V7 High-performan...   \n",
       "96  rd_text  Now $20.12,  YKT - FD013 V7 High-performance R...   \n",
       "97  rd_text  Special $20.12,  YKT - FD013 V7 High-performan...   \n",
       "98  rd_text  Only $20.12,  YKT - FD013 V7 High-performance ...   \n",
       "99  rd_text  Just $20.12,  YKT - FD013 V7 High-performance ...   \n",
       "\n",
       "                                               tokens  \n",
       "0   [[I, liked, a, @YouTube, video, https://t.co/Z...  \n",
       "1   [[RT, @talknaboutcars, :, LISTEN, to, our, new...  \n",
       "2   [[I, liked, a, @YouTube, video, https://t.co/1...  \n",
       "3   [[This, is, the, type, of, @SpotsySheriff, see...  \n",
       "4   [[【, car, audio, U, -, 23, 】, 日産, セレナ, by, サウン...  \n",
       "5   [[RT, @talknaboutcars, :, LISTEN, to, our, new...  \n",
       "6   [[【, car, audio, U, -, 23, 】, 日産, セレナ, by, サウン...  \n",
       "7   [[LISTEN, to, our, new, TAC, #107, podcast, wi...  \n",
       "8   [[レスポンス, 【, car, audio, U, -, 23, 】, 日産, セレナ, ...  \n",
       "9   [[@Pink_About_it, Same, idiocracy, as, plaster...  \n",
       "10  [[RT, @responsejp, :, 【, car, audio, U, -, 23,...  \n",
       "11  [[RT, @MTXAudio, :, Choosing, the, right, subw...  \n",
       "12  [[NEW, KICKER, 11, HS8, 8, ″, 150, W, Hideaway...  \n",
       "13  [[RT, @responsejp, :, 【, car, audio, U, -, 23,...  \n",
       "14  [[RT, @responsejp, :, 【, car, audio, U, -, 23,...  \n",
       "15  [[【, car, audio, U, -, 23, 】, 日産, セレナ, by, サウン...  \n",
       "16  [[@Superlitio, Con, mi, esposo, y, los, hemos,...  \n",
       "17  [[Need, a, new, stereo, ?, Alram, or, accessor...  \n",
       "18  [[then, sends, me, a, 25, minute, audio, and, ...  \n",
       "19  [[I, liked, a, @YouTube, video, https://t.co/g...  \n",
       "20  [[I, liked, a, @YouTube, video, https://t.co/j...  \n",
       "21  [[@holly_the_grey, @MarciIen, Chief, Mark, Sau...  \n",
       "22  [[@mattgallowaycbc, Chief, Mark, Saunders, has...  \n",
       "23  [[I, liked, a, @YouTube, video, https://t.co/O...  \n",
       "24  [[@KarenJohnsonTV, @johnmaher0, @MarciIen, Chi...  \n",
       "25  [[I, added, a, video, to, a, @YouTube, playlis...  \n",
       "26  [[【, car, audio, U, -, 23, 】, 日産, セレナ, by, サウン...  \n",
       "27  [[【, car, audio, U, -, 23, 】, 日産, セレナ, by, サウン...  \n",
       "28  [[レスポンス, 【, car, audio, U, -, 23, 】, 日産, セレナ, ...  \n",
       "29  [[RT, @VAULTFestival, :, CALLING, ALL, WRITERS...  \n",
       "..                                                ...  \n",
       "70  [[RT, @TJBdeals, :, Escort, Passport, S75, Rad...  \n",
       "71  [[BlendMount, Radar, Detector, Mount, for, Uni...  \n",
       "72  [[@steveaustinBSR, what, gimmick, radar, detec...  \n",
       "73  [[I, liked, a, @YouTube, video, https://t.co/D...  \n",
       "74  [[360, Degree, Radar, Detector, https://t.co/O...  \n",
       "75  [[@PureBread_, Gonna, buy, a, radar, detector,...  \n",
       "76  [[@shannon_last, Yet, you, can't, have, a, rad...  \n",
       "77  [[Uniden, R3, Extreme, Long, Range, Radar, Las...  \n",
       "78  [[RT, @AnniemuMary, :, Yeah, ?, Well, my, boyf...  \n",
       "79  [[Uniden, Radar, Detector, Prices, About, to, ...  \n",
       "80  [[#Hgdo_видеорегистратор, автомобильный, 2, в,...  \n",
       "81  [[RT, @xmicky, :, Disassembling, a, coyote, S,...  \n",
       "82  [[360, Degree, Laser, /, Radar, Detector, With...  \n",
       "83  [[@gxrcvi, Accidentally, doing, 70, in, a, 45,...  \n",
       "84  [[2016, @Porsche, GT3, RS, now, available, for...  \n",
       "85  [[@ABC, I'm, sure, no, politicians, get, any, ...  \n",
       "86  [[Escort, Passport, S75, Radar, Detector, w, /...  \n",
       "87  [[The, one, thing, that, lies, to, me, the, mo...  \n",
       "88  [[Just, $, 20.12, ,, YKT, -, FD013, V7, High-p...  \n",
       "89  [[Special, $, 20.12, ,, YKT, -, FD013, V7, Hig...  \n",
       "90  [[Special, $, 20.12, ,, YKT, -, FD013, V7, Hig...  \n",
       "91  [[Special, $, 20.12, ,, YKT, -, FD013, V7, Hig...  \n",
       "92  [[Only, $, 20.12, ,, YKT, -, FD013, V7, High-p...  \n",
       "93  [[Just, $, 20.12, ,, YKT, -, FD013, V7, High-p...  \n",
       "94  [[Only, $, 20.12, ,, YKT, -, FD013, V7, High-p...  \n",
       "95  [[Special, $, 20.12, ,, YKT, -, FD013, V7, Hig...  \n",
       "96  [[Now, $, 20.12, ,, YKT, -, FD013, V7, High-pe...  \n",
       "97  [[Special, $, 20.12, ,, YKT, -, FD013, V7, Hig...  \n",
       "98  [[Only, $, 20.12, ,, YKT, -, FD013, V7, High-p...  \n",
       "99  [[Just, $, 20.12, ,, YKT, -, FD013, V7, High-p...  \n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for row in all_texts.Text:\n",
    "    words = []\n",
    "    words.append(preprocess(row))\n",
    "    tokens.append(words)    \n",
    "    \n",
    "\n",
    "all_texts['tokens'] = tokens\n",
    "all_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tweet = unicode(tweet.decode('utf-8').lower())\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tokens = filter(lambda t: not t.startswith('@'), tokens)\n",
    "        tokens = filter(lambda t: not t.startswith('#'), tokens)\n",
    "        tokens = filter(lambda t: not t.startswith('http'), tokens)\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def postprocess(data, n=1000000):\n",
    "    data = data.head(n)\n",
    "    data['tokens'] = data['Text'].map(tokenize)\n",
    "    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "data = postprocess(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts.Text = str(all_texts.Text)\n",
    "all_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "# Utility function to clean text.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # Getting rid of unneccesary charactersLo\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "       \n",
    "    # Get rid of extra whitespace.\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "for text in all_texts.Text:\n",
    "    all_texts['cleaned_text'] = text_cleaner(text)\n",
    "\n",
    "#text_cleaner(ca_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Unsupervised Neural Network for Sentiment Classification of Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1492\n"
     ]
    }
   ],
   "source": [
    "# Using tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=1, # drop words that occur in more than 10% of the paragraphs\n",
    "                             min_df=0, \n",
    "                             stop_words='english', \n",
    "                             lowercase=False, #convert everything to lower case \n",
    "                             use_idf=True,#inverse document frequencies for weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "results_tfidf=vectorizer.fit_transform(all_texts.Text)\n",
    "print(\"Number of features: %d\" % results_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "results_lsa = lsa.fit_transform(results_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.83149127e-02,  -8.92691192e-04,  -1.93054373e-01, ...,\n",
       "          1.65000718e-01,  -7.70819319e-02,  -1.97944486e-01],\n",
       "       [  7.02783198e-03,   1.09692983e-01,   8.10578137e-02, ...,\n",
       "          9.70030035e-03,   1.10885836e-01,  -1.26836343e-02],\n",
       "       [ -8.70833891e-03,   6.60872201e-02,  -4.46967191e-02, ...,\n",
       "         -1.34022889e-01,  -8.97888714e-04,  -5.96427076e-02],\n",
       "       ..., \n",
       "       [ -3.69604389e-02,   4.29176478e-02,   8.71173012e-03, ...,\n",
       "          1.08913419e-01,  -1.32533237e-01,  -6.74673319e-02],\n",
       "       [ -1.50121248e-01,   8.81365821e-03,  -1.17106476e-01, ...,\n",
       "         -8.93585935e-02,   3.83542442e-02,  -4.14828339e-02],\n",
       "       [ -1.64723699e-04,   5.26535976e-02,  -4.18593683e-02, ...,\n",
       "          8.67506262e-02,   9.70889168e-02,  -3.34390346e-02]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_res = results_lsa.reshape(300,-1)\n",
    "lsa_df = pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Category = all_texts.Category\n",
    "Text = all_texts.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate a non-NDFrame object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-438b157a0a70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mText\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults_lsa\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    204\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m                        copy=copy)\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobjs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot concatenate a non-NDFrame object\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[1;31m# consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot concatenate a non-NDFrame object"
     ]
    }
   ],
   "source": [
    "frames = [Category, Text, results_lsa]\n",
    "data2 = pd.concat(frames, axis = 1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using k-means to cluster together authors\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=2, random_state=42).fit_predict(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupings = pd.DataFrame(y_pred, columns = ['Group'])\n",
    "groupings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Supervised Neural Network for Predicting Rises in Popularity\n",
    "\n",
    "The combined data from all techniques will be used as the features for a Supervised Neural Network that will predict the rise in popularity of a product over a six month period. The predictions will be evaluated against the rise in popularity of the product(s) via Google Trends results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to download Google Trends Data\n",
    "\n",
    "# x is the result of the sentiment analyses\n",
    "\n",
    "# y is whether the product rose in popularity in the last six months on Google Trends Data\n",
    "\n",
    "# Modify paramaters to fit text classification with two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, activation='relu', input_shape=(3072,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# Output shape should be equal to the number of classes (10)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = 64, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, verbose=False)\n",
    "print('Test Loss: ', score[0])\n",
    "print('Test Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII. Appendix\n",
    "\n",
    "Google PageRank Description (from Google Technology page):\n",
    "PageRank Explained\n",
    "PageRank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page’s value. In essence, Google interprets a link from page A to page B as a vote, by page A, for page B. But, Google looks at considerably more than the sheer volume of votes, or links a page receives; for example, it also analyzes the page that casts the vote. Votes cast by pages that are themselves “important” weigh more heavily and help to make other pages “important.” Using these and other factors, Google provides its views on pages’ relative importance.\n",
    "Of course, important pages mean nothing to you if they don’t match your query. So, Google combines PageRank with sophisticated text-matching techniques to find pages that are both important and relevant to your search. Google goes far beyond the number of times a term appears on a page and examines all dozens of aspects of the page’s content (and the content of the pages linking to it) to determine if it’s a good match for your query.\n",
    "\n",
    "Keyword Planner Description (from Google Adwords page):\n",
    "Keyword Planner is a free AdWords tool for new or experienced advertisers that’s like a workshop for building new Search Network campaigns or expanding existing ones. You can search for keyword and ad group ideas, see how a list of keywords might perform, and even create a new keyword list by multiplying several lists of keywords together. Keyword Planner can also help you choose competitive bids and budgets to use with your campaigns.\n",
    "Sources:\n",
    "Facebook: http://graph.facebook.com\n",
    "Google Trends: https://developers.google.com/gdata/docs/directory\n",
    "Google News: https://newsapi.org/s/google-news-api\n",
    "Keyword Planner: https://adwords.google.com/ko/KeywordPlanner/Home?sourceid=awo&__u=3050284228&__c=9347440984&authuser=0#search\n",
    "Twitter: https://developer.twitter.com/en/docs/basics/getting-started\n",
    "\n",
    "Links to AliExpress items: \n",
    "\n",
    "https://www.aliexpress.com/item/2pcs-New-Black-Retevis-RT628-Portable-radio-Walkie-Talkie-sets-0-5W-8CH-UHF-Europe-Frequency/32407070024.html?spm=2114.search0104.3.27.383641a8WAVsOz&ws_ab_test=searchweb0_0,searchweb201602_3_10152_10151_10065_10344_10130_10068_10324_10342_10547_10325_10343_10546_10340_10548_10341_10545_10084_10083_10618_10307_10313_10059_10534_100031_10629_10103_10626_10625_10624_10623_10622_10621_10620_10142,searchweb201603_25,ppcSwitch_5&algo_expid=c512be04-3611-4608-90ee-6e55891cae81-6&algo_pvid=c512be04-3611-4608-90ee-6e55891cae81&priceBeautifyAB=0\n",
    "\n",
    "https://www.aliexpress.com/item/4012B-4-1-inch-1-Din-Car-Radio-Auto-Audio-Stereo-FM-Bluetooth-2-0-Support/32838433123.html?spm=2114.search0104.3.25.6529687a4mQ9YS&ws_ab_test=searchweb0_0,searchweb201602_3_10152_10151_10065_10344_10130_10068_10324_10342_10547_10325_10343_10546_10340_10548_10341_10545_10084_10083_10618_10307_10313_10059_10534_100031_10629_10103_10626_10625_10624_10623_10622_10621_10620_10142,searchweb201603_25,ppcSwitch_5&algo_expid=78926112-aa75-459d-888f-2b887bd13743-3&algo_pvid=78926112-aa75-459d-888f-2b887bd13743&priceBeautifyAB=0\n",
    "\n",
    "https://www.aliexpress.com/item/Clear-Stock-Excelvan-E8-Car-Radar-Detector-360-Degree-Speed-Safety-Anti-Police-Scanning-Advanced-Voice/32839841753.html?spm=2114.search0104.3.1.1bc42b2a9yCeVF&ws_ab_test=searchweb0_0,searchweb201602_3_10152_10151_10065_10344_10130_10068_10324_10342_10547_10325_10343_10546_10340_10548_10341_10545_10084_10083_10618_10307_10313_10059_10534_100031_10629_10103_10626_10625_10624_10623_10622_10621_10620_10142,searchweb201603_25,ppcSwitch_5&algo_expid=9a63780f-134d-4690-9830-ebf6c5fff3aa-0&algo_pvid=9a63780f-134d-4690-9830-ebf6c5fff3aa&priceBeautifyAB=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra code that is not yet implemented:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#Import the necessary modules\n",
    "import datetime\n",
    "from newsapi import NewsApiClient\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Matthew Kennedy's API Key, business will need to register for their own api and place it below (registration is free)\n",
    "newsapi = NewsApiClient(api_key='2613ce5e838a464b814b7d5b4c2e6bf8')\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date = now.day\n",
    "\n",
    "keyword_list = []\n",
    "\n",
    "####################################################################\n",
    "########## Add keywords from Keyword Planner to keyword_list########\n",
    "####################################################################\n",
    "\n",
    "all_articles = newsapi.get_everything(q=[keyword_list]\n",
    "                                     #, from_paramater = '2018-01-08' #this can be changed to any date\n",
    "                                     , to = date\n",
    "                                     , language = 'en'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "data = all_articles['articles']\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# The Json is a list of dictionary values. This code will store it to a DataFrame.\n",
    "results = {'Description': [], 'Publish Date': [], 'URL': []}\n",
    "columns = results.keys()\n",
    "df_data = pd.DataFrame(data=results, columns=columns)\n",
    "\n",
    "for entry in data:\n",
    "    #print(data[entry]['description'])\n",
    "    description = entry['description']\n",
    "    date = entry['publishedAt']\n",
    "    url = entry['url']\n",
    "    results = {'Description':[description], 'Publish Date':[date], 'URL': [url]}\n",
    "    df_data = df_data.append(pd.DataFrame(data=results, columns=results.keys()), ignore_index = False)\n",
    "    \n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "############################################################################\n",
    "###### Now, create a crawler to crawl the information of each url ##########\n",
    "############################################################################\n",
    "\n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# Make a list for the urls to be stored into from the dataframe\n",
    "url_list = []\n",
    "for entry in df_data['URL']:\n",
    "    url_list.append(entry)\n",
    "    \n",
    "#print(url_list)\n",
    "\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = url_list\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        # for article in response.xpath('//article'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                'everything': response.xpath('//text()').extract()\n",
    "                #'all': response.xpath('/descendant-or-self::node()').extract()\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                #'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                #'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                #'text': article.xpath('//*[@id=\"post-669657\"]/div[3]').extract()\n",
    "                #,'body': article.xpath('//*[@id=\"single-post\"]').extract()\n",
    "                #'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'firstpage4.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "######## This doesn't stop running because it is a stream and it stays open. \n",
    "######## Cell after this one downloads the stream, but it is run through cmd line\n",
    "\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    " \n",
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('python.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    " \n",
    "\n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=['Car Audio', 'Walkie Talkies', 'Radar Detector'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# To run this code, first edit config.py with your configuration, then:\n",
    "#\n",
    "# mkdir data\n",
    "# python twitter_stream_download.py -q apple -d data\n",
    "# \n",
    "# It will produce the list of tweets for the query \"apple\" \n",
    "# in the file data/stream_apple.json\n",
    "\n",
    "import tweepy\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import time\n",
    "import argparse\n",
    "import string\n",
    "import config\n",
    "import json\n",
    "\n",
    "def get_parser():\n",
    "    \"\"\"Get parser for command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Twitter Downloader\")\n",
    "    parser.add_argument(\"-q\",\n",
    "                        \"--query\",\n",
    "                        dest=\"query\",\n",
    "                        help=\"Query/Filter\",\n",
    "                        default='-')\n",
    "    parser.add_argument(\"-d\",\n",
    "                        \"--data-dir\",\n",
    "                        dest=\"data_dir\",\n",
    "                        help=\"Output/Data Directory\")\n",
    "    return parser\n",
    "\n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    \"\"\"Custom StreamListener for streaming data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, query):\n",
    "        query_fname = format_filename(query)\n",
    "        self.outfile = \"%s/stream_%s.json\" % (data_dir, query_fname)\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open(self.outfile, 'a') as f:\n",
    "                f.write(data)\n",
    "                print(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "            time.sleep(5)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "\n",
    "\n",
    "def format_filename(fname):\n",
    "    \"\"\"Convert file name into a safe string.\n",
    "    Arguments:\n",
    "        fname -- the file name to convert\n",
    "    Return:\n",
    "        String -- converted file name\n",
    "    \"\"\"\n",
    "    return ''.join(convert_valid(one_char) for one_char in fname)\n",
    "\n",
    "\n",
    "def convert_valid(one_char):\n",
    "    \"\"\"Convert a character into '_' if invalid.\n",
    "    Arguments:\n",
    "        one_char -- the char to convert\n",
    "    Return:\n",
    "        Character -- converted char\n",
    "    \"\"\"\n",
    "    valid_chars = \"-_.%s%s\" % (string.ascii_letters, string.digits)\n",
    "    if one_char in valid_chars:\n",
    "        return one_char\n",
    "    else:\n",
    "        return '_'\n",
    "\n",
    "@classmethod\n",
    "def parse(cls, api, raw):\n",
    "    status = cls.first_parse(api, raw)\n",
    "    setattr(status, 'json', json.dumps(raw))\n",
    "    return status\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = get_parser()\n",
    "    args = parser.parse_args()\n",
    "    auth = OAuthHandler(config.consumer_key, config.consumer_secret)\n",
    "    auth.set_access_token(config.access_token, config.access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    twitter_stream = Stream(auth, MyListener(args.data_dir, args.query))\n",
    "    twitter_stream.filter(track=[args.query])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will send a request for \n",
    "\n",
    "curl -X POST \"https://api.twitter.com/1.1/tweets/search/:product/:label.json\" -d '{\"query\":\"Car Audio, Walkie Talkies, Radar Detector\",\"maxResults\":\"500\"}' -H \"Authorization: 963889537842851841-FGZVIMTpgCS760gVa3xNu9okSg8hB0j, obZiBaCujDaDzgT50s3q5fuKUTkvQuouQ57wUfl1YH2us\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying something else\n",
    "class MyModelParser(tweepy.parsers.ModelParser):\n",
    "    def parse(self, method, payload:\n",
    "              result = super(MyModelparser, self).parse(method, payload)\n",
    "              result._payload = json.loads(payload)\n",
    "api = tweepy.API(auth, parser=MyModelParser())\n",
    "results = api.search(q='Car Audio', lan='en')\n",
    "              \n",
    "for s in results._payload:\n",
    "              print(json.dumps(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "walkie_talkies_json = api.search(q='Walkie Talkies', lan='en') \n",
    "# Optional paramters: [, locale][, rpp][, page][, since_id][, geocode][, show_user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do we need to hit Amazon and Ebay APIs to compare prices vs our manufacturer?\n",
    "\n",
    "############### Yup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for text in all_texts:\n",
    "\n",
    "    results = keras.preprocessing.text.Tokenizer(ca_text,\n",
    "                                   num_words=None,\n",
    "                                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,\n",
    "                                   split=\" \",\n",
    "                                   char_level=False,\n",
    "                                   oov_token=None)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Example from https://github.com/keras-team/keras/blob/master/examples/imdb_bidirectional_lstm.py\n",
    "\n",
    "'''Trains a Bidirectional LSTM on the IMDB sentiment classification task.\n",
    "Output after 4 epochs on CPU: ~0.8146\n",
    "Time per epoch on CPU (Core i7): ~150s.\n",
    "'''\n",
    "\n",
    "########################################################################\n",
    "########### LOOK INTO USING GPU TO SAVE ON CPU LOAD ####################\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import keras\n",
    "from keras.preprocessing import text\n",
    "\n",
    "results = []\n",
    "\n",
    "for text in all_texts:\n",
    "    results = keras.preprocessing.text.text_to_word_sequence(text,\n",
    "                                               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                               lower=True,\n",
    "                                               split=\" \")\n",
    "    \n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
