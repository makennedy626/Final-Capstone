{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TODO: Read 6.1 Time Series and 6.4 Advanced NLP\n",
    "\n",
    "# Abstract\n",
    "Abstract will be written after all analyses are complete.\n",
    "For a copy of the proposal, visit \n",
    "\n",
    "# Table of Contents\n",
    "## I. Objective\n",
    "## II. Data Collection\n",
    "## III. Scraping\n",
    "## IV. Unsupervised Neural Networks on Scraped Data to Generate Sentiment Analysis\n",
    "## V. Supervised Neural Networks to Predict Rises in Popularity\n",
    "## VI. Results\n",
    "## VII. Conclusion\n",
    "## VIII. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Objective\n",
    "\n",
    "What is the Problem You Are Attempting to Solve?\n",
    "\tWith the internet becoming increasingly involved in the average person‚Äôs daily life, many companies have taken to ecommerce, which is the electronic sale of goods through the internet. For an entrepreneur to break into the ecommerce market or for a large company to add their sources of revenue, a niche market must be found.\n",
    "\tFor the sake of clarity in this project, a niche market will be defined using this Wikipedia description:\n",
    "A niche market is the subset of the market on which a specific product is focused. The market niche defines as the product features aimed at satisfying specific market needs, as well as the price range, production quality, and the demographics that is intended to impact.\n",
    "The niche market is highly specialized, and aiming to survive among the competition from numerous super companies. Even established companies create products for different niches, for example, Hewlett-Packard has all-in-one machines for printing, scanning, and faxing targeted for the home office niche while at the same time having separate machines with one of these functions for big businesses.\n",
    "With this definition in mind, the problem will be defined as, ‚ÄúWith competition saturating the ecommerce market, what is the best niche market for a business or entrepreneur to expand into?‚Äù\n",
    "\tTo narrow the scope of this project, the focus will be on electronics appliances. \n",
    "How is Your Solution Valuable?\n",
    "The result of this project will be a solution that exceeds the performance of other niche market research tools in terms of finding the ideal niche market in a given sector. The ideal niche market is one that has some or all of the following criteria:\n",
    "‚Ä¢\thigh search volume\n",
    "‚Ä¢\tproducts that are difficult for consumers to find locally\n",
    "‚Ä¢\tsolves a common problem\n",
    "‚Ä¢\thas a customer base that is passionate\n",
    "‚Ä¢\tis not overrun with websites that have a Google PageRank of five or higher\n",
    "‚Ä¢\thas accessible suppliers.\n",
    "o\tGoogle PageRank is explained in the Appendix\n",
    "The solution will drastically reduce the number of hours and amount of wages spent for market research and will be a valuable tool for anyone in ecommerce, from large companies to individual entrepreneurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data Collection\n",
    "\n",
    "What is Your Data Source and How Will You Access It?\n",
    "Links to sources in Appendix, Access explained in Techniques Section Below\n",
    "Data for Machine Learning Models - Facebook and Twitter APIs\n",
    "Competition Research - PageRank Status for Chrome\n",
    "Validation - Google Trends\n",
    "\n",
    "\n",
    "### What Techniques From the Course Do You Anticipate Using?\n",
    "\n",
    "\n",
    "The project will begin with a broad search of keywords related to technology appliances on Keyword Planner (see appendix for description). For example, a keyword search was conducted for \"Consumer Electronics Appliances\" that returned seven hundred keywords with information on their average monthly searches and their level of competition (Low, Medium, or High). This information will be downloaded and filtered to generate a list of keywords that have a high number of average monthly searches and a low rank of competition. Keyword Planner measures competition by evaluating the number of advertisers that showed on each keyword relative to all keywords across Google. \n",
    "\n",
    "\n",
    "Twitter and Facebook will then be scraped to gather posts that reference the products in the keyword list. With this data, an Unsupervised Neural Network will perform Natural Language Processing to generate sentiment analysis of the products.\n",
    "\n",
    "\n",
    "The combined data from all techniques will be used as the features for a \n",
    "Supervised Neural Network that will predict the rise in popularity of a product over a six month period.\n",
    "The predictions will be evaluated against the rise in popularity of the product(s) via Google Trends results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Planning Results\n",
    "\n",
    "The Keyword Planner returned over seven hundred keywords under \"Consumer Electronics.\" The keywords were filtered to a list that had Average Monthly Searches between ten thousand and one hundred thousand and a Competition value under .70. The resulting list contained forty-eight keywords. After ruling out any brand names and similar keywords, this project will proceed with three of the items: Walkie Talkies, Radar Detectors, and Car Audio systems. An AliExpress search was conducted to verify the existence of manufacturers that were in a tolerable price range and able to ship one-piece orders. The tolerable price range was as follows: Walkie Talkies < \\$25, Radar Detectors < \\$15, and Car Audio < \\$45.\n",
    "\n",
    "Links to these items are in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Scraping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape Twitter's API, I made a new twitter account and registered an application under it to be given the necessary keys and access tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "##### Mining Twitter API #############\n",
    "######################################\n",
    "\n",
    "# https://developer.twitter.com/en/docs/tweets/filter-realtime/api-reference/post-statuses-filter.html\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = '8AZsgftPqH9dbyLfc5IFWo39v'\n",
    "consumer_secret = 'MuMAVS7rIEYa0dsyauTOeJMkjRgmU5NEvG2uzyOtWItJLli48u'\n",
    "access_token = '963889537842851841-FGZVIMTpgCS760gVa3xNu9okSg8hB0j'\n",
    "access_secret = 'obZiBaCujDaDzgT50s3q5fuKUTkvQuouQ57wUfl1YH2us'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "car_audio_json = [status for status in api.search(q='Car Audio', lan='en')] \n",
    "# Optional paramters: [, locale][, rpp][, page][, since_id][, geocode][, show_user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Cubs I was in my car listening to the audio on my phone when Wilson smacked that one, and I missed my L turn thx to that happy distraction üíô Totally worth it. #ThatsCub\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Car Audio'\n",
    "\n",
    "car_audio_tweets = [status for status in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(100)]\n",
    "print(car_audio_tweets[0].full_text)\n",
    "len(car_audio_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Cubs I was in my car listening to the audio on my phone when Wilson smacked that one, and I missed my L turn thx to that happy distraction üíô Totally worth it. #ThatsCub'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_text = []\n",
    "for text in car_audio_tweets:\n",
    "    ca_text.append(text.full_text)\n",
    "ca_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Category, Text]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the lists of texts into a DataFrame\n",
    "import pandas as pd\n",
    "texts = {'Category':[], 'Text':[]}\n",
    "columns = texts.keys()\n",
    "all_texts = pd.DataFrame(data=texts, columns=columns)\n",
    "all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = {'Category':'ca_text', 'Text':ca_text}\n",
    "all_texts = all_texts.append(pd.DataFrame(data=texts, columns=texts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it allowed to use Walkie Talkies in France? | Europe Forum | Fodor's Travel Talk Forums https://t.co/VON7jxLpHR\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Walkie Talkies'\n",
    "\n",
    "walkie_talkie_tweets = [status for status in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(100)]\n",
    "print(walkie_talkie_tweets[0].full_text)\n",
    "len(walkie_talkie_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Is it allowed to use Walkie Talkies in France? | Europe Forum | Fodor's Travel Talk Forums https://t.co/VON7jxLpHR\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_text = []\n",
    "for text in walkie_talkie_tweets:\n",
    "    wt_text.append(text.full_text)\n",
    "wt_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = {'Category':'wt_text', 'Text':wt_text}\n",
    "all_texts = all_texts.append(pd.DataFrame(data=texts, columns=texts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radar Detector (The Loving Hand Remix) - Darwin Deez\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'radar detector'\n",
    "\n",
    "radar_detector_tweets = [status for status in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(100)]\n",
    "print(radar_detector_tweets[0].full_text)\n",
    "len(radar_detector_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Radar Detector (The Loving Hand Remix) - Darwin Deez'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_text = []\n",
    "for text in radar_detector_tweets:\n",
    "    rd_text.append(text.full_text)\n",
    "rd_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = {'Category':'rd_text', 'Text':rd_text}\n",
    "all_texts = all_texts.append(pd.DataFrame(data=texts, columns=texts.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle the results so that you can run this daily and add to your data collection.\n",
    "\n",
    "# Make it remove duplicate tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alex - the below drop_duplicates code removes all rows and only keeps the first row of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame object to csv\n",
    "# Set index=False to prevent the extra column of the original indices\n",
    "# all_texts.to_csv(path_or_buf=r'D:\\GitHub\\Final Capstone\\Ecommerce Data\\Tweets.csv', index=False)\n",
    "\n",
    "# Now that the csv has been created, append it by writing with mode='a'\n",
    "f = 'D:\\GitHub\\Final Capstone\\Ecommerce Data\\Tweets.csv'\n",
    "all_texts.to_csv(f, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original = pd.read_csv('D:\\\\GitHub\\\\Final Capstone\\\\Ecommerce Data\\\\Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>WE GOT IN THE CAR AND MY DAD'S PHONE AUTO CONN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/UyNgZtCY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/JxudunH6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>3?17?????18????Car Audio Club????????Super Hig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                               Text\n",
       "0  ca_text  WE GOT IN THE CAR AND MY DAD'S PHONE AUTO CONN...\n",
       "1  ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...\n",
       "2  ca_text  I liked a @YouTube video https://t.co/UyNgZtCY...\n",
       "3  ca_text  I liked a @YouTube video https://t.co/JxudunH6...\n",
       "4  ca_text  3?17?????18????Car Audio Club????????Super Hig..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Break Point for restarting kernel\n",
    "break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Unsupervised Neural Network for Sentiment Classification of Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below cells until the next markdown are from: http://www.developintelligence.com/blog/2017/06/practical-neural-networks-keras-classifying-yelp-reviews/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    " \n",
    "import json\n",
    " \n",
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5)\n",
    "toytexts = [\"Is is a common word\", \"So is the\", \"the is common\", \"discombobulation is not common\"]\n",
    "tokenizer.fit_on_texts(toytexts)\n",
    "sequences = tokenizer.texts_to_sequences(toytexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = tweets.Text\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=300))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# In line two, we add an Embedding layer. This layer lets the network expand each token to a larger vector, allowing the network to represent words in a meaningful way. We pass 20000 as the first argument, which is the size of our vocabulary (remember, we told the tokenizer to only use the 20 000 most common words earlier), and 128 as the second, which means that each token can be expanded to a vector of size 128. We give it an input_length of 300, which is the length of each of our sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(data, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50265813]\n",
      " [ 0.4973028 ]\n",
      " [ 0.50087929]\n",
      " [ 0.50080127]\n",
      " [ 0.49798906]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.49681658]\n",
      " [ 0.49590829]\n",
      " [ 0.4973028 ]\n",
      " [ 0.50085944]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.49574381]\n",
      " [ 0.50005203]\n",
      " [ 0.4973028 ]\n",
      " [ 0.49813539]\n",
      " [ 0.4973028 ]\n",
      " [ 0.50165164]\n",
      " [ 0.4973028 ]\n",
      " [ 0.500884  ]\n",
      " [ 0.50092459]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.50081927]\n",
      " [ 0.50088549]\n",
      " [ 0.4973028 ]\n",
      " [ 0.50082684]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.49261913]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.49626926]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.4973028 ]\n",
      " [ 0.49651918]\n",
      " [ 0.50082195]\n",
      " [ 0.49943891]\n",
      " [ 0.50122005]\n",
      " [ 0.50182241]\n",
      " [ 0.50165164]\n",
      " [ 0.50115371]\n",
      " [ 0.49929461]\n",
      " [ 0.49811575]\n",
      " [ 0.49909049]\n",
      " [ 0.49721208]\n",
      " [ 0.50217777]\n",
      " [ 0.50165164]\n",
      " [ 0.50165164]\n",
      " [ 0.49379253]\n",
      " [ 0.50165164]\n",
      " [ 0.49765494]\n",
      " [ 0.49990979]\n",
      " [ 0.50165164]\n",
      " [ 0.49785808]\n",
      " [ 0.49634737]\n",
      " [ 0.49721831]\n",
      " [ 0.49673909]\n",
      " [ 0.50047457]\n",
      " [ 0.4942905 ]\n",
      " [ 0.50165164]\n",
      " [ 0.50165164]\n",
      " [ 0.50165164]\n",
      " [ 0.5022521 ]\n",
      " [ 0.49980956]\n",
      " [ 0.50165164]\n",
      " [ 0.50165164]\n",
      " [ 0.49892798]\n",
      " [ 0.50165164]\n",
      " [ 0.49765375]\n",
      " [ 0.50029123]\n",
      " [ 0.50165164]\n",
      " [ 0.49971294]\n",
      " [ 0.49848172]\n",
      " [ 0.49642757]\n",
      " [ 0.50165164]\n",
      " [ 0.50165164]\n",
      " [ 0.50165164]\n",
      " [ 0.50165164]\n",
      " [ 0.50165164]\n",
      " [ 0.49678051]\n",
      " [ 0.49936119]\n",
      " [ 0.49657008]\n",
      " [ 0.49657008]\n",
      " [ 0.49657008]\n",
      " [ 0.49847949]\n",
      " [ 0.49914208]\n",
      " [ 0.49811292]\n",
      " [ 0.49683666]\n",
      " [ 0.4966431 ]\n",
      " [ 0.50603676]\n",
      " [ 0.49811292]\n",
      " [ 0.49811292]\n",
      " [ 0.49786174]\n",
      " [ 0.50045526]\n",
      " [ 0.50279897]\n",
      " [ 0.49848843]\n",
      " [ 0.50231367]\n",
      " [ 0.50104451]\n",
      " [ 0.50110704]\n",
      " [ 0.49710965]\n",
      " [ 0.5000723 ]\n",
      " [ 0.4996343 ]\n",
      " [ 0.49895006]\n",
      " [ 0.49815041]\n",
      " [ 0.50060952]\n",
      " [ 0.49892858]\n",
      " [ 0.49898309]\n",
      " [ 0.49625528]\n",
      " [ 0.49566585]\n",
      " [ 0.49811292]\n",
      " [ 0.50304937]\n",
      " [ 0.49628922]\n",
      " [ 0.50124401]\n",
      " [ 0.49883863]\n",
      " [ 0.49811292]\n",
      " [ 0.50251132]\n",
      " [ 0.49897969]\n",
      " [ 0.50225198]\n",
      " [ 0.49698749]\n",
      " [ 0.49509299]\n",
      " [ 0.50015938]\n",
      " [ 0.50268292]\n",
      " [ 0.49725816]\n",
      " [ 0.49916941]\n",
      " [ 0.50100654]\n",
      " [ 0.4976573 ]\n",
      " [ 0.50153142]\n",
      " [ 0.50227726]\n",
      " [ 0.49811292]\n",
      " [ 0.50159717]\n",
      " [ 0.50305969]\n",
      " [ 0.49882588]\n",
      " [ 0.4987663 ]\n",
      " [ 0.4996343 ]\n",
      " [ 0.49856535]\n",
      " [ 0.49878359]\n",
      " [ 0.49844974]\n",
      " [ 0.50019526]\n",
      " [ 0.50015938]\n",
      " [ 0.49848843]\n",
      " [ 0.50206321]\n",
      " [ 0.49562231]\n",
      " [ 0.49811292]\n",
      " [ 0.49820402]\n",
      " [ 0.49339876]\n",
      " [ 0.49754199]\n",
      " [ 0.49598852]\n",
      " [ 0.49756631]\n",
      " [ 0.50181967]\n",
      " [ 0.50027645]\n",
      " [ 0.50021195]\n",
      " [ 0.50162029]\n",
      " [ 0.49987087]\n",
      " [ 0.49889067]\n",
      " [ 0.50223708]\n",
      " [ 0.4976064 ]\n",
      " [ 0.49697053]\n",
      " [ 0.50046295]\n",
      " [ 0.4996343 ]\n",
      " [ 0.50317734]\n",
      " [ 0.4994486 ]\n",
      " [ 0.4996343 ]\n",
      " [ 0.4941043 ]\n",
      " [ 0.49863485]\n",
      " [ 0.4996343 ]\n",
      " [ 0.4996343 ]\n",
      " [ 0.4996343 ]\n",
      " [ 0.49986431]\n",
      " [ 0.5004496 ]\n",
      " [ 0.50211251]\n",
      " [ 0.50084519]\n",
      " [ 0.49811292]\n",
      " [ 0.4996343 ]\n",
      " [ 0.49512365]\n",
      " [ 0.50009483]\n",
      " [ 0.4963544 ]\n",
      " [ 0.49811292]\n",
      " [ 0.49811292]\n",
      " [ 0.49858382]\n",
      " [ 0.50244743]\n",
      " [ 0.50465298]\n",
      " [ 0.50004697]\n",
      " [ 0.49972764]\n",
      " [ 0.49548203]\n",
      " [ 0.50304383]\n",
      " [ 0.49659488]\n",
      " [ 0.49870795]\n",
      " [ 0.49869925]\n",
      " [ 0.50139511]\n",
      " [ 0.49746647]\n",
      " [ 0.49758971]\n",
      " [ 0.49406841]\n",
      " [ 0.50003779]\n",
      " [ 0.49968168]\n",
      " [ 0.50002563]\n",
      " [ 0.49876583]\n",
      " [ 0.5014627 ]\n",
      " [ 0.49691001]\n",
      " [ 0.49754485]\n",
      " [ 0.50275284]\n",
      " [ 0.49889714]\n",
      " [ 0.49551553]\n",
      " [ 0.50332487]\n",
      " [ 0.49863267]\n",
      " [ 0.49684337]\n",
      " [ 0.49898118]\n",
      " [ 0.49750063]\n",
      " [ 0.49932566]\n",
      " [ 0.50039744]\n",
      " [ 0.49698383]\n",
      " [ 0.49537003]\n",
      " [ 0.49868163]\n",
      " [ 0.49851397]\n",
      " [ 0.49850881]\n",
      " [ 0.49450728]\n",
      " [ 0.49659032]\n",
      " [ 0.49945325]\n",
      " [ 0.50008112]\n",
      " [ 0.49658948]\n",
      " [ 0.49718815]\n",
      " [ 0.49935102]\n",
      " [ 0.49802491]\n",
      " [ 0.5005061 ]\n",
      " [ 0.49830821]\n",
      " [ 0.49931845]\n",
      " [ 0.49669111]\n",
      " [ 0.49653405]\n",
      " [ 0.50338537]\n",
      " [ 0.50340456]\n",
      " [ 0.49895138]\n",
      " [ 0.49685183]\n",
      " [ 0.49935102]\n",
      " [ 0.49895689]\n",
      " [ 0.49806103]\n",
      " [ 0.49938414]\n",
      " [ 0.49543041]\n",
      " [ 0.49563977]\n",
      " [ 0.49763703]\n",
      " [ 0.50044101]\n",
      " [ 0.49825487]\n",
      " [ 0.49851277]\n",
      " [ 0.50023907]\n",
      " [ 0.49293992]\n",
      " [ 0.49563229]\n",
      " [ 0.4996824 ]\n",
      " [ 0.49900517]\n",
      " [ 0.50323099]\n",
      " [ 0.50027478]\n",
      " [ 0.49692672]\n",
      " [ 0.5001201 ]\n",
      " [ 0.49539286]\n",
      " [ 0.49886426]\n",
      " [ 0.49930087]\n",
      " [ 0.50139779]\n",
      " [ 0.50133288]\n",
      " [ 0.50010514]\n",
      " [ 0.4946143 ]\n",
      " [ 0.49926484]\n",
      " [ 0.49693966]\n",
      " [ 0.49564111]\n",
      " [ 0.49824807]\n",
      " [ 0.49483597]\n",
      " [ 0.50108844]\n",
      " [ 0.49843693]\n",
      " [ 0.49930087]\n",
      " [ 0.4986594 ]\n",
      " [ 0.49927163]\n",
      " [ 0.49604601]\n",
      " [ 0.49930087]\n",
      " [ 0.49901152]\n",
      " [ 0.50011557]\n",
      " [ 0.49583089]\n",
      " [ 0.504816  ]\n",
      " [ 0.50014418]\n",
      " [ 0.49957836]\n",
      " [ 0.50150979]\n",
      " [ 0.50275284]\n",
      " [ 0.49849263]\n",
      " [ 0.49758285]\n",
      " [ 0.49897423]\n",
      " [ 0.49556318]\n",
      " [ 0.49835581]\n",
      " [ 0.49579784]\n",
      " [ 0.50018191]\n",
      " [ 0.50165164]\n",
      " [ 0.49457645]\n",
      " [ 0.50126159]\n",
      " [ 0.49803609]\n",
      " [ 0.49842319]\n",
      " [ 0.49710587]\n",
      " [ 0.4912419 ]\n",
      " [ 0.50004983]\n",
      " [ 0.50126159]\n",
      " [ 0.50141543]\n",
      " [ 0.49803412]\n",
      " [ 0.50085086]\n",
      " [ 0.49393937]\n",
      " [ 0.49812463]\n",
      " [ 0.50260818]\n",
      " [ 0.49426964]\n",
      " [ 0.49971941]\n",
      " [ 0.49448058]\n",
      " [ 0.49859858]\n",
      " [ 0.49792251]\n",
      " [ 0.49448758]\n",
      " [ 0.50158083]\n",
      " [ 0.49507323]\n",
      " [ 0.49983996]\n",
      " [ 0.49921143]\n",
      " [ 0.49837121]\n",
      " [ 0.5027141 ]\n",
      " [ 0.496768  ]\n",
      " [ 0.49614751]\n",
      " [ 0.5016343 ]\n",
      " [ 0.49722102]\n",
      " [ 0.49740523]\n",
      " [ 0.49689752]\n",
      " [ 0.49880672]\n",
      " [ 0.49600089]\n",
      " [ 0.496387  ]\n",
      " [ 0.50087613]\n",
      " [ 0.50165164]\n",
      " [ 0.49916145]\n",
      " [ 0.49825847]\n",
      " [ 0.49793398]\n",
      " [ 0.50085866]\n",
      " [ 0.49989381]\n",
      " [ 0.49971715]\n",
      " [ 0.49894556]\n",
      " [ 0.49299914]\n",
      " [ 0.49561441]\n",
      " [ 0.50029141]\n",
      " [ 0.50165641]\n",
      " [ 0.50165963]\n",
      " [ 0.49859005]\n",
      " [ 0.50028753]\n",
      " [ 0.50281066]\n",
      " [ 0.4979426 ]\n",
      " [ 0.49916038]\n",
      " [ 0.49916038]\n",
      " [ 0.49672544]\n",
      " [ 0.49376121]\n",
      " [ 0.5020209 ]\n",
      " [ 0.49940443]\n",
      " [ 0.49656767]\n",
      " [ 0.49936745]\n",
      " [ 0.4983294 ]\n",
      " [ 0.50128871]\n",
      " [ 0.49616006]\n",
      " [ 0.4974817 ]\n",
      " [ 0.49692726]\n",
      " [ 0.49889344]\n",
      " [ 0.49860895]\n",
      " [ 0.50207168]\n",
      " [ 0.49575666]\n",
      " [ 0.50132543]\n",
      " [ 0.49797955]\n",
      " [ 0.50020546]\n",
      " [ 0.50086844]\n",
      " [ 0.49630085]\n",
      " [ 0.49992174]\n",
      " [ 0.50256509]\n",
      " [ 0.4950203 ]\n",
      " [ 0.5003891 ]\n",
      " [ 0.50256509]\n",
      " [ 0.50256509]\n",
      " [ 0.50008404]\n",
      " [ 0.50256509]\n",
      " [ 0.49714547]\n",
      " [ 0.49846822]\n",
      " [ 0.49635783]\n",
      " [ 0.49644819]\n",
      " [ 0.49854785]\n",
      " [ 0.49674827]\n",
      " [ 0.49483418]\n",
      " [ 0.50165164]\n",
      " [ 0.49833295]\n",
      " [ 0.5022673 ]\n",
      " [ 0.49728799]\n",
      " [ 0.49811292]\n",
      " [ 0.49891689]\n",
      " [ 0.50149798]\n",
      " [ 0.49977615]\n",
      " [ 0.49801874]\n",
      " [ 0.49721938]\n",
      " [ 0.49666253]\n",
      " [ 0.50103033]\n",
      " [ 0.50069714]\n",
      " [ 0.498916  ]\n",
      " [ 0.50000459]\n",
      " [ 0.50245243]\n",
      " [ 0.49848843]\n",
      " [ 0.49908674]\n",
      " [ 0.50257552]\n",
      " [ 0.49863669]\n",
      " [ 0.49811292]\n",
      " [ 0.49811587]\n",
      " [ 0.49927756]\n",
      " [ 0.49673152]\n",
      " [ 0.49811292]\n",
      " [ 0.50630105]\n",
      " [ 0.50031036]\n",
      " [ 0.49811292]\n",
      " [ 0.49811918]\n",
      " [ 0.49905437]\n",
      " [ 0.49811292]\n",
      " [ 0.50139725]\n",
      " [ 0.4982377 ]\n",
      " [ 0.49962169]\n",
      " [ 0.49698749]\n",
      " [ 0.50102258]\n",
      " [ 0.4966197 ]\n",
      " [ 0.50073963]\n",
      " [ 0.49811292]\n",
      " [ 0.49998701]\n",
      " [ 0.49811292]\n",
      " [ 0.49877089]\n",
      " [ 0.49887151]\n",
      " [ 0.50490493]\n",
      " [ 0.49914151]\n",
      " [ 0.49961001]\n",
      " [ 0.49868348]\n",
      " [ 0.5026018 ]\n",
      " [ 0.50084245]\n",
      " [ 0.49811292]\n",
      " [ 0.50178367]\n",
      " [ 0.49712604]\n",
      " [ 0.49988267]\n",
      " [ 0.49921718]\n",
      " [ 0.49901369]\n",
      " [ 0.49755612]\n",
      " [ 0.50158781]\n",
      " [ 0.50003958]\n",
      " [ 0.50206417]\n",
      " [ 0.49776727]\n",
      " [ 0.49818781]\n",
      " [ 0.49852771]\n",
      " [ 0.50027984]\n",
      " [ 0.49998605]\n",
      " [ 0.50023001]\n",
      " [ 0.4997274 ]\n",
      " [ 0.50315326]\n",
      " [ 0.501203  ]\n",
      " [ 0.49976897]\n",
      " [ 0.50099075]\n",
      " [ 0.4985238 ]\n",
      " [ 0.50014067]\n",
      " [ 0.49811292]\n",
      " [ 0.49913841]\n",
      " [ 0.4983227 ]\n",
      " [ 0.50150698]\n",
      " [ 0.49451849]\n",
      " [ 0.50046921]\n",
      " [ 0.49811292]\n",
      " [ 0.49904683]\n",
      " [ 0.50023001]\n",
      " [ 0.49566913]\n",
      " [ 0.50023001]\n",
      " [ 0.49692401]\n",
      " [ 0.50024438]\n",
      " [ 0.49753422]\n",
      " [ 0.49748015]\n",
      " [ 0.50019526]\n",
      " [ 0.4968206 ]\n",
      " [ 0.49867198]\n",
      " [ 0.49686602]\n",
      " [ 0.50075835]\n",
      " [ 0.49865288]\n",
      " [ 0.49894407]\n",
      " [ 0.49753073]\n",
      " [ 0.49761119]\n",
      " [ 0.49846053]\n",
      " [ 0.49996138]\n",
      " [ 0.49811292]\n",
      " [ 0.49867362]\n",
      " [ 0.50025576]\n",
      " [ 0.4984912 ]\n",
      " [ 0.49714148]\n",
      " [ 0.50160915]\n",
      " [ 0.4984841 ]\n",
      " [ 0.49947834]\n",
      " [ 0.50023812]\n",
      " [ 0.49869925]\n",
      " [ 0.50287622]\n",
      " [ 0.49868977]\n",
      " [ 0.49897525]\n",
      " [ 0.49471191]\n",
      " [ 0.49471191]\n",
      " [ 0.49471191]\n",
      " [ 0.50111318]\n",
      " [ 0.49471191]\n",
      " [ 0.49471191]\n",
      " [ 0.49680364]\n",
      " [ 0.4992311 ]\n",
      " [ 0.49812415]\n",
      " [ 0.49870092]\n",
      " [ 0.50219363]\n",
      " [ 0.5049929 ]\n",
      " [ 0.495233  ]\n",
      " [ 0.50148797]\n",
      " [ 0.50240088]\n",
      " [ 0.49706206]\n",
      " [ 0.49872023]\n",
      " [ 0.50208235]\n",
      " [ 0.49820793]\n",
      " [ 0.49310225]\n",
      " [ 0.49556974]\n",
      " [ 0.49605983]\n",
      " [ 0.49887297]\n",
      " [ 0.49988118]\n",
      " [ 0.49546319]\n",
      " [ 0.50274539]\n",
      " [ 0.49912795]\n",
      " [ 0.4974502 ]\n",
      " [ 0.49964541]\n",
      " [ 0.49869925]\n",
      " [ 0.49852201]\n",
      " [ 0.49889606]\n",
      " [ 0.50244743]\n",
      " [ 0.50465298]\n",
      " [ 0.50004697]\n",
      " [ 0.49972764]\n",
      " [ 0.49548203]\n",
      " [ 0.50304383]\n",
      " [ 0.49659488]\n",
      " [ 0.49870795]\n",
      " [ 0.49869925]\n",
      " [ 0.50139511]\n",
      " [ 0.49746647]\n",
      " [ 0.49758971]\n",
      " [ 0.49406841]\n",
      " [ 0.50003779]\n",
      " [ 0.49968168]\n",
      " [ 0.50002563]\n",
      " [ 0.49876583]\n",
      " [ 0.5014627 ]\n",
      " [ 0.49691001]\n",
      " [ 0.50275284]\n",
      " [ 0.49889714]\n",
      " [ 0.49551553]\n",
      " [ 0.50332487]\n",
      " [ 0.49684337]\n",
      " [ 0.49898118]\n",
      " [ 0.49750063]\n",
      " [ 0.49932566]\n",
      " [ 0.50039744]\n",
      " [ 0.49698383]\n",
      " [ 0.49537003]\n",
      " [ 0.49868163]\n",
      " [ 0.49851397]\n",
      " [ 0.49850881]\n",
      " [ 0.49450728]\n",
      " [ 0.49659032]\n",
      " [ 0.49945325]\n",
      " [ 0.50008112]\n",
      " [ 0.49658948]\n",
      " [ 0.49718815]\n",
      " [ 0.49935102]\n",
      " [ 0.49802491]\n",
      " [ 0.5005061 ]\n",
      " [ 0.49830821]\n",
      " [ 0.49931845]\n",
      " [ 0.49669111]\n",
      " [ 0.49653405]\n",
      " [ 0.50338537]\n",
      " [ 0.50340456]\n",
      " [ 0.49895138]\n",
      " [ 0.49685183]\n",
      " [ 0.49935102]\n",
      " [ 0.49895689]\n",
      " [ 0.49806103]\n",
      " [ 0.49938414]\n",
      " [ 0.49543041]\n",
      " [ 0.49563977]\n",
      " [ 0.49763703]\n",
      " [ 0.50044101]\n",
      " [ 0.49825487]\n",
      " [ 0.49851277]\n",
      " [ 0.50023907]\n",
      " [ 0.49597877]\n",
      " [ 0.50068855]\n",
      " [ 0.49531388]\n",
      " [ 0.49821764]\n",
      " [ 0.49544293]\n",
      " [ 0.49762806]\n",
      " [ 0.50191337]\n",
      " [ 0.49620196]\n",
      " [ 0.49695781]\n",
      " [ 0.50128543]\n",
      " [ 0.50200325]\n",
      " [ 0.49711883]\n",
      " [ 0.49913201]\n",
      " [ 0.49584308]\n",
      " [ 0.49544293]\n",
      " [ 0.49850336]\n",
      " [ 0.49544293]\n",
      " [ 0.49544293]\n",
      " [ 0.50126159]\n",
      " [ 0.49544293]\n",
      " [ 0.49750352]\n",
      " [ 0.49782652]\n",
      " [ 0.49952266]\n",
      " [ 0.50092465]\n",
      " [ 0.49544293]\n",
      " [ 0.49592143]\n",
      " [ 0.49505267]\n",
      " [ 0.49544293]\n",
      " [ 0.497917  ]\n",
      " [ 0.49544293]\n",
      " [ 0.50202262]\n",
      " [ 0.49544293]\n",
      " [ 0.5008325 ]\n",
      " [ 0.49544293]\n",
      " [ 0.49544293]\n",
      " [ 0.49544293]\n",
      " [ 0.49544293]\n",
      " [ 0.4998987 ]\n",
      " [ 0.50006288]\n",
      " [ 0.49973026]\n",
      " [ 0.49859005]\n",
      " [ 0.50018853]\n",
      " [ 0.49502975]\n",
      " [ 0.49598357]\n",
      " [ 0.49722403]\n",
      " [ 0.4976348 ]\n",
      " [ 0.49629813]\n",
      " [ 0.49665335]\n",
      " [ 0.50034279]\n",
      " [ 0.50165164]\n",
      " [ 0.4974843 ]\n",
      " [ 0.49358851]\n",
      " [ 0.49879593]\n",
      " [ 0.50149387]\n",
      " [ 0.49684373]\n",
      " [ 0.49996448]\n",
      " [ 0.49991211]\n",
      " [ 0.49705404]\n",
      " [ 0.4969807 ]\n",
      " [ 0.49946234]\n",
      " [ 0.49947685]\n",
      " [ 0.4962689 ]\n",
      " [ 0.50153106]\n",
      " [ 0.49478537]\n",
      " [ 0.49972048]\n",
      " [ 0.49776396]\n",
      " [ 0.49949732]\n",
      " [ 0.4949387 ]\n",
      " [ 0.49674839]\n",
      " [ 0.49853459]\n",
      " [ 0.4964188 ]\n",
      " [ 0.50223976]\n",
      " [ 0.49926698]\n",
      " [ 0.50001729]\n",
      " [ 0.49891749]\n",
      " [ 0.49833912]\n",
      " [ 0.49495265]\n",
      " [ 0.49624106]\n",
      " [ 0.49789342]\n",
      " [ 0.49918938]\n",
      " [ 0.50029123]\n",
      " [ 0.50007182]\n",
      " [ 0.50171089]\n",
      " [ 0.49726942]\n",
      " [ 0.49904275]\n",
      " [ 0.49610502]\n",
      " [ 0.4975701 ]\n",
      " [ 0.49918783]\n",
      " [ 0.49694303]\n",
      " [ 0.50140274]\n",
      " [ 0.49948418]\n",
      " [ 0.50050664]\n",
      " [ 0.4963035 ]\n",
      " [ 0.50147963]\n",
      " [ 0.50169623]\n",
      " [ 0.49948418]\n",
      " [ 0.49752417]\n",
      " [ 0.49948418]\n",
      " [ 0.49918854]\n",
      " [ 0.49883702]\n",
      " [ 0.50314564]\n",
      " [ 0.49821818]\n",
      " [ 0.50023001]\n",
      " [ 0.50186741]\n",
      " [ 0.4984296 ]\n",
      " [ 0.49668393]\n",
      " [ 0.49913371]\n",
      " [ 0.49929208]\n",
      " [ 0.49851596]\n",
      " [ 0.49910635]\n",
      " [ 0.50326514]\n",
      " [ 0.50225979]\n",
      " [ 0.50312132]\n",
      " [ 0.50048119]\n",
      " [ 0.50146633]\n",
      " [ 0.49830949]\n",
      " [ 0.50160402]\n",
      " [ 0.49748015]\n",
      " [ 0.49836349]\n",
      " [ 0.50010163]\n",
      " [ 0.49824724]\n",
      " [ 0.49996412]\n",
      " [ 0.49616945]\n",
      " [ 0.49824905]\n",
      " [ 0.49679157]\n",
      " [ 0.50286722]\n",
      " [ 0.49517453]\n",
      " [ 0.49959156]\n",
      " [ 0.49979267]\n",
      " [ 0.4986105 ]\n",
      " [ 0.50068855]\n",
      " [ 0.49821812]\n",
      " [ 0.5048722 ]\n",
      " [ 0.49895018]\n",
      " [ 0.50013649]\n",
      " [ 0.49437428]\n",
      " [ 0.50258482]\n",
      " [ 0.50234818]\n",
      " [ 0.49950233]\n",
      " [ 0.49770802]\n",
      " [ 0.49949548]\n",
      " [ 0.50061721]\n",
      " [ 0.50127017]\n",
      " [ 0.49951231]\n",
      " [ 0.50192821]\n",
      " [ 0.50420308]\n",
      " [ 0.49560621]\n",
      " [ 0.50370318]\n",
      " [ 0.49931288]\n",
      " [ 0.50106609]\n",
      " [ 0.50015938]\n",
      " [ 0.49816   ]\n",
      " [ 0.49782073]\n",
      " [ 0.50023001]\n",
      " [ 0.5008145 ]\n",
      " [ 0.49751717]\n",
      " [ 0.49871069]\n",
      " [ 0.49543521]\n",
      " [ 0.49829602]\n",
      " [ 0.49853516]\n",
      " [ 0.49921185]\n",
      " [ 0.49517059]\n",
      " [ 0.50147438]\n",
      " [ 0.49898142]\n",
      " [ 0.49811292]\n",
      " [ 0.49960667]\n",
      " [ 0.50059295]\n",
      " [ 0.49811292]\n",
      " [ 0.49891689]\n",
      " [ 0.50149798]\n",
      " [ 0.49977615]\n",
      " [ 0.49801874]\n",
      " [ 0.49721938]\n",
      " [ 0.49666253]\n",
      " [ 0.50103033]\n",
      " [ 0.50069714]\n",
      " [ 0.498916  ]\n",
      " [ 0.50000459]\n",
      " [ 0.50245243]\n",
      " [ 0.49848843]\n",
      " [ 0.49908674]\n",
      " [ 0.50257552]\n",
      " [ 0.49811292]\n",
      " [ 0.49811587]\n",
      " [ 0.49927756]\n",
      " [ 0.49673152]\n",
      " [ 0.49811292]\n",
      " [ 0.50630105]\n",
      " [ 0.50031036]\n",
      " [ 0.49811292]\n",
      " [ 0.49811918]\n",
      " [ 0.49905437]\n",
      " [ 0.49811292]\n",
      " [ 0.50139725]\n",
      " [ 0.4982377 ]\n",
      " [ 0.49962169]\n",
      " [ 0.49698749]\n",
      " [ 0.50102258]\n",
      " [ 0.4966197 ]\n",
      " [ 0.50073963]\n",
      " [ 0.50275284]\n",
      " [ 0.5014627 ]\n",
      " [ 0.49644396]\n",
      " [ 0.50072712]\n",
      " [ 0.49934044]\n",
      " [ 0.50276113]\n",
      " [ 0.49829677]\n",
      " [ 0.49581543]\n",
      " [ 0.49574077]\n",
      " [ 0.49972436]\n",
      " [ 0.49737185]\n",
      " [ 0.5000695 ]\n",
      " [ 0.4989534 ]\n",
      " [ 0.49967095]\n",
      " [ 0.50136507]\n",
      " [ 0.4984841 ]\n",
      " [ 0.49947834]\n",
      " [ 0.50023812]\n",
      " [ 0.49869925]\n",
      " [ 0.50287622]\n",
      " [ 0.49868977]\n",
      " [ 0.49897525]\n",
      " [ 0.49471191]\n",
      " [ 0.49471191]\n",
      " [ 0.49471191]\n",
      " [ 0.50111318]\n",
      " [ 0.49471191]\n",
      " [ 0.49471191]\n",
      " [ 0.49680364]\n",
      " [ 0.4992311 ]\n",
      " [ 0.49812415]\n",
      " [ 0.49870092]\n",
      " [ 0.50219363]\n",
      " [ 0.5049929 ]\n",
      " [ 0.495233  ]\n",
      " [ 0.50148797]\n",
      " [ 0.50240088]\n",
      " [ 0.49706206]\n",
      " [ 0.49872023]\n",
      " [ 0.50208235]\n",
      " [ 0.49820793]\n",
      " [ 0.49310225]\n",
      " [ 0.49556974]\n",
      " [ 0.49605983]\n",
      " [ 0.49887297]\n",
      " [ 0.49988118]\n",
      " [ 0.49546319]\n",
      " [ 0.50274539]\n",
      " [ 0.49912795]\n",
      " [ 0.4974502 ]\n",
      " [ 0.49964541]\n",
      " [ 0.49869925]\n",
      " [ 0.49852201]\n",
      " [ 0.49889606]\n",
      " [ 0.50244743]\n",
      " [ 0.50465298]\n",
      " [ 0.49945784]\n",
      " [ 0.50004697]\n",
      " [ 0.49972764]\n",
      " [ 0.49548203]\n",
      " [ 0.50304383]\n",
      " [ 0.49659488]\n",
      " [ 0.49870795]\n",
      " [ 0.49869925]\n",
      " [ 0.50139511]\n",
      " [ 0.49746647]\n",
      " [ 0.49758971]\n",
      " [ 0.49406841]\n",
      " [ 0.50003779]\n",
      " [ 0.49968168]\n",
      " [ 0.50002563]\n",
      " [ 0.49876583]\n",
      " [ 0.5014627 ]\n",
      " [ 0.49691001]\n",
      " [ 0.50275284]\n",
      " [ 0.49889714]\n",
      " [ 0.49551553]\n",
      " [ 0.50332487]\n",
      " [ 0.49684337]\n",
      " [ 0.49898118]\n",
      " [ 0.49750063]\n",
      " [ 0.49932566]\n",
      " [ 0.50039744]\n",
      " [ 0.49698383]\n",
      " [ 0.49537003]\n",
      " [ 0.49868163]\n",
      " [ 0.49851397]\n",
      " [ 0.49850881]\n",
      " [ 0.49450728]\n",
      " [ 0.49659032]\n",
      " [ 0.50326782]\n",
      " [ 0.49945325]\n",
      " [ 0.50008112]\n",
      " [ 0.49658948]\n",
      " [ 0.49718815]\n",
      " [ 0.49935102]\n",
      " [ 0.49802491]\n",
      " [ 0.5005061 ]\n",
      " [ 0.49830821]\n",
      " [ 0.49931845]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['predicted_value'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>predicted_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>WE GOT IN THE CAR AND MY DAD'S PHONE AUTO CONN...</td>\n",
       "      <td>0.502658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/UyNgZtCY...</td>\n",
       "      <td>0.500879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/JxudunH6...</td>\n",
       "      <td>0.500801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>3?17?????18????Car Audio Club????????Super Hig...</td>\n",
       "      <td>0.497989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>(Audio) Pursuit: @KokomoPolice in car chase fr...</td>\n",
       "      <td>0.496817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>3?17?????18????Car Audio Club????????Super Hig...</td>\n",
       "      <td>0.495908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/38K8ydFJ...</td>\n",
       "      <td>0.500859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>@MirrorFootball He set up the phone to record ...</td>\n",
       "      <td>0.495744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>12v Car Auto Boat Audio Fuse High Power 100 Am...</td>\n",
       "      <td>0.500052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>NVX Solid Gold Car Audio Subwoofer  Special Ed...</td>\n",
       "      <td>0.498135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @xeni: Stormy‚Äôs got him by the nuts, her ra...</td>\n",
       "      <td>0.501652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/MrRPBVXU...</td>\n",
       "      <td>0.500884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/rUzAGz5J...</td>\n",
       "      <td>0.500925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/09AxrGuL...</td>\n",
       "      <td>0.500819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/nk70aSNQ...</td>\n",
       "      <td>0.500885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/Z0mcOQWX...</td>\n",
       "      <td>0.500827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Save 27% | Uniden R3 Extreme Long Range Radar ...</td>\n",
       "      <td>0.500026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>AXIS D2050-VE Network #Radar Detector is a rel...</td>\n",
       "      <td>0.498766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>3 Best Radar Detector Under 200 Dollar Reviews...</td>\n",
       "      <td>0.501463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@Josh_Hamrick busy week huh? A thought ? Would...</td>\n",
       "      <td>0.496910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Radar Detector (The Loving Hand Remix) - Darwi...</td>\n",
       "      <td>0.502753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @juggy_hudge: ‚Äúyou need a radar detector li...</td>\n",
       "      <td>0.498897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Molly is playing \"The Disappearance Of The Sun...</td>\n",
       "      <td>0.495516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@Whiteboysunday GET A RADAR DETECTOR MAN</td>\n",
       "      <td>0.503325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Cobra Electronics Irad 950 Iradar Atom Radar D...</td>\n",
       "      <td>0.496843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>‚Äúyou need a radar detector like mine‚Äù -joshua,...</td>\n",
       "      <td>0.498981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Thank god for my radar detector ??</td>\n",
       "      <td>0.497501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Save 27% | Uniden R3 Extreme Long Range Radar ...</td>\n",
       "      <td>0.499326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>#Deal Save 27% | Uniden R3 Extreme Long Range ...</td>\n",
       "      <td>0.500397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>#Save 27% | Uniden R3 Extreme Long Range Radar...</td>\n",
       "      <td>0.496984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>#Deal Save 27% | Uniden R3 Extreme Long Range ...</td>\n",
       "      <td>0.495370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>#Save 27% | Uniden R3 Extreme Long Range Radar...</td>\n",
       "      <td>0.498682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>rd_text</td>\n",
       "      <td># Uniden R3 Extreme Long Range Radar Laser Det...</td>\n",
       "      <td>0.498514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>He owns a radar detector.</td>\n",
       "      <td>0.498509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@ricinmidland @knitterrrr I woulda gave him a ...</td>\n",
       "      <td>0.494507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Ebay Deal: Uniden R3 Extreme Long Range Radar ...</td>\n",
       "      <td>0.496590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Uniden R3 Extreme Long Range Radar Laser Detec...</td>\n",
       "      <td>0.503268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Uniden R3 Extreme Long Range Radar Laser Detec...</td>\n",
       "      <td>0.499453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[$399.99 save 28%] Uniden R3 Extreme Long Rang...</td>\n",
       "      <td>0.500081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>SDP - Sound Depot and Performance in Gainesvil...</td>\n",
       "      <td>0.496589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Uber driver in Vegas couple nights ago decided...</td>\n",
       "      <td>0.497188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>music greatest hits pop rock music: Darwin Dee...</td>\n",
       "      <td>0.499351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[$399.99] Uniden R3 Radar Laser Detector - $36...</td>\n",
       "      <td>0.498025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@ItsMeCathi @doctecazoid Same for me..to me he...</td>\n",
       "      <td>0.500506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Uniden R3 Radar Laser Detector - $360, R1 $270...</td>\n",
       "      <td>0.498308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Uniden R3 Radar Laser Detector - $360, R1 $270...</td>\n",
       "      <td>0.499318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Category                                               Text  \\\n",
       "0    ca_text  WE GOT IN THE CAR AND MY DAD'S PHONE AUTO CONN...   \n",
       "1    ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "2    ca_text  I liked a @YouTube video https://t.co/UyNgZtCY...   \n",
       "3    ca_text  I liked a @YouTube video https://t.co/JxudunH6...   \n",
       "4    ca_text  3?17?????18????Car Audio Club????????Super Hig...   \n",
       "5    ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "6    ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "7    ca_text  (Audio) Pursuit: @KokomoPolice in car chase fr...   \n",
       "8    ca_text  3?17?????18????Car Audio Club????????Super Hig...   \n",
       "9    ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "10   ca_text  I liked a @YouTube video https://t.co/38K8ydFJ...   \n",
       "11   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "12   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "13   ca_text  @MirrorFootball He set up the phone to record ...   \n",
       "14   ca_text  12v Car Auto Boat Audio Fuse High Power 100 Am...   \n",
       "15   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "16   ca_text  NVX Solid Gold Car Audio Subwoofer  Special Ed...   \n",
       "17   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "18   ca_text  RT @xeni: Stormy‚Äôs got him by the nuts, her ra...   \n",
       "19   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "20   ca_text  I liked a @YouTube video https://t.co/MrRPBVXU...   \n",
       "21   ca_text  I liked a @YouTube video https://t.co/rUzAGz5J...   \n",
       "22   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "23   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "24   ca_text  I liked a @YouTube video https://t.co/09AxrGuL...   \n",
       "25   ca_text  I liked a @YouTube video https://t.co/nk70aSNQ...   \n",
       "26   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "27   ca_text  I liked a @YouTube video https://t.co/Z0mcOQWX...   \n",
       "28   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "29   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "..       ...                                                ...   \n",
       "870  rd_text  Save 27% | Uniden R3 Extreme Long Range Radar ...   \n",
       "871  rd_text  AXIS D2050-VE Network #Radar Detector is a rel...   \n",
       "872  rd_text  3 Best Radar Detector Under 200 Dollar Reviews...   \n",
       "873  rd_text  @Josh_Hamrick busy week huh? A thought ? Would...   \n",
       "874  rd_text  Radar Detector (The Loving Hand Remix) - Darwi...   \n",
       "875  rd_text  RT @juggy_hudge: ‚Äúyou need a radar detector li...   \n",
       "876  rd_text  Molly is playing \"The Disappearance Of The Sun...   \n",
       "877  rd_text           @Whiteboysunday GET A RADAR DETECTOR MAN   \n",
       "878  rd_text  Cobra Electronics Irad 950 Iradar Atom Radar D...   \n",
       "879  rd_text  ‚Äúyou need a radar detector like mine‚Äù -joshua,...   \n",
       "880  rd_text                 Thank god for my radar detector ??   \n",
       "881  rd_text  Save 27% | Uniden R3 Extreme Long Range Radar ...   \n",
       "882  rd_text  #Deal Save 27% | Uniden R3 Extreme Long Range ...   \n",
       "883  rd_text  #Save 27% | Uniden R3 Extreme Long Range Radar...   \n",
       "884  rd_text  #Deal Save 27% | Uniden R3 Extreme Long Range ...   \n",
       "885  rd_text  #Save 27% | Uniden R3 Extreme Long Range Radar...   \n",
       "886  rd_text  # Uniden R3 Extreme Long Range Radar Laser Det...   \n",
       "887  rd_text                          He owns a radar detector.   \n",
       "888  rd_text  @ricinmidland @knitterrrr I woulda gave him a ...   \n",
       "889  rd_text  Ebay Deal: Uniden R3 Extreme Long Range Radar ...   \n",
       "890  rd_text  Uniden R3 Extreme Long Range Radar Laser Detec...   \n",
       "891  rd_text  Uniden R3 Extreme Long Range Radar Laser Detec...   \n",
       "892  rd_text  [$399.99 save 28%] Uniden R3 Extreme Long Rang...   \n",
       "893  rd_text  SDP - Sound Depot and Performance in Gainesvil...   \n",
       "894  rd_text  Uber driver in Vegas couple nights ago decided...   \n",
       "895  rd_text  music greatest hits pop rock music: Darwin Dee...   \n",
       "896  rd_text  [$399.99] Uniden R3 Radar Laser Detector - $36...   \n",
       "897  rd_text  @ItsMeCathi @doctecazoid Same for me..to me he...   \n",
       "898  rd_text  Uniden R3 Radar Laser Detector - $360, R1 $270...   \n",
       "899  rd_text  Uniden R3 Radar Laser Detector - $360, R1 $270...   \n",
       "\n",
       "     predicted_value  \n",
       "0           0.502658  \n",
       "1           0.497303  \n",
       "2           0.500879  \n",
       "3           0.500801  \n",
       "4           0.497989  \n",
       "5           0.497303  \n",
       "6           0.497303  \n",
       "7           0.496817  \n",
       "8           0.495908  \n",
       "9           0.497303  \n",
       "10          0.500859  \n",
       "11          0.497303  \n",
       "12          0.497303  \n",
       "13          0.495744  \n",
       "14          0.500052  \n",
       "15          0.497303  \n",
       "16          0.498135  \n",
       "17          0.497303  \n",
       "18          0.501652  \n",
       "19          0.497303  \n",
       "20          0.500884  \n",
       "21          0.500925  \n",
       "22          0.497303  \n",
       "23          0.497303  \n",
       "24          0.500819  \n",
       "25          0.500885  \n",
       "26          0.497303  \n",
       "27          0.500827  \n",
       "28          0.497303  \n",
       "29          0.497303  \n",
       "..               ...  \n",
       "870         0.500026  \n",
       "871         0.498766  \n",
       "872         0.501463  \n",
       "873         0.496910  \n",
       "874         0.502753  \n",
       "875         0.498897  \n",
       "876         0.495516  \n",
       "877         0.503325  \n",
       "878         0.496843  \n",
       "879         0.498981  \n",
       "880         0.497501  \n",
       "881         0.499326  \n",
       "882         0.500397  \n",
       "883         0.496984  \n",
       "884         0.495370  \n",
       "885         0.498682  \n",
       "886         0.498514  \n",
       "887         0.498509  \n",
       "888         0.494507  \n",
       "889         0.496590  \n",
       "890         0.503268  \n",
       "891         0.499453  \n",
       "892         0.500081  \n",
       "893         0.496589  \n",
       "894         0.497188  \n",
       "895         0.499351  \n",
       "896         0.498025  \n",
       "897         0.500506  \n",
       "898         0.498308  \n",
       "899         0.499318  \n",
       "\n",
       "[900 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding more layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=300))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(data, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['predicted_value2'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>predicted_value</th>\n",
       "      <th>predicted_value2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>WE GOT IN THE CAR AND MY DAD'S PHONE AUTO CONN...</td>\n",
       "      <td>0.502658</td>\n",
       "      <td>0.487872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "      <td>0.488042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/UyNgZtCY...</td>\n",
       "      <td>0.500879</td>\n",
       "      <td>0.487355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/JxudunH6...</td>\n",
       "      <td>0.500801</td>\n",
       "      <td>0.486741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>3?17?????18????Car Audio Club????????Super Hig...</td>\n",
       "      <td>0.497989</td>\n",
       "      <td>0.491522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "      <td>0.488042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "      <td>0.488042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>(Audio) Pursuit: @KokomoPolice in car chase fr...</td>\n",
       "      <td>0.496817</td>\n",
       "      <td>0.492052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>3?17?????18????Car Audio Club????????Super Hig...</td>\n",
       "      <td>0.495908</td>\n",
       "      <td>0.491566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "      <td>0.488042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/38K8ydFJ...</td>\n",
       "      <td>0.500859</td>\n",
       "      <td>0.487280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "      <td>0.488042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "      <td>0.488042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>@MirrorFootball He set up the phone to record ...</td>\n",
       "      <td>0.495744</td>\n",
       "      <td>0.485752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>12v Car Auto Boat Audio Fuse High Power 100 Am...</td>\n",
       "      <td>0.500052</td>\n",
       "      <td>0.488494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "      <td>0.488042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>NVX Solid Gold Car Audio Subwoofer  Special Ed...</td>\n",
       "      <td>0.498135</td>\n",
       "      <td>0.492163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "      <td>0.488042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @xeni: Stormy‚Äôs got him by the nuts, her ra...</td>\n",
       "      <td>0.501652</td>\n",
       "      <td>0.488467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "      <td>0.488042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/MrRPBVXU...</td>\n",
       "      <td>0.500884</td>\n",
       "      <td>0.487490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/rUzAGz5J...</td>\n",
       "      <td>0.500925</td>\n",
       "      <td>0.487022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "      <td>0.488042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "      <td>0.488042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/09AxrGuL...</td>\n",
       "      <td>0.500819</td>\n",
       "      <td>0.487458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/nk70aSNQ...</td>\n",
       "      <td>0.500885</td>\n",
       "      <td>0.487078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "      <td>0.488042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/Z0mcOQWX...</td>\n",
       "      <td>0.500827</td>\n",
       "      <td>0.487042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "      <td>0.488042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there‚Äôs no audio...</td>\n",
       "      <td>0.497303</td>\n",
       "      <td>0.488042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Save 27% | Uniden R3 Extreme Long Range Radar ...</td>\n",
       "      <td>0.500026</td>\n",
       "      <td>0.490751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>AXIS D2050-VE Network #Radar Detector is a rel...</td>\n",
       "      <td>0.498766</td>\n",
       "      <td>0.489955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>3 Best Radar Detector Under 200 Dollar Reviews...</td>\n",
       "      <td>0.501463</td>\n",
       "      <td>0.490937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@Josh_Hamrick busy week huh? A thought ? Would...</td>\n",
       "      <td>0.496910</td>\n",
       "      <td>0.487625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Radar Detector (The Loving Hand Remix) - Darwi...</td>\n",
       "      <td>0.502753</td>\n",
       "      <td>0.494283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @juggy_hudge: ‚Äúyou need a radar detector li...</td>\n",
       "      <td>0.498897</td>\n",
       "      <td>0.489053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Molly is playing \"The Disappearance Of The Sun...</td>\n",
       "      <td>0.495516</td>\n",
       "      <td>0.490816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@Whiteboysunday GET A RADAR DETECTOR MAN</td>\n",
       "      <td>0.503325</td>\n",
       "      <td>0.492880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Cobra Electronics Irad 950 Iradar Atom Radar D...</td>\n",
       "      <td>0.496843</td>\n",
       "      <td>0.491978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>‚Äúyou need a radar detector like mine‚Äù -joshua,...</td>\n",
       "      <td>0.498981</td>\n",
       "      <td>0.489594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Thank god for my radar detector ??</td>\n",
       "      <td>0.497501</td>\n",
       "      <td>0.492159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Save 27% | Uniden R3 Extreme Long Range Radar ...</td>\n",
       "      <td>0.499326</td>\n",
       "      <td>0.491366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>#Deal Save 27% | Uniden R3 Extreme Long Range ...</td>\n",
       "      <td>0.500397</td>\n",
       "      <td>0.490458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>#Save 27% | Uniden R3 Extreme Long Range Radar...</td>\n",
       "      <td>0.496984</td>\n",
       "      <td>0.491611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>#Deal Save 27% | Uniden R3 Extreme Long Range ...</td>\n",
       "      <td>0.495370</td>\n",
       "      <td>0.490558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>#Save 27% | Uniden R3 Extreme Long Range Radar...</td>\n",
       "      <td>0.498682</td>\n",
       "      <td>0.491339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>rd_text</td>\n",
       "      <td># Uniden R3 Extreme Long Range Radar Laser Det...</td>\n",
       "      <td>0.498514</td>\n",
       "      <td>0.490099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>He owns a radar detector.</td>\n",
       "      <td>0.498509</td>\n",
       "      <td>0.494449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@ricinmidland @knitterrrr I woulda gave him a ...</td>\n",
       "      <td>0.494507</td>\n",
       "      <td>0.488359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Ebay Deal: Uniden R3 Extreme Long Range Radar ...</td>\n",
       "      <td>0.496590</td>\n",
       "      <td>0.488957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Uniden R3 Extreme Long Range Radar Laser Detec...</td>\n",
       "      <td>0.503268</td>\n",
       "      <td>0.490824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Uniden R3 Extreme Long Range Radar Laser Detec...</td>\n",
       "      <td>0.499453</td>\n",
       "      <td>0.491201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[$399.99 save 28%] Uniden R3 Extreme Long Rang...</td>\n",
       "      <td>0.500081</td>\n",
       "      <td>0.488798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>SDP - Sound Depot and Performance in Gainesvil...</td>\n",
       "      <td>0.496589</td>\n",
       "      <td>0.491705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Uber driver in Vegas couple nights ago decided...</td>\n",
       "      <td>0.497188</td>\n",
       "      <td>0.484665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>music greatest hits pop rock music: Darwin Dee...</td>\n",
       "      <td>0.499351</td>\n",
       "      <td>0.490996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>[$399.99] Uniden R3 Radar Laser Detector - $36...</td>\n",
       "      <td>0.498025</td>\n",
       "      <td>0.491274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@ItsMeCathi @doctecazoid Same for me..to me he...</td>\n",
       "      <td>0.500506</td>\n",
       "      <td>0.489324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Uniden R3 Radar Laser Detector - $360, R1 $270...</td>\n",
       "      <td>0.498308</td>\n",
       "      <td>0.491377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Uniden R3 Radar Laser Detector - $360, R1 $270...</td>\n",
       "      <td>0.499318</td>\n",
       "      <td>0.492574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Category                                               Text  \\\n",
       "0    ca_text  WE GOT IN THE CAR AND MY DAD'S PHONE AUTO CONN...   \n",
       "1    ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "2    ca_text  I liked a @YouTube video https://t.co/UyNgZtCY...   \n",
       "3    ca_text  I liked a @YouTube video https://t.co/JxudunH6...   \n",
       "4    ca_text  3?17?????18????Car Audio Club????????Super Hig...   \n",
       "5    ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "6    ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "7    ca_text  (Audio) Pursuit: @KokomoPolice in car chase fr...   \n",
       "8    ca_text  3?17?????18????Car Audio Club????????Super Hig...   \n",
       "9    ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "10   ca_text  I liked a @YouTube video https://t.co/38K8ydFJ...   \n",
       "11   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "12   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "13   ca_text  @MirrorFootball He set up the phone to record ...   \n",
       "14   ca_text  12v Car Auto Boat Audio Fuse High Power 100 Am...   \n",
       "15   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "16   ca_text  NVX Solid Gold Car Audio Subwoofer  Special Ed...   \n",
       "17   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "18   ca_text  RT @xeni: Stormy‚Äôs got him by the nuts, her ra...   \n",
       "19   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "20   ca_text  I liked a @YouTube video https://t.co/MrRPBVXU...   \n",
       "21   ca_text  I liked a @YouTube video https://t.co/rUzAGz5J...   \n",
       "22   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "23   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "24   ca_text  I liked a @YouTube video https://t.co/09AxrGuL...   \n",
       "25   ca_text  I liked a @YouTube video https://t.co/nk70aSNQ...   \n",
       "26   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "27   ca_text  I liked a @YouTube video https://t.co/Z0mcOQWX...   \n",
       "28   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "29   ca_text  RT @utdxtra: For some reason, there‚Äôs no audio...   \n",
       "..       ...                                                ...   \n",
       "870  rd_text  Save 27% | Uniden R3 Extreme Long Range Radar ...   \n",
       "871  rd_text  AXIS D2050-VE Network #Radar Detector is a rel...   \n",
       "872  rd_text  3 Best Radar Detector Under 200 Dollar Reviews...   \n",
       "873  rd_text  @Josh_Hamrick busy week huh? A thought ? Would...   \n",
       "874  rd_text  Radar Detector (The Loving Hand Remix) - Darwi...   \n",
       "875  rd_text  RT @juggy_hudge: ‚Äúyou need a radar detector li...   \n",
       "876  rd_text  Molly is playing \"The Disappearance Of The Sun...   \n",
       "877  rd_text           @Whiteboysunday GET A RADAR DETECTOR MAN   \n",
       "878  rd_text  Cobra Electronics Irad 950 Iradar Atom Radar D...   \n",
       "879  rd_text  ‚Äúyou need a radar detector like mine‚Äù -joshua,...   \n",
       "880  rd_text                 Thank god for my radar detector ??   \n",
       "881  rd_text  Save 27% | Uniden R3 Extreme Long Range Radar ...   \n",
       "882  rd_text  #Deal Save 27% | Uniden R3 Extreme Long Range ...   \n",
       "883  rd_text  #Save 27% | Uniden R3 Extreme Long Range Radar...   \n",
       "884  rd_text  #Deal Save 27% | Uniden R3 Extreme Long Range ...   \n",
       "885  rd_text  #Save 27% | Uniden R3 Extreme Long Range Radar...   \n",
       "886  rd_text  # Uniden R3 Extreme Long Range Radar Laser Det...   \n",
       "887  rd_text                          He owns a radar detector.   \n",
       "888  rd_text  @ricinmidland @knitterrrr I woulda gave him a ...   \n",
       "889  rd_text  Ebay Deal: Uniden R3 Extreme Long Range Radar ...   \n",
       "890  rd_text  Uniden R3 Extreme Long Range Radar Laser Detec...   \n",
       "891  rd_text  Uniden R3 Extreme Long Range Radar Laser Detec...   \n",
       "892  rd_text  [$399.99 save 28%] Uniden R3 Extreme Long Rang...   \n",
       "893  rd_text  SDP - Sound Depot and Performance in Gainesvil...   \n",
       "894  rd_text  Uber driver in Vegas couple nights ago decided...   \n",
       "895  rd_text  music greatest hits pop rock music: Darwin Dee...   \n",
       "896  rd_text  [$399.99] Uniden R3 Radar Laser Detector - $36...   \n",
       "897  rd_text  @ItsMeCathi @doctecazoid Same for me..to me he...   \n",
       "898  rd_text  Uniden R3 Radar Laser Detector - $360, R1 $270...   \n",
       "899  rd_text  Uniden R3 Radar Laser Detector - $360, R1 $270...   \n",
       "\n",
       "     predicted_value  predicted_value2  \n",
       "0           0.502658          0.487872  \n",
       "1           0.497303          0.488042  \n",
       "2           0.500879          0.487355  \n",
       "3           0.500801          0.486741  \n",
       "4           0.497989          0.491522  \n",
       "5           0.497303          0.488042  \n",
       "6           0.497303          0.488042  \n",
       "7           0.496817          0.492052  \n",
       "8           0.495908          0.491566  \n",
       "9           0.497303          0.488042  \n",
       "10          0.500859          0.487280  \n",
       "11          0.497303          0.488042  \n",
       "12          0.497303          0.488042  \n",
       "13          0.495744          0.485752  \n",
       "14          0.500052          0.488494  \n",
       "15          0.497303          0.488042  \n",
       "16          0.498135          0.492163  \n",
       "17          0.497303          0.488042  \n",
       "18          0.501652          0.488467  \n",
       "19          0.497303          0.488042  \n",
       "20          0.500884          0.487490  \n",
       "21          0.500925          0.487022  \n",
       "22          0.497303          0.488042  \n",
       "23          0.497303          0.488042  \n",
       "24          0.500819          0.487458  \n",
       "25          0.500885          0.487078  \n",
       "26          0.497303          0.488042  \n",
       "27          0.500827          0.487042  \n",
       "28          0.497303          0.488042  \n",
       "29          0.497303          0.488042  \n",
       "..               ...               ...  \n",
       "870         0.500026          0.490751  \n",
       "871         0.498766          0.489955  \n",
       "872         0.501463          0.490937  \n",
       "873         0.496910          0.487625  \n",
       "874         0.502753          0.494283  \n",
       "875         0.498897          0.489053  \n",
       "876         0.495516          0.490816  \n",
       "877         0.503325          0.492880  \n",
       "878         0.496843          0.491978  \n",
       "879         0.498981          0.489594  \n",
       "880         0.497501          0.492159  \n",
       "881         0.499326          0.491366  \n",
       "882         0.500397          0.490458  \n",
       "883         0.496984          0.491611  \n",
       "884         0.495370          0.490558  \n",
       "885         0.498682          0.491339  \n",
       "886         0.498514          0.490099  \n",
       "887         0.498509          0.494449  \n",
       "888         0.494507          0.488359  \n",
       "889         0.496590          0.488957  \n",
       "890         0.503268          0.490824  \n",
       "891         0.499453          0.491201  \n",
       "892         0.500081          0.488798  \n",
       "893         0.496589          0.491705  \n",
       "894         0.497188          0.484665  \n",
       "895         0.499351          0.490996  \n",
       "896         0.498025          0.491274  \n",
       "897         0.500506          0.489324  \n",
       "898         0.498308          0.491377  \n",
       "899         0.499318          0.492574  \n",
       "\n",
       "[900 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "import pickle\n",
    " \n",
    "# save the tokenizer and model\n",
    "with open(\"keras_tokenizer.pickle\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "model.save(\"tweets_sentiment_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that the csv has been created, append it by writing with mode='a'\n",
    "f = 'D:\\GitHub\\Final Capstone\\Ecommerce Data\\Tweets_With_Sentiments.csv'\n",
    "tweets.to_csv(f, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where the info from DevelopIntelligence ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Supervised Neural Network for Predicting Rises in Popularity\n",
    "\n",
    "The combined data from all techniques will be used as the features for a Supervised Neural Network that will predict the rise in popularity of a product over a six month period. The predictions will be evaluated against the rise in popularity of the product(s) via Google Trends results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x is the result of the sentiment analyses\n",
    "\n",
    "### y is whether the product rose in popularity in the last six months on Google Trends Data\n",
    "\n",
    "### Modify paramaters to fit text classification with two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csvs from Google Trends\n",
    "ca = pd.read_csv('D:\\\\GitHub\\\\Final Capstone\\\\Trends Car Audio.csv')\n",
    "rd = pd.read_csv('D:\\\\GitHub\\\\Final Capstone\\\\Trends Radar Detector.csv')\n",
    "wt = pd.read_csv('D:\\\\GitHub\\\\Final Capstone\\\\Trends Walkie Talkies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Searches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03-05</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-03-12</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-03-19</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-04-02</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-04-16</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-04-23</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-04-30</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-05-07</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-05-14</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017-05-21</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017-05-28</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-06-04</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-06-11</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017-06-18</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017-06-25</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017-07-02</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017-07-09</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017-07-16</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017-07-23</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017-07-30</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017-08-06</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2017-08-13</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2017-08-27</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017-09-03</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2017-09-17</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2017-09-24</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2017-10-08</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2017-10-15</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2017-10-22</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2017-10-29</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2017-11-05</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2017-11-12</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2017-11-19</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2017-11-26</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2017-12-03</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2017-12-10</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2017-12-17</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2017-12-24</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2018-02-11</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2018-02-18</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Searches\n",
       "0   2017-03-05      97.0\n",
       "1   2017-03-12      92.0\n",
       "2   2017-03-19      96.0\n",
       "3   2017-03-26      96.0\n",
       "4   2017-04-02      97.0\n",
       "5   2017-04-09      95.0\n",
       "6   2017-04-16      91.0\n",
       "7   2017-04-23      89.0\n",
       "8   2017-04-30      90.0\n",
       "9   2017-05-07      86.0\n",
       "10  2017-05-14      89.0\n",
       "11  2017-05-21      91.0\n",
       "12  2017-05-28      91.0\n",
       "13  2017-06-04      89.0\n",
       "14  2017-06-11      93.0\n",
       "15  2017-06-18      93.0\n",
       "16  2017-06-25      96.0\n",
       "17  2017-07-02      93.0\n",
       "18  2017-07-09      93.0\n",
       "19  2017-07-16      95.0\n",
       "20  2017-07-23      93.0\n",
       "21  2017-07-30     100.0\n",
       "22  2017-08-06      95.0\n",
       "23  2017-08-13      88.0\n",
       "24  2017-08-20      86.0\n",
       "25  2017-08-27      82.0\n",
       "26  2017-09-03      87.0\n",
       "27  2017-09-10      85.0\n",
       "28  2017-09-17      81.0\n",
       "29  2017-09-24      83.0\n",
       "30  2017-10-01      85.0\n",
       "31  2017-10-08      84.0\n",
       "32  2017-10-15      87.0\n",
       "33  2017-10-22      81.0\n",
       "34  2017-10-29      83.0\n",
       "35  2017-11-05      83.0\n",
       "36  2017-11-12      88.0\n",
       "37  2017-11-19      92.0\n",
       "38  2017-11-26      91.0\n",
       "39  2017-12-03      90.0\n",
       "40  2017-12-10      86.0\n",
       "41  2017-12-17      91.0\n",
       "42  2017-12-24      98.0\n",
       "43  2017-12-31      87.0\n",
       "44  2018-01-07      89.0\n",
       "45  2018-01-14      83.0\n",
       "46  2018-01-21      83.0\n",
       "47  2018-01-28      84.0\n",
       "48  2018-02-04      86.0\n",
       "49  2018-02-11      84.0\n",
       "50  2018-02-18      89.0\n",
       "51  2018-02-25      89.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca.Searches.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "97",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   2476\u001b[0m             return self._engine.get_value(s, k,\n\u001b[1;32m-> 2477\u001b[1;33m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[0;32m   2478\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 97",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-d1ed23a282a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Have to start at row 29 to get the previous 30 days\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSearches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mthirty_day_average\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSearches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    628\u001b[0m                                                              kind='getitem')\n\u001b[0;32m    629\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   2475\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2476\u001b[0m             return self._engine.get_value(s, k,\n\u001b[1;32m-> 2477\u001b[1;33m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[0;32m   2478\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2479\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'integer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'boolean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 97"
     ]
    }
   ],
   "source": [
    "# Making a new field for thirty day average\n",
    "# These averages will be compared to the previous ones\n",
    "# to see if the average increase\n",
    "\n",
    "thirty_day_average = []\n",
    "\n",
    "# Have to start at row 29 to get the previous 30 days\n",
    "for i in ca.Searches[29:]:\n",
    "    av = int(ca.Seaches[i]) - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "daily_movement = []\n",
    "\n",
    "for i in ca.thirty_day_average:\n",
    "    daily_movement.append(ca.thirty_day_average[i] - ca.thirty_day_average[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, activation='relu', input_shape=(3072,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# Output shape should be equal to the number of classes (10)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = 64, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, verbose=False)\n",
    "print('Test Loss: ', score[0])\n",
    "print('Test Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII. Appendix\n",
    "\n",
    "Google PageRank Description (from Google Technology page):\n",
    "PageRank Explained\n",
    "PageRank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page‚Äôs value. In essence, Google interprets a link from page A to page B as a vote, by page A, for page B. But, Google looks at considerably more than the sheer volume of votes, or links a page receives; for example, it also analyzes the page that casts the vote. Votes cast by pages that are themselves ‚Äúimportant‚Äù weigh more heavily and help to make other pages ‚Äúimportant.‚Äù Using these and other factors, Google provides its views on pages‚Äô relative importance.\n",
    "Of course, important pages mean nothing to you if they don‚Äôt match your query. So, Google combines PageRank with sophisticated text-matching techniques to find pages that are both important and relevant to your search. Google goes far beyond the number of times a term appears on a page and examines all dozens of aspects of the page‚Äôs content (and the content of the pages linking to it) to determine if it‚Äôs a good match for your query.\n",
    "\n",
    "Keyword Planner Description (from Google Adwords page):\n",
    "Keyword Planner is a free AdWords tool for new or experienced advertisers that‚Äôs like a workshop for building new Search Network campaigns or expanding existing ones. You can search for keyword and ad group ideas, see how a list of keywords might perform, and even create a new keyword list by multiplying several lists of keywords together. Keyword Planner can also help you choose competitive bids and budgets to use with your campaigns.\n",
    "Sources:\n",
    "Facebook: http://graph.facebook.com\n",
    "Google Trends: https://developers.google.com/gdata/docs/directory\n",
    "Google News: https://newsapi.org/s/google-news-api\n",
    "Keyword Planner: https://adwords.google.com/ko/KeywordPlanner/Home?sourceid=awo&__u=3050284228&__c=9347440984&authuser=0#search\n",
    "Twitter: https://developer.twitter.com/en/docs/basics/getting-started\n",
    "\n",
    "Links to AliExpress items: \n",
    "\n",
    "https://www.aliexpress.com/item/2pcs-New-Black-Retevis-RT628-Portable-radio-Walkie-Talkie-sets-0-5W-8CH-UHF-Europe-Frequency/32407070024.html?spm=2114.search0104.3.27.383641a8WAVsOz&ws_ab_test=searchweb0_0,searchweb201602_3_10152_10151_10065_10344_10130_10068_10324_10342_10547_10325_10343_10546_10340_10548_10341_10545_10084_10083_10618_10307_10313_10059_10534_100031_10629_10103_10626_10625_10624_10623_10622_10621_10620_10142,searchweb201603_25,ppcSwitch_5&algo_expid=c512be04-3611-4608-90ee-6e55891cae81-6&algo_pvid=c512be04-3611-4608-90ee-6e55891cae81&priceBeautifyAB=0\n",
    "\n",
    "https://www.aliexpress.com/item/4012B-4-1-inch-1-Din-Car-Radio-Auto-Audio-Stereo-FM-Bluetooth-2-0-Support/32838433123.html?spm=2114.search0104.3.25.6529687a4mQ9YS&ws_ab_test=searchweb0_0,searchweb201602_3_10152_10151_10065_10344_10130_10068_10324_10342_10547_10325_10343_10546_10340_10548_10341_10545_10084_10083_10618_10307_10313_10059_10534_100031_10629_10103_10626_10625_10624_10623_10622_10621_10620_10142,searchweb201603_25,ppcSwitch_5&algo_expid=78926112-aa75-459d-888f-2b887bd13743-3&algo_pvid=78926112-aa75-459d-888f-2b887bd13743&priceBeautifyAB=0\n",
    "\n",
    "https://www.aliexpress.com/item/Clear-Stock-Excelvan-E8-Car-Radar-Detector-360-Degree-Speed-Safety-Anti-Police-Scanning-Advanced-Voice/32839841753.html?spm=2114.search0104.3.1.1bc42b2a9yCeVF&ws_ab_test=searchweb0_0,searchweb201602_3_10152_10151_10065_10344_10130_10068_10324_10342_10547_10325_10343_10546_10340_10548_10341_10545_10084_10083_10618_10307_10313_10059_10534_100031_10629_10103_10626_10625_10624_10623_10622_10621_10620_10142,searchweb201603_25,ppcSwitch_5&algo_expid=9a63780f-134d-4690-9830-ebf6c5fff3aa-0&algo_pvid=9a63780f-134d-4690-9830-ebf6c5fff3aa&priceBeautifyAB=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra code that is not yet implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for row in all_texts.Text:\n",
    "    words = []\n",
    "    words.append(preprocess(row))\n",
    "    tokens.append(words)    \n",
    "    \n",
    "\n",
    "all_texts['tokens'] = tokens\n",
    "all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_texts.Text = str(all_texts.Text)\n",
    "all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(#max_df=1, # drop words that occur in more than \n",
    "                             #min_df=0, \n",
    "                             vocabulary = None,\n",
    "                             stop_words='english', \n",
    "                             lowercase=False, #convert everything to lower case \n",
    "                             use_idf=True,#inverse document frequencies for weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "results_tfidf=vectorizer.fit_transform(all_texts.Text)\n",
    "print(\"Number of features: %d\" % results_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "results_lsa = lsa.fit_transform(results_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsa_res = results_lsa.reshape(300,-1)\n",
    "tweets_by_component=pd.DataFrame(results_lsa)\n",
    "tweets_by_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(130,)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = results_lsa\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(X, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using k-means to cluster together authors\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=2, random_state=42).fit_predict(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = all_texts.loc[~all_texts.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupings = pd.DataFrame(y_pred, columns = ['Group'])\n",
    "groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupings['Text'] = data2['Text']\n",
    "groupings['Category'] = data2['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "# Utility function to clean text.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # Getting rid of unneccesary charactersLo\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "       \n",
    "    # Get rid of extra whitespace.\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "for text in all_texts.Text:\n",
    "    all_texts['cleaned_text'] = text_cleaner(text)\n",
    "\n",
    "#text_cleaner(ca_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tweet = unicode(tweet.decode('utf-8').lower())\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tokens = filter(lambda t: not t.startswith('@'), tokens)\n",
    "        tokens = filter(lambda t: not t.startswith('#'), tokens)\n",
    "        tokens = filter(lambda t: not t.startswith('http'), tokens)\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def postprocess(data, n=1000000):\n",
    "    data = data.head(n)\n",
    "    data['tokens'] = data['Text'].map(tokenize)\n",
    "    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "data = postprocess(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#Import the necessary modules\n",
    "import datetime\n",
    "from newsapi import NewsApiClient\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Matthew Kennedy's API Key, business will need to register for their own api and place it below (registration is free)\n",
    "newsapi = NewsApiClient(api_key='2613ce5e838a464b814b7d5b4c2e6bf8')\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date = now.day\n",
    "\n",
    "keyword_list = []\n",
    "\n",
    "####################################################################\n",
    "########## Add keywords from Keyword Planner to keyword_list########\n",
    "####################################################################\n",
    "\n",
    "all_articles = newsapi.get_everything(q=[keyword_list]\n",
    "                                     #, from_paramater = '2018-01-08' #this can be changed to any date\n",
    "                                     , to = date\n",
    "                                     , language = 'en'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "data = all_articles['articles']\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# The Json is a list of dictionary values. This code will store it to a DataFrame.\n",
    "results = {'Description': [], 'Publish Date': [], 'URL': []}\n",
    "columns = results.keys()\n",
    "df_data = pd.DataFrame(data=results, columns=columns)\n",
    "\n",
    "for entry in data:\n",
    "    #print(data[entry]['description'])\n",
    "    description = entry['description']\n",
    "    date = entry['publishedAt']\n",
    "    url = entry['url']\n",
    "    results = {'Description':[description], 'Publish Date':[date], 'URL': [url]}\n",
    "    df_data = df_data.append(pd.DataFrame(data=results, columns=results.keys()), ignore_index = False)\n",
    "    \n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "############################################################################\n",
    "###### Now, create a crawler to crawl the information of each url ##########\n",
    "############################################################################\n",
    "\n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# Make a list for the urls to be stored into from the dataframe\n",
    "url_list = []\n",
    "for entry in df_data['URL']:\n",
    "    url_list.append(entry)\n",
    "    \n",
    "#print(url_list)\n",
    "\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = url_list\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        # for article in response.xpath('//article'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                'everything': response.xpath('//text()').extract()\n",
    "                #'all': response.xpath('/descendant-or-self::node()').extract()\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                #'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                #'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                #'text': article.xpath('//*[@id=\"post-669657\"]/div[3]').extract()\n",
    "                #,'body': article.xpath('//*[@id=\"single-post\"]').extract()\n",
    "                #'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'firstpage4.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "######## This doesn't stop running because it is a stream and it stays open. \n",
    "######## Cell after this one downloads the stream, but it is run through cmd line\n",
    "\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    " \n",
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('python.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    " \n",
    "\n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=['Car Audio', 'Walkie Talkies', 'Radar Detector'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# To run this code, first edit config.py with your configuration, then:\n",
    "#\n",
    "# mkdir data\n",
    "# python twitter_stream_download.py -q apple -d data\n",
    "# \n",
    "# It will produce the list of tweets for the query \"apple\" \n",
    "# in the file data/stream_apple.json\n",
    "\n",
    "import tweepy\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import time\n",
    "import argparse\n",
    "import string\n",
    "import config\n",
    "import json\n",
    "\n",
    "def get_parser():\n",
    "    \"\"\"Get parser for command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Twitter Downloader\")\n",
    "    parser.add_argument(\"-q\",\n",
    "                        \"--query\",\n",
    "                        dest=\"query\",\n",
    "                        help=\"Query/Filter\",\n",
    "                        default='-')\n",
    "    parser.add_argument(\"-d\",\n",
    "                        \"--data-dir\",\n",
    "                        dest=\"data_dir\",\n",
    "                        help=\"Output/Data Directory\")\n",
    "    return parser\n",
    "\n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    \"\"\"Custom StreamListener for streaming data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, query):\n",
    "        query_fname = format_filename(query)\n",
    "        self.outfile = \"%s/stream_%s.json\" % (data_dir, query_fname)\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open(self.outfile, 'a') as f:\n",
    "                f.write(data)\n",
    "                print(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "            time.sleep(5)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "\n",
    "\n",
    "def format_filename(fname):\n",
    "    \"\"\"Convert file name into a safe string.\n",
    "    Arguments:\n",
    "        fname -- the file name to convert\n",
    "    Return:\n",
    "        String -- converted file name\n",
    "    \"\"\"\n",
    "    return ''.join(convert_valid(one_char) for one_char in fname)\n",
    "\n",
    "\n",
    "def convert_valid(one_char):\n",
    "    \"\"\"Convert a character into '_' if invalid.\n",
    "    Arguments:\n",
    "        one_char -- the char to convert\n",
    "    Return:\n",
    "        Character -- converted char\n",
    "    \"\"\"\n",
    "    valid_chars = \"-_.%s%s\" % (string.ascii_letters, string.digits)\n",
    "    if one_char in valid_chars:\n",
    "        return one_char\n",
    "    else:\n",
    "        return '_'\n",
    "\n",
    "@classmethod\n",
    "def parse(cls, api, raw):\n",
    "    status = cls.first_parse(api, raw)\n",
    "    setattr(status, 'json', json.dumps(raw))\n",
    "    return status\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = get_parser()\n",
    "    args = parser.parse_args()\n",
    "    auth = OAuthHandler(config.consumer_key, config.consumer_secret)\n",
    "    auth.set_access_token(config.access_token, config.access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    twitter_stream = Stream(auth, MyListener(args.data_dir, args.query))\n",
    "    twitter_stream.filter(track=[args.query])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will send a request for \n",
    "\n",
    "curl -X POST \"https://api.twitter.com/1.1/tweets/search/:product/:label.json\" -d '{\"query\":\"Car Audio, Walkie Talkies, Radar Detector\",\"maxResults\":\"500\"}' -H \"Authorization: 963889537842851841-FGZVIMTpgCS760gVa3xNu9okSg8hB0j, obZiBaCujDaDzgT50s3q5fuKUTkvQuouQ57wUfl1YH2us\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying something else\n",
    "class MyModelParser(tweepy.parsers.ModelParser):\n",
    "    def parse(self, method, payload:\n",
    "              result = super(MyModelparser, self).parse(method, payload)\n",
    "              result._payload = json.loads(payload)\n",
    "api = tweepy.API(auth, parser=MyModelParser())\n",
    "results = api.search(q='Car Audio', lan='en')\n",
    "              \n",
    "for s in results._payload:\n",
    "              print(json.dumps(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "walkie_talkies_json = api.search(q='Walkie Talkies', lan='en') \n",
    "# Optional paramters: [, locale][, rpp][, page][, since_id][, geocode][, show_user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do we need to hit Amazon and Ebay APIs to compare prices vs our manufacturer?\n",
    "\n",
    "############### Yup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for text in all_texts:\n",
    "\n",
    "    results = keras.preprocessing.text.Tokenizer(ca_text,\n",
    "                                   num_words=None,\n",
    "                                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,\n",
    "                                   split=\" \",\n",
    "                                   char_level=False,\n",
    "                                   oov_token=None)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Example from https://github.com/keras-team/keras/blob/master/examples/imdb_bidirectional_lstm.py\n",
    "\n",
    "'''Trains a Bidirectional LSTM on the IMDB sentiment classification task.\n",
    "Output after 4 epochs on CPU: ~0.8146\n",
    "Time per epoch on CPU (Core i7): ~150s.\n",
    "'''\n",
    "\n",
    "########################################################################\n",
    "########### LOOK INTO USING GPU TO SAVE ON CPU LOAD ####################\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import keras\n",
    "from keras.preprocessing import text\n",
    "\n",
    "results = []\n",
    "\n",
    "for text in all_texts:\n",
    "    results = keras.preprocessing.text.text_to_word_sequence(text,\n",
    "                                               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                               lower=True,\n",
    "                                               split=\" \")\n",
    "    \n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
