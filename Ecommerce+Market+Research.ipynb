{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TODO: Read 6.1 Time Series and 6.4 Advanced NLP\n",
    "\n",
    "# Abstract\n",
    "Abstract will be written after all analyses are complete.\n",
    "For a copy of the proposal, visit \n",
    "\n",
    "# Table of Contents\n",
    "## I. Objective\n",
    "## II. Data Collection\n",
    "## III. Scraping\n",
    "## IV. Unsupervised Neural Networks on Scraped Data to Generate Sentiment Analysis\n",
    "## V. Supervised Neural Networks to Predict Rises in Popularity\n",
    "## VI. Results\n",
    "## VII. Conclusion\n",
    "## VIII. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Objective\n",
    "\n",
    "What is the Problem You Are Attempting to Solve?\n",
    "\tWith the internet becoming increasingly involved in the average person’s daily life, many companies have taken to ecommerce, which is the electronic sale of goods through the internet. For an entrepreneur to break into the ecommerce market or for a large company to add their sources of revenue, a niche market must be found.\n",
    "\tFor the sake of clarity in this project, a niche market will be defined using this Wikipedia description:\n",
    "A niche market is the subset of the market on which a specific product is focused. The market niche defines as the product features aimed at satisfying specific market needs, as well as the price range, production quality, and the demographics that is intended to impact.\n",
    "The niche market is highly specialized, and aiming to survive among the competition from numerous super companies. Even established companies create products for different niches, for example, Hewlett-Packard has all-in-one machines for printing, scanning, and faxing targeted for the home office niche while at the same time having separate machines with one of these functions for big businesses.\n",
    "With this definition in mind, the problem will be defined as, “With competition saturating the ecommerce market, what is the best niche market for a business or entrepreneur to expand into?”\n",
    "\tTo narrow the scope of this project, the focus will be on electronics appliances. \n",
    "How is Your Solution Valuable?\n",
    "The result of this project will be a solution that exceeds the performance of other niche market research tools in terms of finding the ideal niche market in a given sector. The ideal niche market is one that has some or all of the following criteria:\n",
    "•\thigh search volume\n",
    "•\tproducts that are difficult for consumers to find locally\n",
    "•\tsolves a common problem\n",
    "•\thas a customer base that is passionate\n",
    "•\tis not overrun with websites that have a Google PageRank of five or higher\n",
    "•\thas accessible suppliers.\n",
    "o\tGoogle PageRank is explained in the Appendix\n",
    "The solution will drastically reduce the number of hours and amount of wages spent for market research and will be a valuable tool for anyone in ecommerce, from large companies to individual entrepreneurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data Collection\n",
    "\n",
    "What is Your Data Source and How Will You Access It?\n",
    "Links to sources in Appendix, Access explained in Techniques Section Below\n",
    "Data for Machine Learning Models - Facebook and Twitter APIs\n",
    "Competition Research - PageRank Status for Chrome\n",
    "Validation - Google Trends\n",
    "\n",
    "\n",
    "### What Techniques From the Course Do You Anticipate Using?\n",
    "\n",
    "\n",
    "The project will begin with a broad search of keywords related to technology appliances on Keyword Planner (see appendix for description). For example, a keyword search was conducted for \"Consumer Electronics Appliances\" that returned seven hundred keywords with information on their average monthly searches and their level of competition (Low, Medium, or High). This information will be downloaded and filtered to generate a list of keywords that have a high number of average monthly searches and a low rank of competition. Keyword Planner measures competition by evaluating the number of advertisers that showed on each keyword relative to all keywords across Google. \n",
    "\n",
    "\n",
    "Twitter and Facebook will then be scraped to gather posts that reference the products in the keyword list. With this data, an Unsupervised Neural Network will perform Natural Language Processing to generate sentiment analysis of the products.\n",
    "\n",
    "\n",
    "The combined data from all techniques will be used as the features for a \n",
    "Supervised Neural Network that will predict the rise in popularity of a product over a six month period.\n",
    "The predictions will be evaluated against the rise in popularity of the product(s) via Google Trends results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Planning Results\n",
    "\n",
    "The Keyword Planner returned over seven hundred keywords under \"Consumer Electronics.\" The keywords were filtered to a list that had Average Monthly Searches between ten thousand and one hundred thousand and a Competition value under .70. The resulting list contained forty-eight keywords. After ruling out any brand names and similar keywords, this project will proceed with three of the items: Walkie Talkies, Radar Detectors, and Car Audio systems. An AliExpress search was conducted to verify the existence of manufacturers that were in a tolerable price range and able to ship one-piece orders. The tolerable price range was as follows: Walkie Talkies < \\$25, Radar Detectors < \\$15, and Car Audio < \\$45.\n",
    "\n",
    "Links to these items are in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Scraping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do we need to hit Amazon and Ebay APIs to compare prices vs our manufacturer?\n",
    "\n",
    "############### Yup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape Twitter's API, I made a new twitter account and registered an application under it to be given the necessary keys and access tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "##### Mining Twitter API #############\n",
    "######################################\n",
    "\n",
    "# https://developer.twitter.com/en/docs/tweets/filter-realtime/api-reference/post-statuses-filter.html\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = '8AZsgftPqH9dbyLfc5IFWo39v'\n",
    "consumer_secret = 'MuMAVS7rIEYa0dsyauTOeJMkjRgmU5NEvG2uzyOtWItJLli48u'\n",
    "access_token = '963889537842851841-FGZVIMTpgCS760gVa3xNu9okSg8hB0j'\n",
    "access_secret = 'obZiBaCujDaDzgT50s3q5fuKUTkvQuouQ57wUfl1YH2us'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "car_audio_json = [status for status in api.search(q='Car Audio', lan='en')] \n",
    "# Optional paramters: [, locale][, rpp][, page][, since_id][, geocode][, show_user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I liked a @YouTube video https://t.co/iAjUqJN4SL This is why your car's audio system leaves you flat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Car Audio'\n",
    "\n",
    "car_audio_tweets = [status for status in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(100)]\n",
    "print(car_audio_tweets[0].full_text)\n",
    "len(car_audio_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I liked a @YouTube video https://t.co/iAjUqJN4SL This is why your car's audio system leaves you flat\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_text = []\n",
    "for text in car_audio_tweets:\n",
    "    ca_text.append(text.full_text)\n",
    "ca_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@LordMcAlpin @FiveRights A 45 acre campus like the one we are talking about absolutely needs more armed guards w/walkie-talkies on segways &amp; golf carts. There must be a study somewhere that determines the right number of guards per student or acre. But 1 guard on that campus &amp; no camera monitors is nuts!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Walkie Talkies'\n",
    "\n",
    "walkie_talkie_tweets = [status for status in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(100)]\n",
    "print(walkie_talkie_tweets[0].full_text)\n",
    "len(walkie_talkie_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@LordMcAlpin @FiveRights A 45 acre campus like the one we are talking about absolutely needs more armed guards w/walkie-talkies on segways &amp; golf carts. There must be a study somewhere that determines the right number of guards per student or acre. But 1 guard on that campus &amp; no camera monitors is nuts!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_text = []\n",
    "for text in walkie_talkie_tweets:\n",
    "    wt_text.append(text.full_text)\n",
    "wt_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 Degree Radar Detector https://t.co/wwAZEbYXly #Radar #Detector https://t.co/TMCqanzTlY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'radar detector'\n",
    "\n",
    "radar_detector_tweets = [status for status in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(100)]\n",
    "print(radar_detector_tweets[0].full_text)\n",
    "len(radar_detector_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'360 Degree Radar Detector https://t.co/wwAZEbYXly #Radar #Detector https://t.co/TMCqanzTlY'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_text = []\n",
    "for text in radar_detector_tweets:\n",
    "    rd_text.append(text.full_text)\n",
    "rd_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Unsupervised Neural Network for Sentiment Classification of Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 158s - loss: 0.4126 - acc: 0.8075 - val_loss: 0.3451 - val_acc: 0.8506\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 152s - loss: 0.2379 - acc: 0.9058 - val_loss: 0.3773 - val_acc: 0.8420\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 150s - loss: 0.1473 - acc: 0.9454 - val_loss: 0.3926 - val_acc: 0.8376\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 154s - loss: 0.0858 - acc: 0.9703 - val_loss: 0.5430 - val_acc: 0.8327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x275cdba5898>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example from https://github.com/keras-team/keras/blob/master/examples/imdb_bidirectional_lstm.py\n",
    "\n",
    "'''Trains a Bidirectional LSTM on the IMDB sentiment classification task.\n",
    "Output after 4 epochs on CPU: ~0.8146\n",
    "Time per epoch on CPU (Core i7): ~150s.\n",
    "'''\n",
    "\n",
    "########################################################################\n",
    "########### LOOK INTO USING GPU TO SAVE ON CPU LOAD ####################\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  163    11  3215 10156     4  1153     9   194   775     7  8255 11596\n",
      "   349  2637   148   605 15358  8003    15   123   125    68     2  6853\n",
      "    15   349   165  4362    98     5     4   228     9    43     2  1157\n",
      "    15   299   120     5   120   174    11   220   175   136    50     9\n",
      "  4373   228  8255     5     2   656   245  2350     5     4  9837   131\n",
      "   152   491    18     2    32  7464  1212    14     9     6   371    78\n",
      "    22   625    64  1382     9     8   168   145    23     4  1690    15\n",
      "    16     4  1355     5    28     6    52   154   462    33    89    78\n",
      "   285    16   145    95]\n"
     ]
    }
   ],
   "source": [
    "##############################################################################################\n",
    "######### Use the text lists as x and get y_pred, which will be the sentiment analysis########\n",
    "##############################################################################################\n",
    "\n",
    "### How do I get my data into this vectorized format?\n",
    "\n",
    "print(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Supervised Neural Network for Predicting Rises in Popularity\n",
    "\n",
    "The combined data from all techniques will be used as the features for a Supervised Neural Network that will predict the rise in popularity of a product over a six month period. The predictions will be evaluated against the rise in popularity of the product(s) via Google Trends results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII. Appendix\n",
    "\n",
    "Google PageRank Description (from Google Technology page):\n",
    "PageRank Explained\n",
    "PageRank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page’s value. In essence, Google interprets a link from page A to page B as a vote, by page A, for page B. But, Google looks at considerably more than the sheer volume of votes, or links a page receives; for example, it also analyzes the page that casts the vote. Votes cast by pages that are themselves “important” weigh more heavily and help to make other pages “important.” Using these and other factors, Google provides its views on pages’ relative importance.\n",
    "Of course, important pages mean nothing to you if they don’t match your query. So, Google combines PageRank with sophisticated text-matching techniques to find pages that are both important and relevant to your search. Google goes far beyond the number of times a term appears on a page and examines all dozens of aspects of the page’s content (and the content of the pages linking to it) to determine if it’s a good match for your query.\n",
    "\n",
    "Keyword Planner Description (from Google Adwords page):\n",
    "Keyword Planner is a free AdWords tool for new or experienced advertisers that’s like a workshop for building new Search Network campaigns or expanding existing ones. You can search for keyword and ad group ideas, see how a list of keywords might perform, and even create a new keyword list by multiplying several lists of keywords together. Keyword Planner can also help you choose competitive bids and budgets to use with your campaigns.\n",
    "Sources:\n",
    "Facebook: http://graph.facebook.com\n",
    "Google Trends: https://developers.google.com/gdata/docs/directory\n",
    "Google News: https://newsapi.org/s/google-news-api\n",
    "Keyword Planner: https://adwords.google.com/ko/KeywordPlanner/Home?sourceid=awo&__u=3050284228&__c=9347440984&authuser=0#search\n",
    "Twitter: https://developer.twitter.com/en/docs/basics/getting-started\n",
    "\n",
    "Links to AliExpress items: \n",
    "\n",
    "https://www.aliexpress.com/item/2pcs-New-Black-Retevis-RT628-Portable-radio-Walkie-Talkie-sets-0-5W-8CH-UHF-Europe-Frequency/32407070024.html?spm=2114.search0104.3.27.383641a8WAVsOz&ws_ab_test=searchweb0_0,searchweb201602_3_10152_10151_10065_10344_10130_10068_10324_10342_10547_10325_10343_10546_10340_10548_10341_10545_10084_10083_10618_10307_10313_10059_10534_100031_10629_10103_10626_10625_10624_10623_10622_10621_10620_10142,searchweb201603_25,ppcSwitch_5&algo_expid=c512be04-3611-4608-90ee-6e55891cae81-6&algo_pvid=c512be04-3611-4608-90ee-6e55891cae81&priceBeautifyAB=0\n",
    "\n",
    "https://www.aliexpress.com/item/4012B-4-1-inch-1-Din-Car-Radio-Auto-Audio-Stereo-FM-Bluetooth-2-0-Support/32838433123.html?spm=2114.search0104.3.25.6529687a4mQ9YS&ws_ab_test=searchweb0_0,searchweb201602_3_10152_10151_10065_10344_10130_10068_10324_10342_10547_10325_10343_10546_10340_10548_10341_10545_10084_10083_10618_10307_10313_10059_10534_100031_10629_10103_10626_10625_10624_10623_10622_10621_10620_10142,searchweb201603_25,ppcSwitch_5&algo_expid=78926112-aa75-459d-888f-2b887bd13743-3&algo_pvid=78926112-aa75-459d-888f-2b887bd13743&priceBeautifyAB=0\n",
    "\n",
    "https://www.aliexpress.com/item/Clear-Stock-Excelvan-E8-Car-Radar-Detector-360-Degree-Speed-Safety-Anti-Police-Scanning-Advanced-Voice/32839841753.html?spm=2114.search0104.3.1.1bc42b2a9yCeVF&ws_ab_test=searchweb0_0,searchweb201602_3_10152_10151_10065_10344_10130_10068_10324_10342_10547_10325_10343_10546_10340_10548_10341_10545_10084_10083_10618_10307_10313_10059_10534_100031_10629_10103_10626_10625_10624_10623_10622_10621_10620_10142,searchweb201603_25,ppcSwitch_5&algo_expid=9a63780f-134d-4690-9830-ebf6c5fff3aa-0&algo_pvid=9a63780f-134d-4690-9830-ebf6c5fff3aa&priceBeautifyAB=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra code that is not yet implemented:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#Import the necessary modules\n",
    "import datetime\n",
    "from newsapi import NewsApiClient\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Matthew Kennedy's API Key, business will need to register for their own api and place it below (registration is free)\n",
    "newsapi = NewsApiClient(api_key='2613ce5e838a464b814b7d5b4c2e6bf8')\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date = now.day\n",
    "\n",
    "keyword_list = []\n",
    "\n",
    "####################################################################\n",
    "########## Add keywords from Keyword Planner to keyword_list########\n",
    "####################################################################\n",
    "\n",
    "all_articles = newsapi.get_everything(q=[keyword_list]\n",
    "                                     #, from_paramater = '2018-01-08' #this can be changed to any date\n",
    "                                     , to = date\n",
    "                                     , language = 'en'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "data = all_articles['articles']\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# The Json is a list of dictionary values. This code will store it to a DataFrame.\n",
    "results = {'Description': [], 'Publish Date': [], 'URL': []}\n",
    "columns = results.keys()\n",
    "df_data = pd.DataFrame(data=results, columns=columns)\n",
    "\n",
    "for entry in data:\n",
    "    #print(data[entry]['description'])\n",
    "    description = entry['description']\n",
    "    date = entry['publishedAt']\n",
    "    url = entry['url']\n",
    "    results = {'Description':[description], 'Publish Date':[date], 'URL': [url]}\n",
    "    df_data = df_data.append(pd.DataFrame(data=results, columns=results.keys()), ignore_index = False)\n",
    "    \n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "############################################################################\n",
    "###### Now, create a crawler to crawl the information of each url ##########\n",
    "############################################################################\n",
    "\n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# Make a list for the urls to be stored into from the dataframe\n",
    "url_list = []\n",
    "for entry in df_data['URL']:\n",
    "    url_list.append(entry)\n",
    "    \n",
    "#print(url_list)\n",
    "\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = url_list\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        # for article in response.xpath('//article'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                'everything': response.xpath('//text()').extract()\n",
    "                #'all': response.xpath('/descendant-or-self::node()').extract()\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                #'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                #'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                #'text': article.xpath('//*[@id=\"post-669657\"]/div[3]').extract()\n",
    "                #,'body': article.xpath('//*[@id=\"single-post\"]').extract()\n",
    "                #'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'firstpage4.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "######## This doesn't stop running because it is a stream and it stays open. \n",
    "######## Cell after this one downloads the stream, but it is run through cmd line\n",
    "\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    " \n",
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('python.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    " \n",
    "\n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=['Car Audio', 'Walkie Talkies', 'Radar Detector'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# To run this code, first edit config.py with your configuration, then:\n",
    "#\n",
    "# mkdir data\n",
    "# python twitter_stream_download.py -q apple -d data\n",
    "# \n",
    "# It will produce the list of tweets for the query \"apple\" \n",
    "# in the file data/stream_apple.json\n",
    "\n",
    "import tweepy\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import time\n",
    "import argparse\n",
    "import string\n",
    "import config\n",
    "import json\n",
    "\n",
    "def get_parser():\n",
    "    \"\"\"Get parser for command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Twitter Downloader\")\n",
    "    parser.add_argument(\"-q\",\n",
    "                        \"--query\",\n",
    "                        dest=\"query\",\n",
    "                        help=\"Query/Filter\",\n",
    "                        default='-')\n",
    "    parser.add_argument(\"-d\",\n",
    "                        \"--data-dir\",\n",
    "                        dest=\"data_dir\",\n",
    "                        help=\"Output/Data Directory\")\n",
    "    return parser\n",
    "\n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    \"\"\"Custom StreamListener for streaming data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, query):\n",
    "        query_fname = format_filename(query)\n",
    "        self.outfile = \"%s/stream_%s.json\" % (data_dir, query_fname)\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open(self.outfile, 'a') as f:\n",
    "                f.write(data)\n",
    "                print(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "            time.sleep(5)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "\n",
    "\n",
    "def format_filename(fname):\n",
    "    \"\"\"Convert file name into a safe string.\n",
    "    Arguments:\n",
    "        fname -- the file name to convert\n",
    "    Return:\n",
    "        String -- converted file name\n",
    "    \"\"\"\n",
    "    return ''.join(convert_valid(one_char) for one_char in fname)\n",
    "\n",
    "\n",
    "def convert_valid(one_char):\n",
    "    \"\"\"Convert a character into '_' if invalid.\n",
    "    Arguments:\n",
    "        one_char -- the char to convert\n",
    "    Return:\n",
    "        Character -- converted char\n",
    "    \"\"\"\n",
    "    valid_chars = \"-_.%s%s\" % (string.ascii_letters, string.digits)\n",
    "    if one_char in valid_chars:\n",
    "        return one_char\n",
    "    else:\n",
    "        return '_'\n",
    "\n",
    "@classmethod\n",
    "def parse(cls, api, raw):\n",
    "    status = cls.first_parse(api, raw)\n",
    "    setattr(status, 'json', json.dumps(raw))\n",
    "    return status\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = get_parser()\n",
    "    args = parser.parse_args()\n",
    "    auth = OAuthHandler(config.consumer_key, config.consumer_secret)\n",
    "    auth.set_access_token(config.access_token, config.access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    twitter_stream = Stream(auth, MyListener(args.data_dir, args.query))\n",
    "    twitter_stream.filter(track=[args.query])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will send a request for \n",
    "\n",
    "curl -X POST \"https://api.twitter.com/1.1/tweets/search/:product/:label.json\" -d '{\"query\":\"Car Audio, Walkie Talkies, Radar Detector\",\"maxResults\":\"500\"}' -H \"Authorization: 963889537842851841-FGZVIMTpgCS760gVa3xNu9okSg8hB0j, obZiBaCujDaDzgT50s3q5fuKUTkvQuouQ57wUfl1YH2us\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying something else\n",
    "class MyModelParser(tweepy.parsers.ModelParser):\n",
    "    def parse(self, method, payload:\n",
    "              result = super(MyModelparser, self).parse(method, payload)\n",
    "              result._payload = json.loads(payload)\n",
    "api = tweepy.API(auth, parser=MyModelParser())\n",
    "results = api.search(q='Car Audio', lan='en')\n",
    "              \n",
    "for s in results._payload:\n",
    "              print(json.dumps(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "walkie_talkies_json = api.search(q='Walkie Talkies', lan='en') \n",
    "# Optional paramters: [, locale][, rpp][, page][, since_id][, geocode][, show_user])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
