{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TODO: Read 6.1 Time Series and 6.4 Advanced NLP\n",
    "\n",
    "# Abstract\n",
    "Abstract will be written after all analyses are complete.\n",
    "For a copy of the proposal, visit \n",
    "\n",
    "# Table of Contents\n",
    "## I. Objective\n",
    "## II. Data Collection\n",
    "## III. Scraping\n",
    "## IV. Unsupervised Neural Networks on Scraped Data to Generate Sentiment Analysis\n",
    "## V. Supervised Neural Networks to Predict Rises in Popularity\n",
    "## VI. Results\n",
    "## VII. Conclusion\n",
    "## VIII. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Objective\n",
    "\n",
    "What is the Problem You Are Attempting to Solve?\n",
    "\tWith the internet becoming increasingly involved in the average person’s daily life, many companies have taken to ecommerce, which is the electronic sale of goods through the internet. For an entrepreneur to break into the ecommerce market or for a large company to add their sources of revenue, a niche market must be found.\n",
    "\tFor the sake of clarity in this project, a niche market will be defined using this Wikipedia description:\n",
    "A niche market is the subset of the market on which a specific product is focused. The market niche defines as the product features aimed at satisfying specific market needs, as well as the price range, production quality, and the demographics that is intended to impact.\n",
    "The niche market is highly specialized, and aiming to survive among the competition from numerous super companies. Even established companies create products for different niches, for example, Hewlett-Packard has all-in-one machines for printing, scanning, and faxing targeted for the home office niche while at the same time having separate machines with one of these functions for big businesses.\n",
    "With this definition in mind, the problem will be defined as, “With competition saturating the ecommerce market, what is the best niche market for a business or entrepreneur to expand into?”\n",
    "\tTo narrow the scope of this project, the focus will be on electronics appliances. \n",
    "How is Your Solution Valuable?\n",
    "The result of this project will be a solution that exceeds the performance of other niche market research tools in terms of finding the ideal niche market in a given sector. The ideal niche market is one that has some or all of the following criteria:\n",
    "•\thigh search volume\n",
    "•\tproducts that are difficult for consumers to find locally\n",
    "•\tsolves a common problem\n",
    "•\thas a customer base that is passionate\n",
    "•\tis not overrun with websites that have a Google PageRank of five or higher\n",
    "•\thas accessible suppliers.\n",
    "o\tGoogle PageRank is explained in the Appendix\n",
    "The solution will drastically reduce the number of hours and amount of wages spent for market research and will be a valuable tool for anyone in ecommerce, from large companies to individual entrepreneurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data Collection\n",
    "\n",
    "What is Your Data Source and How Will You Access It?\n",
    "Links to sources in Appendix, Access explained in Techniques Section Below\n",
    "Data for Machine Learning Models - Facebook and Twitter APIs\n",
    "Competition Research - PageRank Status for Chrome\n",
    "Validation - Google Trends\n",
    "\n",
    "\n",
    "### What Techniques From the Course Do You Anticipate Using?\n",
    "\n",
    "\n",
    "The project will begin with a broad search of keywords related to technology appliances on Keyword Planner (see appendix for description). For example, a keyword search was conducted for \"Consumer Electronics Appliances\" that returned seven hundred keywords with information on their average monthly searches and their level of competition (Low, Medium, or High). This information will be downloaded and filtered to generate a list of keywords that have a high number of average monthly searches and a low rank of competition. Keyword Planner measures competition by evaluating the number of advertisers that showed on each keyword relative to all keywords across Google. \n",
    "\n",
    "\n",
    "Twitter and Facebook will then be scraped to gather posts that reference the products in the keyword list. With this data, an Unsupervised Neural Network will perform Natural Language Processing to generate sentiment analysis of the products.\n",
    "\n",
    "\n",
    "The combined data from all techniques will be used as the features for a \n",
    "Supervised Neural Network that will predict the rise in popularity of a product over a six month period.\n",
    "The predictions will be evaluated against the rise in popularity of the product(s) via Google Trends results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Planning Results\n",
    "\n",
    "The Keyword Planner returned over seven hundred keywords under \"Consumer Electronics.\" The keywords were filtered to a list that had Average Monthly Searches between ten thousand and one hundred thousand and a Competition value under .70. The resulting list contained forty-eight keywords. After ruling out any brand names and similar keywords, this project will proceed with three of the items: Walkie Talkies, Radar Detectors, and Car Audio systems. An AliExpress search was conducted to verify the existence of manufacturers that were in a tolerable price range and able to ship one-piece orders. The tolerable price range was as follows: Walkie Talkies < \\$25, Radar Detectors < \\$15, and Car Audio < \\$45.\n",
    "\n",
    "Links to these items are in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Scraping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape Twitter's API, I made a new twitter account and registered an application under it to be given the necessary keys and access tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "##### Mining Twitter API #############\n",
    "######################################\n",
    "\n",
    "# https://developer.twitter.com/en/docs/tweets/filter-realtime/api-reference/post-statuses-filter.html\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = '8AZsgftPqH9dbyLfc5IFWo39v'\n",
    "consumer_secret = 'MuMAVS7rIEYa0dsyauTOeJMkjRgmU5NEvG2uzyOtWItJLli48u'\n",
    "access_token = '963889537842851841-FGZVIMTpgCS760gVa3xNu9okSg8hB0j'\n",
    "access_secret = 'obZiBaCujDaDzgT50s3q5fuKUTkvQuouQ57wUfl1YH2us'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "car_audio_json = [status for status in api.search(q='Car Audio', lan='en')] \n",
    "# Optional paramters: [, locale][, rpp][, page][, since_id][, geocode][, show_user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand new car stereo\n",
      "https://t.co/a0zyYfAWez https://t.co/tJ9qXbSTSs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Car Audio'\n",
    "\n",
    "car_audio_tweets = [status for status in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(100)]\n",
    "print(car_audio_tweets[0].full_text)\n",
    "len(car_audio_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brand new car stereo\\nhttps://t.co/a0zyYfAWez https://t.co/tJ9qXbSTSs'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_text = []\n",
    "for text in car_audio_tweets:\n",
    "    ca_text.append(text.full_text)\n",
    "ca_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Category, Text]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the lists of texts into a DataFrame\n",
    "import pandas as pd\n",
    "texts = {'Category':[], 'Text':[]}\n",
    "columns = texts.keys()\n",
    "all_texts = pd.DataFrame(data=texts, columns=columns)\n",
    "all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = {'Category':'ca_text', 'Text':ca_text}\n",
    "all_texts = all_texts.append(pd.DataFrame(data=texts, columns=texts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Walkie Talkies of 2017 | Top Ten Reviews https://t.co/0Qfr4QL4pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Walkie Talkies'\n",
    "\n",
    "walkie_talkie_tweets = [status for status in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(100)]\n",
    "print(walkie_talkie_tweets[0].full_text)\n",
    "len(walkie_talkie_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Best Walkie Talkies of 2017 | Top Ten Reviews https://t.co/0Qfr4QL4pb'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_text = []\n",
    "for text in walkie_talkie_tweets:\n",
    "    wt_text.append(text.full_text)\n",
    "wt_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = {'Category':'wt_text', 'Text':wt_text}\n",
    "all_texts = all_texts.append(pd.DataFrame(data=texts, columns=texts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help me win an Escort Redline EX #RadarDetector from @EscortRadar and @VortexRadar! https://t.co/3I4yVuquGh #giveaway https://t.co/ryDm4SKB6i\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'radar detector'\n",
    "\n",
    "radar_detector_tweets = [status for status in tweepy.Cursor(api.search, q=query, tweet_mode='extended').items(100)]\n",
    "print(radar_detector_tweets[0].full_text)\n",
    "len(radar_detector_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Help me win an Escort Redline EX #RadarDetector from @EscortRadar and @VortexRadar! https://t.co/3I4yVuquGh #giveaway https://t.co/ryDm4SKB6i'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_text = []\n",
    "for text in radar_detector_tweets:\n",
    "    rd_text.append(text.full_text)\n",
    "rd_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = {'Category':'rd_text', 'Text':rd_text}\n",
    "all_texts = all_texts.append(pd.DataFrame(data=texts, columns=texts.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle the results so that you can run this daily and add to your data collection.\n",
    "\n",
    "# Make it remove duplicate tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alex - the below drop_duplicates code removes all rows and only keeps the first row of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save DataFrame object to csv\n",
    "# Set index=False to prevent the extra column of the original indices\n",
    "# all_texts.to_csv(path_or_buf=r'D:\\GitHub\\Final Capstone\\Ecommerce Data\\Tweets.csv', index=False)\n",
    "\n",
    "# Now that the csv has been created, append it by writing with mode='a'\n",
    "f = 'D:\\GitHub\\Final Capstone\\Ecommerce Data\\Tweets.csv'\n",
    "all_texts.to_csv(f, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original = pd.read_csv('D:\\\\GitHub\\\\Final Capstone\\\\Ecommerce Data\\\\Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>WE GOT IN THE CAR AND MY DAD'S PHONE AUTO CONN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there’s no audio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/UyNgZtCY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/JxudunH6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>3?17?????18????Car Audio Club????????Super Hig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                               Text\n",
       "0  ca_text  WE GOT IN THE CAR AND MY DAD'S PHONE AUTO CONN...\n",
       "1  ca_text  RT @utdxtra: For some reason, there’s no audio...\n",
       "2  ca_text  I liked a @YouTube video https://t.co/UyNgZtCY...\n",
       "3  ca_text  I liked a @YouTube video https://t.co/JxudunH6...\n",
       "4  ca_text  3?17?????18????Car Audio Club????????Super Hig..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Break Point for restarting kernel\n",
    "break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Unsupervised Neural Network for Sentiment Classification of Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below cells until the next markdown are from: http://www.developintelligence.com/blog/2017/06/practical-neural-networks-keras-classifying-yelp-reviews/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    " \n",
    "import json\n",
    " \n",
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5)\n",
    "toytexts = [\"Is is a common word\", \"So is the\", \"the is common\", \"discombobulation is not common\"]\n",
    "tokenizer.fit_on_texts(toytexts)\n",
    "sequences = tokenizer.texts_to_sequences(toytexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = tweets.Text\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1199"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=300))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# In line two, we add an Embedding layer. This layer lets the network expand each token to a larger vector, allowing the network to represent words in a meaningful way. We pass 20000 as the first argument, which is the size of our vocabulary (remember, we told the tokenizer to only use the 20 000 most common words earlier), and 128 as the second, which means that each token can be expanded to a vector of size 128. We give it an input_length of 300, which is the length of each of our sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(data, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50282139]\n",
      " [ 0.50153965]\n",
      " [ 0.50307637]\n",
      " ..., \n",
      " [ 0.49861246]\n",
      " [ 0.5001632 ]\n",
      " [ 0.50112617]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['predicted_value3'] = list(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>predicted_value3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>WE GOT IN THE CAR AND MY DAD'S PHONE AUTO CONN...</td>\n",
       "      <td>[0.502821]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there’s no audio...</td>\n",
       "      <td>[0.50154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/UyNgZtCY...</td>\n",
       "      <td>[0.503076]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/JxudunH6...</td>\n",
       "      <td>[0.503111]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>3?17?????18????Car Audio Club????????Super Hig...</td>\n",
       "      <td>[0.496957]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there’s no audio...</td>\n",
       "      <td>[0.50154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there’s no audio...</td>\n",
       "      <td>[0.50154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>(Audio) Pursuit: @KokomoPolice in car chase fr...</td>\n",
       "      <td>[0.499988]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>3?17?????18????Car Audio Club????????Super Hig...</td>\n",
       "      <td>[0.501713]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there’s no audio...</td>\n",
       "      <td>[0.50154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/38K8ydFJ...</td>\n",
       "      <td>[0.503063]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there’s no audio...</td>\n",
       "      <td>[0.50154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there’s no audio...</td>\n",
       "      <td>[0.50154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>@MirrorFootball He set up the phone to record ...</td>\n",
       "      <td>[0.497613]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>12v Car Auto Boat Audio Fuse High Power 100 Am...</td>\n",
       "      <td>[0.497387]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there’s no audio...</td>\n",
       "      <td>[0.50154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>NVX Solid Gold Car Audio Subwoofer  Special Ed...</td>\n",
       "      <td>[0.500222]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there’s no audio...</td>\n",
       "      <td>[0.50154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @xeni: Stormy’s got him by the nuts, her ra...</td>\n",
       "      <td>[0.498106]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there’s no audio...</td>\n",
       "      <td>[0.50154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/MrRPBVXU...</td>\n",
       "      <td>[0.503007]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/rUzAGz5J...</td>\n",
       "      <td>[0.503073]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there’s no audio...</td>\n",
       "      <td>[0.50154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there’s no audio...</td>\n",
       "      <td>[0.50154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/09AxrGuL...</td>\n",
       "      <td>[0.502994]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/nk70aSNQ...</td>\n",
       "      <td>[0.503008]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there’s no audio...</td>\n",
       "      <td>[0.50154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>I liked a @YouTube video https://t.co/Z0mcOQWX...</td>\n",
       "      <td>[0.502967]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there’s no audio...</td>\n",
       "      <td>[0.50154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ca_text</td>\n",
       "      <td>RT @utdxtra: For some reason, there’s no audio...</td>\n",
       "      <td>[0.50154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Zhe owns duct tape, bolt cutters, and a radar ...</td>\n",
       "      <td>[0.501743]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>?Radar detector use in #California\\nhttps://t....</td>\n",
       "      <td>[0.500233]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Buy in china: https://t.co/clxyaWUQUM\\n - Univ...</td>\n",
       "      <td>[0.500559]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>$3 Discount On Car Radar Detector Speed Voice ...</td>\n",
       "      <td>[0.501139]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Good #pop #music now playing Darwin Deez - Rad...</td>\n",
       "      <td>[0.500239]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Comanda  acum Sistem Navigatie, Covorase Auto,...</td>\n",
       "      <td>[0.499637]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>China offers: https://t.co/Qy0UYSpqgO -V7 360 ...</td>\n",
       "      <td>[0.501638]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Now $15.08,  V7 Car Radar Detector $15.08 http...</td>\n",
       "      <td>[0.501523]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @ClimaYucatan: Celda de tormenta fuerte apr...</td>\n",
       "      <td>[0.501651]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @ClimaYucatan: Celda de tormenta fuerte apr...</td>\n",
       "      <td>[0.501651]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @ClimaYucatan: Celda de tormenta fuerte apr...</td>\n",
       "      <td>[0.501651]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>I added a video to a @YouTube playlist https:/...</td>\n",
       "      <td>[0.498681]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @ClimaYucatan: Celda de tormenta fuerte apr...</td>\n",
       "      <td>[0.501651]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @ClimaYucatan: Celda de tormenta fuerte apr...</td>\n",
       "      <td>[0.501651]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Celda de tormenta fuerte aproxim�ndose a la ci...</td>\n",
       "      <td>[0.498748]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@jesscnspots My radar detector has saved my as...</td>\n",
       "      <td>[0.500596]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Once again my radar detector saved my life</td>\n",
       "      <td>[0.504208]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>_With the Driving100 Police Radar Detector_\\n\\...</td>\n",
       "      <td>[0.502231]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>someone get ya girl a freaking radar detector!...</td>\n",
       "      <td>[0.500338]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>This old lady had her handicap sign hanging fr...</td>\n",
       "      <td>[0.502156]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>.@AxisIPVideo D2050-VE Network Radar Detector ...</td>\n",
       "      <td>[0.501751]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@RobBanks_WYRK @BuffaloDigest @EverythingBuf @...</td>\n",
       "      <td>[0.502179]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@CharlieDaniels Adam Schiff Mueller will take ...</td>\n",
       "      <td>[0.498182]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Ex-display Aguri Skyway GTX50 GPS/Radar/Laser ...</td>\n",
       "      <td>[0.502558]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>10 % bonus cashback pentru Detector de radar V...</td>\n",
       "      <td>[0.499341]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>@Q_BEE15 Have you considered a good radar dete...</td>\n",
       "      <td>[0.50217]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Gold Radar best gold detector https://t.co/MkA...</td>\n",
       "      <td>[0.501959]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>RT @Avigilon: The Avigilon Presence Detector (...</td>\n",
       "      <td>[0.498612]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>Have you ever received a speeding ticket? Thes...</td>\n",
       "      <td>[0.500163]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>rd_text</td>\n",
       "      <td>China offers: https://t.co/ioK8XJ0tv6 -720P 2....</td>\n",
       "      <td>[0.501126]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1199 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                               Text  \\\n",
       "0     ca_text  WE GOT IN THE CAR AND MY DAD'S PHONE AUTO CONN...   \n",
       "1     ca_text  RT @utdxtra: For some reason, there’s no audio...   \n",
       "2     ca_text  I liked a @YouTube video https://t.co/UyNgZtCY...   \n",
       "3     ca_text  I liked a @YouTube video https://t.co/JxudunH6...   \n",
       "4     ca_text  3?17?????18????Car Audio Club????????Super Hig...   \n",
       "5     ca_text  RT @utdxtra: For some reason, there’s no audio...   \n",
       "6     ca_text  RT @utdxtra: For some reason, there’s no audio...   \n",
       "7     ca_text  (Audio) Pursuit: @KokomoPolice in car chase fr...   \n",
       "8     ca_text  3?17?????18????Car Audio Club????????Super Hig...   \n",
       "9     ca_text  RT @utdxtra: For some reason, there’s no audio...   \n",
       "10    ca_text  I liked a @YouTube video https://t.co/38K8ydFJ...   \n",
       "11    ca_text  RT @utdxtra: For some reason, there’s no audio...   \n",
       "12    ca_text  RT @utdxtra: For some reason, there’s no audio...   \n",
       "13    ca_text  @MirrorFootball He set up the phone to record ...   \n",
       "14    ca_text  12v Car Auto Boat Audio Fuse High Power 100 Am...   \n",
       "15    ca_text  RT @utdxtra: For some reason, there’s no audio...   \n",
       "16    ca_text  NVX Solid Gold Car Audio Subwoofer  Special Ed...   \n",
       "17    ca_text  RT @utdxtra: For some reason, there’s no audio...   \n",
       "18    ca_text  RT @xeni: Stormy’s got him by the nuts, her ra...   \n",
       "19    ca_text  RT @utdxtra: For some reason, there’s no audio...   \n",
       "20    ca_text  I liked a @YouTube video https://t.co/MrRPBVXU...   \n",
       "21    ca_text  I liked a @YouTube video https://t.co/rUzAGz5J...   \n",
       "22    ca_text  RT @utdxtra: For some reason, there’s no audio...   \n",
       "23    ca_text  RT @utdxtra: For some reason, there’s no audio...   \n",
       "24    ca_text  I liked a @YouTube video https://t.co/09AxrGuL...   \n",
       "25    ca_text  I liked a @YouTube video https://t.co/nk70aSNQ...   \n",
       "26    ca_text  RT @utdxtra: For some reason, there’s no audio...   \n",
       "27    ca_text  I liked a @YouTube video https://t.co/Z0mcOQWX...   \n",
       "28    ca_text  RT @utdxtra: For some reason, there’s no audio...   \n",
       "29    ca_text  RT @utdxtra: For some reason, there’s no audio...   \n",
       "...       ...                                                ...   \n",
       "1169  rd_text  Zhe owns duct tape, bolt cutters, and a radar ...   \n",
       "1170  rd_text  ?Radar detector use in #California\\nhttps://t....   \n",
       "1171  rd_text  Buy in china: https://t.co/clxyaWUQUM\\n - Univ...   \n",
       "1172  rd_text  $3 Discount On Car Radar Detector Speed Voice ...   \n",
       "1173  rd_text  Good #pop #music now playing Darwin Deez - Rad...   \n",
       "1174  rd_text  Comanda  acum Sistem Navigatie, Covorase Auto,...   \n",
       "1175  rd_text  China offers: https://t.co/Qy0UYSpqgO -V7 360 ...   \n",
       "1176  rd_text  Now $15.08,  V7 Car Radar Detector $15.08 http...   \n",
       "1177  rd_text  RT @ClimaYucatan: Celda de tormenta fuerte apr...   \n",
       "1178  rd_text  RT @ClimaYucatan: Celda de tormenta fuerte apr...   \n",
       "1179  rd_text  RT @ClimaYucatan: Celda de tormenta fuerte apr...   \n",
       "1180  rd_text  I added a video to a @YouTube playlist https:/...   \n",
       "1181  rd_text  RT @ClimaYucatan: Celda de tormenta fuerte apr...   \n",
       "1182  rd_text  RT @ClimaYucatan: Celda de tormenta fuerte apr...   \n",
       "1183  rd_text  Celda de tormenta fuerte aproxim�ndose a la ci...   \n",
       "1184  rd_text  @jesscnspots My radar detector has saved my as...   \n",
       "1185  rd_text         Once again my radar detector saved my life   \n",
       "1186  rd_text  _With the Driving100 Police Radar Detector_\\n\\...   \n",
       "1187  rd_text  someone get ya girl a freaking radar detector!...   \n",
       "1188  rd_text  This old lady had her handicap sign hanging fr...   \n",
       "1189  rd_text  .@AxisIPVideo D2050-VE Network Radar Detector ...   \n",
       "1190  rd_text  @RobBanks_WYRK @BuffaloDigest @EverythingBuf @...   \n",
       "1191  rd_text  @CharlieDaniels Adam Schiff Mueller will take ...   \n",
       "1192  rd_text  Ex-display Aguri Skyway GTX50 GPS/Radar/Laser ...   \n",
       "1193  rd_text  10 % bonus cashback pentru Detector de radar V...   \n",
       "1194  rd_text  @Q_BEE15 Have you considered a good radar dete...   \n",
       "1195  rd_text  Gold Radar best gold detector https://t.co/MkA...   \n",
       "1196  rd_text  RT @Avigilon: The Avigilon Presence Detector (...   \n",
       "1197  rd_text  Have you ever received a speeding ticket? Thes...   \n",
       "1198  rd_text  China offers: https://t.co/ioK8XJ0tv6 -720P 2....   \n",
       "\n",
       "     predicted_value3  \n",
       "0          [0.502821]  \n",
       "1           [0.50154]  \n",
       "2          [0.503076]  \n",
       "3          [0.503111]  \n",
       "4          [0.496957]  \n",
       "5           [0.50154]  \n",
       "6           [0.50154]  \n",
       "7          [0.499988]  \n",
       "8          [0.501713]  \n",
       "9           [0.50154]  \n",
       "10         [0.503063]  \n",
       "11          [0.50154]  \n",
       "12          [0.50154]  \n",
       "13         [0.497613]  \n",
       "14         [0.497387]  \n",
       "15          [0.50154]  \n",
       "16         [0.500222]  \n",
       "17          [0.50154]  \n",
       "18         [0.498106]  \n",
       "19          [0.50154]  \n",
       "20         [0.503007]  \n",
       "21         [0.503073]  \n",
       "22          [0.50154]  \n",
       "23          [0.50154]  \n",
       "24         [0.502994]  \n",
       "25         [0.503008]  \n",
       "26          [0.50154]  \n",
       "27         [0.502967]  \n",
       "28          [0.50154]  \n",
       "29          [0.50154]  \n",
       "...               ...  \n",
       "1169       [0.501743]  \n",
       "1170       [0.500233]  \n",
       "1171       [0.500559]  \n",
       "1172       [0.501139]  \n",
       "1173       [0.500239]  \n",
       "1174       [0.499637]  \n",
       "1175       [0.501638]  \n",
       "1176       [0.501523]  \n",
       "1177       [0.501651]  \n",
       "1178       [0.501651]  \n",
       "1179       [0.501651]  \n",
       "1180       [0.498681]  \n",
       "1181       [0.501651]  \n",
       "1182       [0.501651]  \n",
       "1183       [0.498748]  \n",
       "1184       [0.500596]  \n",
       "1185       [0.504208]  \n",
       "1186       [0.502231]  \n",
       "1187       [0.500338]  \n",
       "1188       [0.502156]  \n",
       "1189       [0.501751]  \n",
       "1190       [0.502179]  \n",
       "1191       [0.498182]  \n",
       "1192       [0.502558]  \n",
       "1193       [0.499341]  \n",
       "1194        [0.50217]  \n",
       "1195       [0.501959]  \n",
       "1196       [0.498612]  \n",
       "1197       [0.500163]  \n",
       "1198       [0.501126]  \n",
       "\n",
       "[1199 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding more layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=300))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(data, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['predicted_value2'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "import pickle\n",
    " \n",
    "# save the tokenizer and model\n",
    "with open(\"keras_tokenizer.pickle\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "model.save(\"tweets_sentiment_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that the csv has been created, append it by writing with mode='a'\n",
    "f = 'D:\\GitHub\\Final Capstone\\Ecommerce Data\\Tweets_With_Sentiments.csv'\n",
    "tweets.to_csv(f, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where the info from DevelopIntelligence ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Supervised Neural Network for Predicting Rises in Popularity\n",
    "\n",
    "The combined data from all techniques will be used as the features for a Supervised Neural Network that will predict the rise in popularity of a product over a six month period. The predictions will be evaluated against the rise in popularity of the product(s) via Google Trends results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x is the result of the sentiment analyses\n",
    "\n",
    "### y is whether the product rose in popularity in the last six months on Google Trends Data\n",
    "\n",
    "### Modify paramaters to fit text classification with two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import csvs from Google Trends\n",
    "ca = pd.read_csv('D:\\\\GitHub\\\\Final Capstone\\\\Trends Car Audio.csv')\n",
    "rd = pd.read_csv('D:\\\\GitHub\\\\Final Capstone\\\\Trends Radar Detector.csv')\n",
    "wt = pd.read_csv('D:\\\\GitHub\\\\Final Capstone\\\\Trends Walkie Talkies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.Searches.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a new field for thirty day average\n",
    "# These averages will be compared to the previous ones\n",
    "# to see if the average increase\n",
    "\n",
    "thirty_day_average = []\n",
    "\n",
    "# Have to start at row 29 to get the previous 30 days\n",
    "for i in ca.Searches[29:]:\n",
    "    av = int(ca.Seaches[i]) - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "daily_movement = []\n",
    "\n",
    "for i in ca.thirty_day_average:\n",
    "    daily_movement.append(ca.thirty_day_average[i] - ca.thirty_day_average[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, activation='relu', input_shape=(3072,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# Output shape should be equal to the number of classes (10)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = 64, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, verbose=False)\n",
    "print('Test Loss: ', score[0])\n",
    "print('Test Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII. Appendix\n",
    "\n",
    "Google PageRank Description (from Google Technology page):\n",
    "PageRank Explained\n",
    "PageRank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page’s value. In essence, Google interprets a link from page A to page B as a vote, by page A, for page B. But, Google looks at considerably more than the sheer volume of votes, or links a page receives; for example, it also analyzes the page that casts the vote. Votes cast by pages that are themselves “important” weigh more heavily and help to make other pages “important.” Using these and other factors, Google provides its views on pages’ relative importance.\n",
    "Of course, important pages mean nothing to you if they don’t match your query. So, Google combines PageRank with sophisticated text-matching techniques to find pages that are both important and relevant to your search. Google goes far beyond the number of times a term appears on a page and examines all dozens of aspects of the page’s content (and the content of the pages linking to it) to determine if it’s a good match for your query.\n",
    "\n",
    "Keyword Planner Description (from Google Adwords page):\n",
    "Keyword Planner is a free AdWords tool for new or experienced advertisers that’s like a workshop for building new Search Network campaigns or expanding existing ones. You can search for keyword and ad group ideas, see how a list of keywords might perform, and even create a new keyword list by multiplying several lists of keywords together. Keyword Planner can also help you choose competitive bids and budgets to use with your campaigns.\n",
    "Sources:\n",
    "Facebook: http://graph.facebook.com\n",
    "Google Trends: https://developers.google.com/gdata/docs/directory\n",
    "Google News: https://newsapi.org/s/google-news-api\n",
    "Keyword Planner: https://adwords.google.com/ko/KeywordPlanner/Home?sourceid=awo&__u=3050284228&__c=9347440984&authuser=0#search\n",
    "Twitter: https://developer.twitter.com/en/docs/basics/getting-started\n",
    "\n",
    "Links to AliExpress items: \n",
    "\n",
    "https://www.aliexpress.com/item/2pcs-New-Black-Retevis-RT628-Portable-radio-Walkie-Talkie-sets-0-5W-8CH-UHF-Europe-Frequency/32407070024.html?spm=2114.search0104.3.27.383641a8WAVsOz&ws_ab_test=searchweb0_0,searchweb201602_3_10152_10151_10065_10344_10130_10068_10324_10342_10547_10325_10343_10546_10340_10548_10341_10545_10084_10083_10618_10307_10313_10059_10534_100031_10629_10103_10626_10625_10624_10623_10622_10621_10620_10142,searchweb201603_25,ppcSwitch_5&algo_expid=c512be04-3611-4608-90ee-6e55891cae81-6&algo_pvid=c512be04-3611-4608-90ee-6e55891cae81&priceBeautifyAB=0\n",
    "\n",
    "https://www.aliexpress.com/item/4012B-4-1-inch-1-Din-Car-Radio-Auto-Audio-Stereo-FM-Bluetooth-2-0-Support/32838433123.html?spm=2114.search0104.3.25.6529687a4mQ9YS&ws_ab_test=searchweb0_0,searchweb201602_3_10152_10151_10065_10344_10130_10068_10324_10342_10547_10325_10343_10546_10340_10548_10341_10545_10084_10083_10618_10307_10313_10059_10534_100031_10629_10103_10626_10625_10624_10623_10622_10621_10620_10142,searchweb201603_25,ppcSwitch_5&algo_expid=78926112-aa75-459d-888f-2b887bd13743-3&algo_pvid=78926112-aa75-459d-888f-2b887bd13743&priceBeautifyAB=0\n",
    "\n",
    "https://www.aliexpress.com/item/Clear-Stock-Excelvan-E8-Car-Radar-Detector-360-Degree-Speed-Safety-Anti-Police-Scanning-Advanced-Voice/32839841753.html?spm=2114.search0104.3.1.1bc42b2a9yCeVF&ws_ab_test=searchweb0_0,searchweb201602_3_10152_10151_10065_10344_10130_10068_10324_10342_10547_10325_10343_10546_10340_10548_10341_10545_10084_10083_10618_10307_10313_10059_10534_100031_10629_10103_10626_10625_10624_10623_10622_10621_10620_10142,searchweb201603_25,ppcSwitch_5&algo_expid=9a63780f-134d-4690-9830-ebf6c5fff3aa-0&algo_pvid=9a63780f-134d-4690-9830-ebf6c5fff3aa&priceBeautifyAB=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra code that is not yet implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for row in all_texts.Text:\n",
    "    words = []\n",
    "    words.append(preprocess(row))\n",
    "    tokens.append(words)    \n",
    "    \n",
    "\n",
    "all_texts['tokens'] = tokens\n",
    "all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_texts.Text = str(all_texts.Text)\n",
    "all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(#max_df=1, # drop words that occur in more than \n",
    "                             #min_df=0, \n",
    "                             vocabulary = None,\n",
    "                             stop_words='english', \n",
    "                             lowercase=False, #convert everything to lower case \n",
    "                             use_idf=True,#inverse document frequencies for weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "results_tfidf=vectorizer.fit_transform(all_texts.Text)\n",
    "print(\"Number of features: %d\" % results_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "results_lsa = lsa.fit_transform(results_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsa_res = results_lsa.reshape(300,-1)\n",
    "tweets_by_component=pd.DataFrame(results_lsa)\n",
    "tweets_by_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(130,)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = results_lsa\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(X, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using k-means to cluster together authors\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=2, random_state=42).fit_predict(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = all_texts.loc[~all_texts.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupings = pd.DataFrame(y_pred, columns = ['Group'])\n",
    "groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupings['Text'] = data2['Text']\n",
    "groupings['Category'] = data2['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "# Utility function to clean text.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # Getting rid of unneccesary charactersLo\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "       \n",
    "    # Get rid of extra whitespace.\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "for text in all_texts.Text:\n",
    "    all_texts['cleaned_text'] = text_cleaner(text)\n",
    "\n",
    "#text_cleaner(ca_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tweet = unicode(tweet.decode('utf-8').lower())\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tokens = filter(lambda t: not t.startswith('@'), tokens)\n",
    "        tokens = filter(lambda t: not t.startswith('#'), tokens)\n",
    "        tokens = filter(lambda t: not t.startswith('http'), tokens)\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def postprocess(data, n=1000000):\n",
    "    data = data.head(n)\n",
    "    data['tokens'] = data['Text'].map(tokenize)\n",
    "    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "data = postprocess(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#Import the necessary modules\n",
    "import datetime\n",
    "from newsapi import NewsApiClient\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Matthew Kennedy's API Key, business will need to register for their own api and place it below (registration is free)\n",
    "newsapi = NewsApiClient(api_key='2613ce5e838a464b814b7d5b4c2e6bf8')\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date = now.day\n",
    "\n",
    "keyword_list = []\n",
    "\n",
    "####################################################################\n",
    "########## Add keywords from Keyword Planner to keyword_list########\n",
    "####################################################################\n",
    "\n",
    "all_articles = newsapi.get_everything(q=[keyword_list]\n",
    "                                     #, from_paramater = '2018-01-08' #this can be changed to any date\n",
    "                                     , to = date\n",
    "                                     , language = 'en'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "data = all_articles['articles']\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# The Json is a list of dictionary values. This code will store it to a DataFrame.\n",
    "results = {'Description': [], 'Publish Date': [], 'URL': []}\n",
    "columns = results.keys()\n",
    "df_data = pd.DataFrame(data=results, columns=columns)\n",
    "\n",
    "for entry in data:\n",
    "    #print(data[entry]['description'])\n",
    "    description = entry['description']\n",
    "    date = entry['publishedAt']\n",
    "    url = entry['url']\n",
    "    results = {'Description':[description], 'Publish Date':[date], 'URL': [url]}\n",
    "    df_data = df_data.append(pd.DataFrame(data=results, columns=results.keys()), ignore_index = False)\n",
    "    \n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "############################################################################\n",
    "###### Now, create a crawler to crawl the information of each url ##########\n",
    "############################################################################\n",
    "\n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# Make a list for the urls to be stored into from the dataframe\n",
    "url_list = []\n",
    "for entry in df_data['URL']:\n",
    "    url_list.append(entry)\n",
    "    \n",
    "#print(url_list)\n",
    "\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = url_list\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        # for article in response.xpath('//article'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                'everything': response.xpath('//text()').extract()\n",
    "                #'all': response.xpath('/descendant-or-self::node()').extract()\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                #'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                #'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                #'text': article.xpath('//*[@id=\"post-669657\"]/div[3]').extract()\n",
    "                #,'body': article.xpath('//*[@id=\"single-post\"]').extract()\n",
    "                #'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'firstpage4.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "######## This doesn't stop running because it is a stream and it stays open. \n",
    "######## Cell after this one downloads the stream, but it is run through cmd line\n",
    "\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    " \n",
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('python.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    " \n",
    "\n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=['Car Audio', 'Walkie Talkies', 'Radar Detector'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# To run this code, first edit config.py with your configuration, then:\n",
    "#\n",
    "# mkdir data\n",
    "# python twitter_stream_download.py -q apple -d data\n",
    "# \n",
    "# It will produce the list of tweets for the query \"apple\" \n",
    "# in the file data/stream_apple.json\n",
    "\n",
    "import tweepy\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import time\n",
    "import argparse\n",
    "import string\n",
    "import config\n",
    "import json\n",
    "\n",
    "def get_parser():\n",
    "    \"\"\"Get parser for command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Twitter Downloader\")\n",
    "    parser.add_argument(\"-q\",\n",
    "                        \"--query\",\n",
    "                        dest=\"query\",\n",
    "                        help=\"Query/Filter\",\n",
    "                        default='-')\n",
    "    parser.add_argument(\"-d\",\n",
    "                        \"--data-dir\",\n",
    "                        dest=\"data_dir\",\n",
    "                        help=\"Output/Data Directory\")\n",
    "    return parser\n",
    "\n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    \"\"\"Custom StreamListener for streaming data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, query):\n",
    "        query_fname = format_filename(query)\n",
    "        self.outfile = \"%s/stream_%s.json\" % (data_dir, query_fname)\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open(self.outfile, 'a') as f:\n",
    "                f.write(data)\n",
    "                print(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "            time.sleep(5)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "\n",
    "\n",
    "def format_filename(fname):\n",
    "    \"\"\"Convert file name into a safe string.\n",
    "    Arguments:\n",
    "        fname -- the file name to convert\n",
    "    Return:\n",
    "        String -- converted file name\n",
    "    \"\"\"\n",
    "    return ''.join(convert_valid(one_char) for one_char in fname)\n",
    "\n",
    "\n",
    "def convert_valid(one_char):\n",
    "    \"\"\"Convert a character into '_' if invalid.\n",
    "    Arguments:\n",
    "        one_char -- the char to convert\n",
    "    Return:\n",
    "        Character -- converted char\n",
    "    \"\"\"\n",
    "    valid_chars = \"-_.%s%s\" % (string.ascii_letters, string.digits)\n",
    "    if one_char in valid_chars:\n",
    "        return one_char\n",
    "    else:\n",
    "        return '_'\n",
    "\n",
    "@classmethod\n",
    "def parse(cls, api, raw):\n",
    "    status = cls.first_parse(api, raw)\n",
    "    setattr(status, 'json', json.dumps(raw))\n",
    "    return status\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = get_parser()\n",
    "    args = parser.parse_args()\n",
    "    auth = OAuthHandler(config.consumer_key, config.consumer_secret)\n",
    "    auth.set_access_token(config.access_token, config.access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    twitter_stream = Stream(auth, MyListener(args.data_dir, args.query))\n",
    "    twitter_stream.filter(track=[args.query])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will send a request for \n",
    "\n",
    "curl -X POST \"https://api.twitter.com/1.1/tweets/search/:product/:label.json\" -d '{\"query\":\"Car Audio, Walkie Talkies, Radar Detector\",\"maxResults\":\"500\"}' -H \"Authorization: 963889537842851841-FGZVIMTpgCS760gVa3xNu9okSg8hB0j, obZiBaCujDaDzgT50s3q5fuKUTkvQuouQ57wUfl1YH2us\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying something else\n",
    "class MyModelParser(tweepy.parsers.ModelParser):\n",
    "    def parse(self, method, payload:\n",
    "              result = super(MyModelparser, self).parse(method, payload)\n",
    "              result._payload = json.loads(payload)\n",
    "api = tweepy.API(auth, parser=MyModelParser())\n",
    "results = api.search(q='Car Audio', lan='en')\n",
    "              \n",
    "for s in results._payload:\n",
    "              print(json.dumps(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "walkie_talkies_json = api.search(q='Walkie Talkies', lan='en') \n",
    "# Optional paramters: [, locale][, rpp][, page][, since_id][, geocode][, show_user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do we need to hit Amazon and Ebay APIs to compare prices vs our manufacturer?\n",
    "\n",
    "############### Yup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for text in all_texts:\n",
    "\n",
    "    results = keras.preprocessing.text.Tokenizer(ca_text,\n",
    "                                   num_words=None,\n",
    "                                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,\n",
    "                                   split=\" \",\n",
    "                                   char_level=False,\n",
    "                                   oov_token=None)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Example from https://github.com/keras-team/keras/blob/master/examples/imdb_bidirectional_lstm.py\n",
    "\n",
    "'''Trains a Bidirectional LSTM on the IMDB sentiment classification task.\n",
    "Output after 4 epochs on CPU: ~0.8146\n",
    "Time per epoch on CPU (Core i7): ~150s.\n",
    "'''\n",
    "\n",
    "########################################################################\n",
    "########### LOOK INTO USING GPU TO SAVE ON CPU LOAD ####################\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import keras\n",
    "from keras.preprocessing import text\n",
    "\n",
    "results = []\n",
    "\n",
    "for text in all_texts:\n",
    "    results = keras.preprocessing.text.text_to_word_sequence(text,\n",
    "                                               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                               lower=True,\n",
    "                                               split=\" \")\n",
    "    \n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
